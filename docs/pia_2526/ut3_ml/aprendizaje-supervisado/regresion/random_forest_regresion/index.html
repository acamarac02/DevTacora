<!doctype html>
<html lang="es" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-pia_2526/ut3_ml/aprendizaje-supervisado/regresion/random_forest_regresion" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">Random Forest Regresi√≥n | DevTacora</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://acamarac02.github.io/DevTacora/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://acamarac02.github.io/DevTacora/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://acamarac02.github.io/DevTacora/docs/pia_2526/ut3_ml/aprendizaje-supervisado/regresion/random_forest_regresion"><meta data-rh="true" property="og:locale" content="es"><meta data-rh="true" name="docusaurus_locale" content="es"><meta data-rh="true" name="docsearch:language" content="es"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Random Forest Regresi√≥n | DevTacora"><meta data-rh="true" name="description" content="Introducci√≥n a Random Forest Regression en Machine Learning. Funcionamiento del algoritmo (bootstrap y submuestreo de variables), diferencias con √°rboles de clasificaci√≥n, hiperpar√°metros principales, importancia de variables y m√©tricas de evaluaci√≥n."><meta data-rh="true" property="og:description" content="Introducci√≥n a Random Forest Regression en Machine Learning. Funcionamiento del algoritmo (bootstrap y submuestreo de variables), diferencias con √°rboles de clasificaci√≥n, hiperpar√°metros principales, importancia de variables y m√©tricas de evaluaci√≥n."><meta data-rh="true" name="keywords" content="Random Forest,Bosques Aleatorios,Regresi√≥n,Random Forest Regression,Machine Learning,scikit-learn,bootstrap,bagging"><link data-rh="true" rel="icon" href="/DevTacora/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://acamarac02.github.io/DevTacora/docs/pia_2526/ut3_ml/aprendizaje-supervisado/regresion/random_forest_regresion"><link data-rh="true" rel="alternate" href="https://acamarac02.github.io/DevTacora/docs/pia_2526/ut3_ml/aprendizaje-supervisado/regresion/random_forest_regresion" hreflang="es"><link data-rh="true" rel="alternate" href="https://acamarac02.github.io/DevTacora/docs/pia_2526/ut3_ml/aprendizaje-supervisado/regresion/random_forest_regresion" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"UT3. Machine Learning","item":"https://acamarac02.github.io/DevTacora/docs/category/ut3-machine-learning"},{"@type":"ListItem","position":2,"name":"Aprendizaje supervisado","item":"https://acamarac02.github.io/DevTacora/docs/category/aprendizaje-supervisado"},{"@type":"ListItem","position":3,"name":"Regresi√≥n","item":"https://acamarac02.github.io/DevTacora/docs/category/regresi√≥n"},{"@type":"ListItem","position":4,"name":"Random Forest Regresi√≥n","item":"https://acamarac02.github.io/DevTacora/docs/pia_2526/ut3_ml/aprendizaje-supervisado/regresion/random_forest_regresion"}]}</script><link rel="alternate" type="application/rss+xml" href="/DevTacora/blog/rss.xml" title="DevTacora RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/DevTacora/blog/atom.xml" title="DevTacora Atom Feed">




<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7WCKN9ZY1F"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-7WCKN9ZY1F",{anonymize_ip:!0})</script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/DevTacora/assets/css/styles.71ba2beb.css">
<script src="/DevTacora/assets/js/runtime~main.58b605d6.js" defer="defer"></script>
<script src="/DevTacora/assets/js/main.8d31ca9b.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Saltar al contenido principal"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Saltar al contenido principal</a></div><nav aria-label="Principal" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Alternar barra lateral" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/DevTacora/"><div class="navbar__logo"><img src="/DevTacora/img/devtacora_logo.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/DevTacora/img/devtacora_logo.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">DevTacora</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/DevTacora/docs/pia_2526/">PIA</a><a class="navbar__item navbar__link" href="/DevTacora/docs/pmdm_2526/">PMDM</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/DevTacora/docs/licencia">Licencia</a><a href="https://www.linkedin.com/in/aliciacamcas" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Cambiar entre modo oscuro y claro (actualmente system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Volver al principio" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Barra lateral de Documentos" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/DevTacora/docs/pia_2526/">Presentaci√≥n</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/DevTacora/docs/category/ut1-introducci√≥n-a-la-ia">UT1. Introducci√≥n a la IA</a><button aria-label="Ampliar la categor√≠a &#x27;UT1. Introducci√≥n a la IA&#x27; de la barra lateral" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/DevTacora/docs/category/ut2-python-para-la-ia">UT2. Python para la IA</a><button aria-label="Ampliar la categor√≠a &#x27;UT2. Python para la IA&#x27; de la barra lateral" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/DevTacora/docs/category/ut3-machine-learning">UT3. Machine Learning</a><button aria-label="Colapsar categor√≠a &#x27;UT3. Machine Learning&#x27; de la barra lateral" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/DevTacora/docs/pia_2526/ut3_ml/introduccion">Introducci√≥n</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/DevTacora/docs/pia_2526/ut3_ml/primer-modelo">Primer modelo de ML: Titanic</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/DevTacora/docs/pia_2526/ut3_ml/eda">Exploratory Data Analysis (EDA)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/DevTacora/docs/pia_2526/ut3_ml/preprocesamiento">Preprocesamiento de Datos</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/DevTacora/docs/category/aprendizaje-supervisado">Aprendizaje supervisado</a><button aria-label="Colapsar categor√≠a &#x27;Aprendizaje supervisado&#x27; de la barra lateral" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/DevTacora/docs/pia_2526/ut3_ml/aprendizaje-supervisado/tips-generales">Tips generales</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/DevTacora/docs/category/clasificaci√≥n">Clasificaci√≥n</a><button aria-label="Ampliar la categor√≠a &#x27;Clasificaci√≥n&#x27; de la barra lateral" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/DevTacora/docs/category/regresi√≥n">Regresi√≥n</a><button aria-label="Colapsar categor√≠a &#x27;Regresi√≥n&#x27; de la barra lateral" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/DevTacora/docs/pia_2526/ut3_ml/aprendizaje-supervisado/regresion/intro_regresion">Ideas generales</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/DevTacora/docs/pia_2526/ut3_ml/aprendizaje-supervisado/regresion/regresion_lineal">Regresi√≥n Lineal</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/DevTacora/docs/pia_2526/ut3_ml/aprendizaje-supervisado/regresion/knn_regresion">KNN Regresi√≥n</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/DevTacora/docs/pia_2526/ut3_ml/aprendizaje-supervisado/regresion/arbol_decision_regresion">Decision Trees Regresi√≥n</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/DevTacora/docs/pia_2526/ut3_ml/aprendizaje-supervisado/regresion/random_forest_regresion">Random Forest Regresi√≥n</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/DevTacora/docs/pia_2526/ut3_ml/aprendizaje-supervisado/regresion/regresion_ridge_lasso">Ridge y Lasso Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/DevTacora/docs/pia_2526/ut3_ml/aprendizaje-supervisado/regresion/gradient_boosting">Gradient Boosting Regresi√≥n</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/DevTacora/docs/pia_2526/ut3_ml/aprendizaje-supervisado/regresion/svr">Support Vector Regression (SVR)</a></li></ul></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/DevTacora/docs/category/ut4-deep-learning">UT4. Deep Learning</a><button aria-label="Ampliar la categor√≠a &#x27;UT4. Deep Learning&#x27; de la barra lateral" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Rastro de navegaci√≥n"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="P√°gina de Inicio" class="breadcrumbs__link" href="/DevTacora/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/DevTacora/docs/category/ut3-machine-learning"><span>UT3. Machine Learning</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/DevTacora/docs/category/aprendizaje-supervisado"><span>Aprendizaje supervisado</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/DevTacora/docs/category/regresi√≥n"><span>Regresi√≥n</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Random Forest Regresi√≥n</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">En esta p√°gina</button></div><div class="theme-doc-markdown markdown"><header><h1>Random Forest Regresi√≥n</h1></header><p>Los <strong>Random Forest para Regresi√≥n (Random Forest Regression)</strong> son algoritmos de Machine Learning utilizados para <strong>predecir valores num√©ricos</strong> combinando las predicciones de <strong>muchos √°rboles de decisi√≥n</strong>.</p>
<p>Un Random Forest es, literalmente, un <em>‚Äúbosque‚Äù</em> de √°rboles:</p>
<ul>
<li>Cada √°rbol aprende reglas <em>if‚Äìelse</em> como un Decision Tree</li>
<li>Pero se entrena con <strong>variaci√≥n</strong> (datos y variables diferentes)</li>
<li>La predicci√≥n final se obtiene <strong>promediando</strong> las predicciones de todos los √°rboles</li>
</ul>
<p>En la pr√°ctica, Random Forest suele ser m√°s <strong>robusto</strong> y generaliza mejor que un √∫nico √°rbol, porque reduce el <strong>overfitting</strong> t√≠pico de los Decision Trees.</p>
<p><img decoding="async" loading="lazy" alt="Gr√°fico EDA" src="/DevTacora/assets/images/resumen-rfr-cd1a684c24fd86b0040fa97375d70e4a.png" width="1400" height="1000" class="img_ev3q"></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="idea-principal-del-algoritmo">Idea principal del algoritmo<a href="#idea-principal-del-algoritmo" class="hash-link" aria-label="Enlace directo al Idea principal del algoritmo" title="Enlace directo al Idea principal del algoritmo">‚Äã</a></h2>
<p>La idea es sencilla:</p>
<blockquote>
<p>‚ÄúEn vez de confiar en un solo √°rbol (que puede sobreajustar), entrenamos muchos √°rboles diferentes y combinamos sus predicciones.‚Äù</p>
</blockquote>
<p>Para conseguir √°rboles <strong>diferentes</strong>, Random Forest introduce dos fuentes de aleatoriedad:</p>
<ol>
<li><strong>Bootstrap de filas (bagging)</strong>: cada √°rbol se entrena con una muestra aleatoria de los datos</li>
<li><strong>Submuestreo de variables</strong>: en cada split, el √°rbol solo puede probar un subconjunto aleatorio de features</li>
</ol>
<p>Esta combinaci√≥n hace que los √°rboles no sean copias unos de otros, y por tanto el promedio final sea m√°s estable.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Random Forest es un ensemble</div><div class="admonitionContent_BuS1"><p>Random Forest es un modelo <strong>ensemble</strong>, es decir, un modelo formado por la combinaci√≥n de muchos modelos simples (en este caso, √°rboles).</p><ul>
<li>Un √°rbol individual puede ser inestable y sobreajustar</li>
<li>Un conjunto de √°rboles tiende a ser m√°s robusto</li>
<li>La combinaci√≥n (media) reduce errores por ‚Äúcasualidades‚Äù del entrenamiento</li>
</ul></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="funcionamiento-interno-del-modelo">Funcionamiento interno del modelo<a href="#funcionamiento-interno-del-modelo" class="hash-link" aria-label="Enlace directo al Funcionamiento interno del modelo" title="Enlace directo al Funcionamiento interno del modelo">‚Äã</a></h2>
<p>Un Random Forest se entrena generando muchos √°rboles de decisi√≥n, pero <strong>no todos ven exactamente los mismos datos ni las mismas variables</strong>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="paso-1-bootstrap-muestreo-con-reemplazo">Paso 1: Bootstrap (muestreo con reemplazo)<a href="#paso-1-bootstrap-muestreo-con-reemplazo" class="hash-link" aria-label="Enlace directo al Paso 1: Bootstrap (muestreo con reemplazo)" title="Enlace directo al Paso 1: Bootstrap (muestreo con reemplazo)">‚Äã</a></h3>
<p>Para entrenar cada √°rbol:</p>
<ul>
<li>Se crea una muestra aleatoria del dataset <strong>con reemplazo</strong></li>
<li>Esto significa que:<!-- -->
<ul>
<li>algunas filas aparecen repetidas</li>
<li>algunas filas no aparecen en ese √°rbol</li>
</ul>
</li>
</ul>
<p>Ejemplo: si el dataset tiene 1.000 filas, cada √°rbol suele entrenarse con 1.000 filas muestreadas con reemplazo (por defecto), pero no ser√°n las mismas que en otro √°rbol.</p>
<p>Esto se conoce como <strong>bagging</strong> (<em>bootstrap aggregating</em>).</p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="paso-2-entrenar-un-√°rbol-con-submuestreo-de-variables">Paso 2: Entrenar un √°rbol con submuestreo de variables<a href="#paso-2-entrenar-un-√°rbol-con-submuestreo-de-variables" class="hash-link" aria-label="Enlace directo al Paso 2: Entrenar un √°rbol con submuestreo de variables" title="Enlace directo al Paso 2: Entrenar un √°rbol con submuestreo de variables">‚Äã</a></h3>
<p>Mientras el √°rbol se construye:</p>
<ul>
<li>En cada nodo, el algoritmo <strong>no prueba todas las variables</strong></li>
<li>En su lugar, selecciona un subconjunto aleatorio de features (por ejemplo, <code>sqrt(n_features)</code>)</li>
</ul>
<p>Luego:</p>
<ul>
<li>Eval√∫a posibles splits usando solo esas variables disponibles</li>
<li>Elige el split que mejor reduce el error</li>
</ul>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>¬øC√≥mo se elige la ‚Äúmejor divisi√≥n‚Äù en regresi√≥n?</div><div class="admonitionContent_BuS1"><p>En Random Forest para <strong>regresi√≥n</strong>, cada √°rbol usa el mismo criterio que un √°rbol de decisi√≥n de regresi√≥n:</p><ul>
<li>La divisi√≥n se elige buscando <strong>minimizar el error</strong></li>
<li>Habitualmente se usa <strong>MSE (Mean Squared Error)</strong> como medida del error</li>
</ul><p>El objetivo de cada split es que, dentro de cada grupo, los valores del target sean lo m√°s parecidos posible.</p></div></div>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="paso-3-repetir-para-crear-muchos-√°rboles">Paso 3: Repetir para crear muchos √°rboles<a href="#paso-3-repetir-para-crear-muchos-√°rboles" class="hash-link" aria-label="Enlace directo al Paso 3: Repetir para crear muchos √°rboles" title="Enlace directo al Paso 3: Repetir para crear muchos √°rboles">‚Äã</a></h3>
<p>El proceso se repite tantas veces como indique el hiperpar√°metro <code>n_estimators</code>:</p>
<ul>
<li>100 √°rboles</li>
<li>200 √°rboles</li>
<li>500 √°rboles‚Ä¶</li>
</ul>
<p>Cada √°rbol ser√° distinto porque:</p>
<ul>
<li>ha visto un bootstrap distinto</li>
<li>y ha tomado decisiones basadas en subconjuntos aleatorios de variables</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="entrenamiento-vs-predicci√≥n">Entrenamiento vs predicci√≥n<a href="#entrenamiento-vs-predicci√≥n" class="hash-link" aria-label="Enlace directo al Entrenamiento vs predicci√≥n" title="Enlace directo al Entrenamiento vs predicci√≥n">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="entrenamiento">Entrenamiento<a href="#entrenamiento" class="hash-link" aria-label="Enlace directo al Entrenamiento" title="Enlace directo al Entrenamiento">‚Äã</a></h3>
<p>Durante el entrenamiento, el Random Forest:</p>
<ol>
<li>Genera muchos conjuntos bootstrap</li>
<li>Entrena un √°rbol por cada bootstrap</li>
<li>Introduce aleatoriedad en las features de cada split</li>
<li>Guarda todos los √°rboles</li>
</ol>
<p>Este proceso puede ser <strong>costoso computacionalmente</strong>, porque no entrenamos un modelo, sino muchos.</p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="predicci√≥n">Predicci√≥n<a href="#predicci√≥n" class="hash-link" aria-label="Enlace directo al Predicci√≥n" title="Enlace directo al Predicci√≥n">‚Äã</a></h3>
<p>Para predecir un nuevo dato:</p>
<ol>
<li>El dato se pasa por <strong>cada √°rbol</strong></li>
<li>Cada √°rbol devuelve un valor num√©rico (su predicci√≥n)</li>
<li>El Random Forest devuelve la <strong>media</strong> de todas las predicciones</li>
</ol>
<blockquote>
<p>En regresi√≥n, Random Forest predice promediando.</p>
</blockquote>
<p>Esto suele dar resultados m√°s estables que un √°rbol individual, especialmente si los datos tienen ruido.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="random-forest-en-regresi√≥n-vs-clasificaci√≥n">Random Forest en regresi√≥n vs clasificaci√≥n<a href="#random-forest-en-regresi√≥n-vs-clasificaci√≥n" class="hash-link" aria-label="Enlace directo al Random Forest en regresi√≥n vs clasificaci√≥n" title="Enlace directo al Random Forest en regresi√≥n vs clasificaci√≥n">‚Äã</a></h2>
<p>El funcionamiento general es el mismo en ambos casos:</p>
<ul>
<li>muchos √°rboles</li>
<li>bootstrap</li>
<li>submuestreo de variables</li>
<li>agregaci√≥n final</li>
</ul>
<p>La diferencia est√° en <strong>c√≥mo se combinan las predicciones</strong>:</p>
<ul>
<li><strong>Clasificaci√≥n</strong> ‚Üí votaci√≥n mayoritaria</li>
<li><strong>Regresi√≥n</strong> ‚Üí media (promedio)</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="uso-de-random-forest-en-regresi√≥n">Uso de Random Forest en Regresi√≥n<a href="#uso-de-random-forest-en-regresi√≥n" class="hash-link" aria-label="Enlace directo al Uso de Random Forest en Regresi√≥n" title="Enlace directo al Uso de Random Forest en Regresi√≥n">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cu√°ndo-s√≠-usarlos">Cu√°ndo S√ç usarlos<a href="#cu√°ndo-s√≠-usarlos" class="hash-link" aria-label="Enlace directo al Cu√°ndo S√ç usarlos" title="Enlace directo al Cu√°ndo S√ç usarlos">‚Äã</a></h3>
<p>Random Forest suele funcionar muy bien cuando:</p>
<ul>
<li>Hay relaciones no lineales</li>
<li>El dataset tiene ruido moderado</li>
<li>Se busca buen rendimiento con poco ajuste</li>
<li>Se quiere un modelo robusto sin demasiada feature engineering</li>
</ul>
<p>En muchos problemas tabulares, Random Forest es un modelo ‚Äútodoterreno‚Äù.</p>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cu√°ndo-no-funcionan-bien">Cu√°ndo NO funcionan bien<a href="#cu√°ndo-no-funcionan-bien" class="hash-link" aria-label="Enlace directo al Cu√°ndo NO funcionan bien" title="Enlace directo al Cu√°ndo NO funcionan bien">‚Äã</a></h3>
<p>Puede no ser la mejor opci√≥n cuando:</p>
<ul>
<li>Hay much√≠simas variables (muy alta dimensionalidad)</li>
<li>Se necesita interpretabilidad total (un bosque es menos interpretable que un √°rbol)</li>
<li>El dataset es enorme y el entrenamiento se vuelve lento</li>
<li>Se requiere extrapolar fuera del rango observado (no es su punto fuerte)</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="importancia-del-preprocesamiento">Importancia del preprocesamiento<a href="#importancia-del-preprocesamiento" class="hash-link" aria-label="Enlace directo al Importancia del preprocesamiento" title="Enlace directo al Importancia del preprocesamiento">‚Äã</a></h2>
<table><thead><tr><th>Aspecto</th><th>¬øEs necesario?</th><th>Explicaci√≥n</th></tr></thead><tbody><tr><td>Tratamiento de nulos</td><td>‚úî S√≠</td><td>No admite valores nulos</td></tr><tr><td>Escalado</td><td>‚ùå No</td><td>No usa distancias</td></tr><tr><td>Variables categ√≥ricas</td><td>‚ö†Ô∏è Depende</td><td>Hay que codificarlas (one-hot, ordinal, etc.)</td></tr><tr><td>Outliers</td><td>‚ö†Ô∏è Importante</td><td>Un bosque es m√°s robusto que un √°rbol, pero outliers extremos a√∫n pueden afectar</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="principales-hiperpar√°metros">Principales hiperpar√°metros<a href="#principales-hiperpar√°metros" class="hash-link" aria-label="Enlace directo al Principales hiperpar√°metros" title="Enlace directo al Principales hiperpar√°metros">‚Äã</a></h2>
<p>Random Forest suele rendir muy bien ‚Äúpor defecto‚Äù, pero estos hiperpar√°metros son clave para controlar rendimiento, overfitting y coste:</p>
<ul>
<li><code>n_estimators</code></li>
<li><code>max_depth</code></li>
<li><code>min_samples_split</code></li>
<li><code>min_samples_leaf</code></li>
<li><code>max_features</code></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="n√∫mero-de-√°rboles-n_estimators">N√∫mero de √°rboles (<code>n_estimators</code>)<a href="#n√∫mero-de-√°rboles-n_estimators" class="hash-link" aria-label="Enlace directo al n√∫mero-de-√°rboles-n_estimators" title="Enlace directo al n√∫mero-de-√°rboles-n_estimators">‚Äã</a></h3>
<p>Controla cu√°ntos √°rboles se entrenan. Por lo general, m√°s √°rboles dan predicciones m√°s estables (hasta cierto punto) pero con un mayor coste de entrenamiento</p>
<p>En general:</p>
<ul>
<li>100 suele ser un buen punto de partida</li>
<li>200‚Äì500 puede mejorar en datasets complejos</li>
</ul>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="profundidad-m√°xima-max_depth">Profundidad m√°xima (<code>max_depth</code>)<a href="#profundidad-m√°xima-max_depth" class="hash-link" aria-label="Enlace directo al profundidad-m√°xima-max_depth" title="Enlace directo al profundidad-m√°xima-max_depth">‚Äã</a></h3>
<p>Controla cu√°n complejos pueden ser los √°rboles. √Årboles muy profundos tiene m√°s riesgo de overfitting en cada √°rbol
pero el promedio del bosque suele reducirlo</p>
<p>Aun as√≠, limitar <code>max_depth</code> puede:</p>
<ul>
<li>acelerar entrenamiento</li>
<li>mejorar generalizaci√≥n en datasets peque√±os/ruidosos</li>
</ul>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="muestras-m√≠nimas-min_samples_split-min_samples_leaf">Muestras m√≠nimas (<code>min_samples_split</code>, <code>min_samples_leaf</code>)<a href="#muestras-m√≠nimas-min_samples_split-min_samples_leaf" class="hash-link" aria-label="Enlace directo al muestras-m√≠nimas-min_samples_split-min_samples_leaf" title="Enlace directo al muestras-m√≠nimas-min_samples_split-min_samples_leaf">‚Äã</a></h3>
<p>Igual que en un √°rbol:</p>
<ul>
<li><code>min_samples_split</code>: m√≠nimo para dividir un nodo</li>
<li><code>min_samples_leaf</code>: m√≠nimo para que una hoja sea v√°lida</li>
</ul>
<p>Subir estos valores suele:</p>
<ul>
<li>suavizar las predicciones</li>
<li>reducir overfitting</li>
<li>hacer el modelo m√°s estable</li>
</ul>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="n√∫mero-de-variables-por-split-max_features">N√∫mero de variables por split (<code>max_features</code>)<a href="#n√∫mero-de-variables-por-split-max_features" class="hash-link" aria-label="Enlace directo al n√∫mero-de-variables-por-split-max_features" title="Enlace directo al n√∫mero-de-variables-por-split-max_features">‚Äã</a></h3>
<p>Este es uno de los hiperpar√°metros m√°s caracter√≠sticos del Random Forest.</p>
<p>Indica cu√°ntas variables se consideran <strong>en cada split</strong>.</p>
<ul>
<li>Menos variables ‚Üí √°rboles m√°s diferentes ‚Üí mejor ‚Äúdiversidad‚Äù</li>
<li>Demasiadas variables ‚Üí √°rboles m√°s parecidos ‚Üí menos beneficio del ensemble</li>
</ul>
<p>Valores t√≠picos:</p>
<ul>
<li><code>sqrt</code> (muy com√∫n)</li>
<li><code>log2</code></li>
<li>un porcentaje (ej. <code>0.7</code>)</li>
</ul>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ajuste-de-hiperpar√°metros">Ajuste de hiperpar√°metros<a href="#ajuste-de-hiperpar√°metros" class="hash-link" aria-label="Enlace directo al Ajuste de hiperpar√°metros" title="Enlace directo al Ajuste de hiperpar√°metros">‚Äã</a></h3>
<p>Como en otros algoritmos:</p>
<ul>
<li>Se puede usar validaci√≥n cruzada</li>
<li>Es habitual usar <code>GridSearchCV</code></li>
</ul>
<p>Tabla orientativa de rangos para empezar:</p>
<table><thead><tr><th>Tama√±o del dataset</th><th>N¬∫ de registros</th><th><code>n_estimators</code></th><th><code>max_depth</code></th><th><code>min_samples_leaf</code></th><th>Comentario</th></tr></thead><tbody><tr><td>Peque√±o</td><td>&lt; 1.000</td><td>100 ‚Äì 300</td><td>3 ‚Äì 10</td><td>2 ‚Äì 20</td><td>Limitar complejidad para evitar overfitting</td></tr><tr><td>Mediano</td><td>1.000 ‚Äì 10.000</td><td>200 ‚Äì 500</td><td>5 ‚Äì 20</td><td>1 ‚Äì 10</td><td>Buen equilibrio rendimiento/tiempo</td></tr><tr><td>Grande</td><td>&gt; 10.000</td><td>300 ‚Äì 800</td><td>10 ‚Äì None</td><td>1 ‚Äì 5</td><td>M√°s √°rboles y profundidad pueden ayudar</td></tr></tbody></table>
<blockquote>
<p>Estos rangos sirven como punto de partida. Los mejores hiperpar√°metros dependen del dataset y deben ajustarse con validaci√≥n cruzada.</p>
</blockquote>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="importancia-de-variables">Importancia de variables<a href="#importancia-de-variables" class="hash-link" aria-label="Enlace directo al Importancia de variables" title="Enlace directo al Importancia de variables">‚Äã</a></h2>
<p>Random Forest permite calcular <strong>importancia de variables</strong>, normalmente basada en:</p>
<ul>
<li>cu √°nto reduce el error cuando se usa una feature en los splits</li>
<li>promediado a lo largo de todos los √°rboles</li>
</ul>
<p>Esto es √∫til para:</p>
<ul>
<li>interpretaci√≥n parcial del modelo</li>
<li>selecci√≥n de variables</li>
<li>entender qu√© features aportan m√°s informaci√≥n</li>
</ul>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Importancia ‚â† causalidad</div><div class="admonitionContent_BuS1"><p>Que una variable sea importante no significa que sea la causa del fen√≥meno.
Indica que el modelo la usa mucho para reducir el error, pero puede haber correlaciones o variables redundantes.</p></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="m√©tricas-de-evaluaci√≥n">M√©tricas de evaluaci√≥n<a href="#m√©tricas-de-evaluaci√≥n" class="hash-link" aria-label="Enlace directo al M√©tricas de evaluaci√≥n" title="Enlace directo al M√©tricas de evaluaci√≥n">‚Äã</a></h2>
<p>En Random Forest Regression se usan las mismas m√©tricas que en otros modelos de regresi√≥n:</p>
<ul>
<li><strong>MAE</strong> (Mean Absolute Error)</li>
<li><strong>MSE</strong> (Mean Squared Error)</li>
<li><strong>R¬≤</strong> (Coeficiente de determinaci√≥n)</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="flujo-recomendado-en-un-problema-de-random-forest-regresi√≥n">Flujo recomendado en un problema de Random Forest (Regresi√≥n)<a href="#flujo-recomendado-en-un-problema-de-random-forest-regresi√≥n" class="hash-link" aria-label="Enlace directo al Flujo recomendado en un problema de Random Forest (Regresi√≥n)" title="Enlace directo al Flujo recomendado en un problema de Random Forest (Regresi√≥n)">‚Äã</a></h2>
<table><thead><tr><th>Paso</th><th>Qu √© se hace</th><th>Por qu√© es importante</th></tr></thead><tbody><tr><td>1. EDA</td><td>Distribuciones, outliers, nulos</td><td>Asegura calidad de datos</td></tr><tr><td>2. Preprocesamiento</td><td>Limpieza + encoding categ√≥ricas</td><td>No admite nulos, necesita num√©ricos</td></tr><tr><td>3. Entrenamiento</td><td>Ajustar hiperpar√°metros</td><td>Controla rendimiento y coste</td></tr><tr><td>4. Evaluaci√≥n</td><td>MAE, MSE, R¬≤ + gr√°ficos</td><td>Medir generalizaci√≥n</td></tr><tr><td>5. Interpretaci√≥n</td><td>Importancia de variables</td><td>Entender el modelo</td></tr><tr><td>6. Comparaci√≥n</td><td>Comparar con otros modelos</td><td>Elegir el mejor para el dataset</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="ejemplo-random-forest-para-regresi√≥n">Ejemplo: Random Forest para Regresi√≥n<a href="#ejemplo-random-forest-para-regresi√≥n" class="hash-link" aria-label="Enlace directo al Ejemplo: Random Forest para Regresi√≥n" title="Enlace directo al Ejemplo: Random Forest para Regresi√≥n">‚Äã</a></h2>
<p>Para ver c√≥mo funciona un <strong>Random Forest Regressor</strong> en la pr√°ctica, puedes ejecutar este ejemplo utilizando el dataset <strong>California Housing</strong>.</p>
<p>üëâ <strong>Puedes abrir el cuaderno aqu√≠:</strong>
<a href="/DevTacora/assets/files/ejemplo_random_forest_regresion-e01cd51d2cf389acf84830ce5b5ef844.ipynb" target="_blank">Colab: Random Forest Regression</a></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="actividad-de-seguimiento-bike-sharing-dataset">Actividad de seguimiento: Bike Sharing Dataset<a href="#actividad-de-seguimiento-bike-sharing-dataset" class="hash-link" aria-label="Enlace directo al Actividad de seguimiento: Bike Sharing Dataset" title="Enlace directo al Actividad de seguimiento: Bike Sharing Dataset">‚Äã</a></h2>
<p>Utiliza el <strong>Bike Sharing Dataset</strong> y compara:</p>
<ul>
<li>Regresi√≥n Lineal</li>
<li>KNN Regresi√≥n</li>
<li>√Årbol de Decisi√≥n (Regresi√≥n)</li>
<li>Random Forest (Regresi√≥n)</li>
</ul>
<p>Incluye:</p>
<ul>
<li>Ajuste de hiperpar√°metros (<code>n_estimators</code>, <code>max_depth</code>, <code>max_features</code>, etc.)</li>
<li>M√©tricas de evaluaci√≥n</li>
<li>Importancia de variables</li>
<li>Conclusiones razonadas</li>
</ul>
<p><strong>Entrega:</strong> Notebook (Colab) con conclusiones claras y justificadas.</p></div></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="P√°gina del documento"><a class="pagination-nav__link pagination-nav__link--prev" href="/DevTacora/docs/pia_2526/ut3_ml/aprendizaje-supervisado/regresion/arbol_decision_regresion"><div class="pagination-nav__sublabel">Anterior</div><div class="pagination-nav__label">Decision Trees Regresi√≥n</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/DevTacora/docs/pia_2526/ut3_ml/aprendizaje-supervisado/regresion/regresion_ridge_lasso"><div class="pagination-nav__sublabel">Siguiente</div><div class="pagination-nav__label">Ridge y Lasso Regression</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#idea-principal-del-algoritmo" class="table-of-contents__link toc-highlight">Idea principal del algoritmo</a></li><li><a href="#funcionamiento-interno-del-modelo" class="table-of-contents__link toc-highlight">Funcionamiento interno del modelo</a><ul><li><a href="#paso-1-bootstrap-muestreo-con-reemplazo" class="table-of-contents__link toc-highlight">Paso 1: Bootstrap (muestreo con reemplazo)</a></li><li><a href="#paso-2-entrenar-un-√°rbol-con-submuestreo-de-variables" class="table-of-contents__link toc-highlight">Paso 2: Entrenar un √°rbol con submuestreo de variables</a></li><li><a href="#paso-3-repetir-para-crear-muchos-√°rboles" class="table-of-contents__link toc-highlight">Paso 3: Repetir para crear muchos √°rboles</a></li></ul></li><li><a href="#entrenamiento-vs-predicci√≥n" class="table-of-contents__link toc-highlight">Entrenamiento vs predicci√≥n</a><ul><li><a href="#entrenamiento" class="table-of-contents__link toc-highlight">Entrenamiento</a></li><li><a href="#predicci√≥n" class="table-of-contents__link toc-highlight">Predicci√≥n</a></li></ul></li><li><a href="#random-forest-en-regresi√≥n-vs-clasificaci√≥n" class="table-of-contents__link toc-highlight">Random Forest en regresi√≥n vs clasificaci√≥n</a></li><li><a href="#uso-de-random-forest-en-regresi√≥n" class="table-of-contents__link toc-highlight">Uso de Random Forest en Regresi√≥n</a><ul><li><a href="#cu√°ndo-s√≠-usarlos" class="table-of-contents__link toc-highlight">Cu√°ndo S√ç usarlos</a></li><li><a href="#cu√°ndo-no-funcionan-bien" class="table-of-contents__link toc-highlight">Cu√°ndo NO funcionan bien</a></li></ul></li><li><a href="#importancia-del-preprocesamiento" class="table-of-contents__link toc-highlight">Importancia del preprocesamiento</a></li><li><a href="#principales-hiperpar√°metros" class="table-of-contents__link toc-highlight">Principales hiperpar√°metros</a><ul><li><a href="#n√∫mero-de-√°rboles-n_estimators" class="table-of-contents__link toc-highlight">N√∫mero de √°rboles (<code>n_estimators</code>)</a></li><li><a href="#profundidad-m√°xima-max_depth" class="table-of-contents__link toc-highlight">Profundidad m√°xima (<code>max_depth</code>)</a></li><li><a href="#muestras-m√≠nimas-min_samples_split-min_samples_leaf" class="table-of-contents__link toc-highlight">Muestras m√≠nimas (<code>min_samples_split</code>, <code>min_samples_leaf</code>)</a></li><li><a href="#n√∫mero-de-variables-por-split-max_features" class="table-of-contents__link toc-highlight">N√∫mero de variables por split (<code>max_features</code>)</a></li><li><a href="#ajuste-de-hiperpar√°metros" class="table-of-contents__link toc-highlight">Ajuste de hiperpar√°metros</a></li></ul></li><li><a href="#importancia-de-variables" class="table-of-contents__link toc-highlight">Importancia de variables</a></li><li><a href="#m√©tricas-de-evaluaci√≥n" class="table-of-contents__link toc-highlight">M√©tricas de evaluaci√≥n</a></li><li><a href="#flujo-recomendado-en-un-problema-de-random-forest-regresi√≥n" class="table-of-contents__link toc-highlight">Flujo recomendado en un problema de Random Forest (Regresi√≥n)</a></li><li><a href="#ejemplo-random-forest-para-regresi√≥n" class="table-of-contents__link toc-highlight">Ejemplo: Random Forest para Regresi√≥n</a></li><li><a href="#actividad-de-seguimiento-bike-sharing-dataset" class="table-of-contents__link toc-highlight">Actividad de seguimiento: Bike Sharing Dataset</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">
        <section classname="bg-white border-t border-gray-100">
      <div classname="max-w-4xl mx-auto px-6 py-16 text-center">
        <h3 classname="text-2xl font-bold text-gray-900 mb-4">
          Construyendo conocimiento, l√≠nea por l√≠nea
        </h3>
        <p classname="text-gray-600 leading-relaxed">
          ¬© 2026 Alicia C√°mara Casares - Contenido bajo licencia 
              <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener noreferrer" classname="text-blue-600 hover:text-blue-700 underline">
                CC BY-NC-SA 4.0
              </a>.
        </p>
      </div>
    </section>
      </div></div></div></footer></div>
</body>
</html>