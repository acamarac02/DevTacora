---
title: "Deep Learning"
sidebar_position: 4
description: "Introducci√≥n al Deep Learning: qu√© son las redes neuronales profundas, c√≥mo funcionan, principales tipos (MLP, CNN, RNN, Transformers), ejemplos de aplicaci√≥n y una demo interactiva con QuickDraw."
keywords:
  ["inteligencia artificial","IA","machine learning","deep learning","aprendizaje profundo","redes neuronales","CNN","RNN","Transformers","visi√≥n por computador","NLP","IA generativa","QuickDraw"]
---


El **Deep Learning (Aprendizaje Profundo)** es una rama del *Machine Learning* que ha revolucionado la Inteligencia Artificial en la √∫ltima d√©cada. Su √©xito ha hecho posible avances espectaculares en reconocimiento de im√°genes, traducci√≥n autom√°tica, coches aut√≥nomos o chatbots como ChatGPT.

* Se basa en **redes neuronales artificiales** con muchas capas (*deep = profundo*).
* Cada capa transforma los datos un poco m√°s hasta que, al final, el sistema es capaz de reconocer patrones muy complejos.
* La inspiraci√≥n original vino del cerebro humano, aunque las redes artificiales son mucho m√°s simples que las biol√≥gicas.

:::tip DIFERENCIA RECONOCIMIENTO DE IM√ÅGENES CON ML CL√ÅSICO VS REDES NEURONALES
En **Machine Learning cl√°sico**, el proceso se basaba en **feature engineering manual**:

* Si quer√≠as que un algoritmo reconociera gatos en fotos, primero **deb√≠as programar t√∫ qu√© caracter√≠sticas mirar**: bordes, colores, texturas, formas, tama√±os‚Ä¶
* El modelo (ej. un SVM o un √°rbol de decisi√≥n) luego usaba esas caracter√≠sticas para aprender.
  üëâ Si tus *features* eran malas, el modelo no funcionaba bien.

En cambio, en **Deep Learning**, la red neuronal **aprende autom√°ticamente las features intermedias**:

* Las primeras capas detectan patrones simples (ej. bordes en una imagen).
* Las siguientes capas combinan esos patrones en estructuras m√°s complejas (ej. orejas, patas).
* Las √∫ltimas capas entienden el concepto completo (ej. ‚Äúgato‚Äù).
  üëâ El humano ya no necesita decidir qu√© variables son importantes: la red las descubre sola.
:::

---

## C√≥mo funciona una red neuronal (idea b√°sica)

1. **Neuronas artificiales**

   * Inspiradas en las neuronas biol√≥gicas, pero mucho m√°s simples.
   * Cada neurona recibe varios n√∫meros de entrada (features), los multiplica por unos valores llamados **pesos**, suma todo, aplica una **funci√≥n de activaci√≥n** y produce una salida.
   * üëâ Ejemplo: una neurona podr√≠a recibir el brillo y el color de un p√≠xel de una imagen y dar como salida ‚Äúprobabilidad de que sea parte de un borde‚Äù.

2. **Capas de neuronas**

   * Una red neuronal conecta muchas neuronas en **capas**:

     * **Capa de entrada**: recibe los datos originales (ej. los p√≠xeles de una foto).
     * **Capas ocultas**: transforman la informaci√≥n paso a paso.
     * **Capa de salida**: da la predicci√≥n final (ej. ‚Äúgato‚Äù o ‚Äúperro‚Äù).
   * Al apilar muchas capas, la red puede **aprender representaciones jer√°rquicas**:

     * Bordes ‚Üí formas ‚Üí partes del objeto ‚Üí objeto completo.

3. **Entrenamiento de la red**

   * La red empieza con pesos aleatorios (al principio no acierta nada).
   * Cada vez que ve un ejemplo, compara su predicci√≥n con la respuesta real (**error o funci√≥n de p√©rdida**).
   * Gracias al algoritmo de **backpropagation** y al uso de **optimizadores** (ej. *gradient descent*), los pesos se ajustan un poquito para reducir el error.
   * Repitiendo este proceso con miles o millones de ejemplos, la red va mejorando hasta reconocer patrones muy complejos.

üëâ **Analog√≠a sencilla:**
Una red neuronal es como un **equipo de filtros encadenados**. Cada capa pule un poco m√°s la informaci√≥n: de lo m√°s simple (l√≠neas y colores) a lo m√°s complejo (caras, palabras o significados).

![Red neuronal](../0-img/red-neuronal.png)

**üîπ Ejemplo de una red neuronal: Reconocimiento de d√≠gitos manuscritos (MNIST)**

* **Datos de entrada:** im√°genes 28√ó28 p√≠xeles en blanco y negro (784 neuronas de entrada).
* **Capas ocultas:** 1 o 2 capas con 128‚Äì256 neuronas. (El n√∫mero de capas y las neuronas de cada uno lo eliges t√∫, son hiperpar√°metros)
* **Capa de salida:** 10 neuronas (una por cada d√≠gito del 0 al 9).
  üëâ Red peque√±a, se puede entrenar en un port√°til en pocos minutos.

:::tip Demo interactiva: QuickDraw

Para ver en acci√≥n c√≥mo una red neuronal aprende a reconocer patrones, usaremos:
üëâ [QuickDraw de Google](https://quickdraw.withgoogle.com/)

* Es un juego en el que dibujas un objeto (ej. una bicicleta, un sol, un perro) en pocos segundos.
* Una red neuronal trata de adivinar lo que dibujaste bas√°ndose en millones de ejemplos recopilados de usuarios.
* Muestra c√≥mo el **Deep Learning puede reconocer dibujos imperfectos** y generalizar a partir de datos ruidosos.

üëâ El modelo **no memoriza dibujos concretos**, sino que aprende los patrones comunes (formas, trazos) de cada categor√≠a.
:::

---

## Tipos de redes neuronales

Aunque todas se basan en el mismo principio (neuronas conectadas en capas), existen arquitecturas diferentes seg√∫n el tipo de datos y el problema a resolver:

### Redes totalmente conectadas (MLP ‚Äì *Multilayer Perceptron*)

Cada neurona de una capa est√° conectada con *todas* las neuronas de la capa siguiente.

* **Aplicaciones t√≠picas:**

  * Datos tabulares (ej. predicci√≥n de ventas con variables como precio, publicidad, temporada).
  * Problemas sencillos de clasificaci√≥n o regresi√≥n.


### Redes convolucionales (CNN ‚Äì *Convolutional Neural Networks*)

Utilizan capas de **convoluci√≥n**, que aplican filtros sobre regiones locales de la imagen.
As√≠ pueden detectar patrones simples (bordes, colores) y combinarlos en otros m√°s complejos (formas, objetos).

* **Aplicaciones t√≠picas:**

  * Visi√≥n por computador: reconocimiento facial, detecci√≥n de objetos en fotos o v√≠deos, coches aut√≥nomos.
  * Medicina: an√°lisis de radiograf√≠as o resonancias.
  * Sistemas de vigilancia y biometr√≠a.

### Redes recurrentes (RNN ‚Äì *Recurrent Neural Networks*)

Incorporan **conexiones de retroalimentaci√≥n**, lo que les permite ‚Äúrecordar‚Äù informaci√≥n previa. Son ideales para trabajar con **secuencias** de datos.

* **Aplicaciones t√≠picas:**

  * Procesamiento de lenguaje natural (NLP): an√°lisis de sentimientos, chatbots tempranos.
  * Reconocimiento de voz.
  * Series temporales: predicci√≥n de la bolsa, demanda energ√©tica.

### Transformers

Introducen el mecanismo de **atenci√≥n**, que permite a la red enfocarse en las partes m√°s relevantes de una secuencia de datos. Superan las limitaciones de las RNN porque procesan secuencias en paralelo, de manera mucho m√°s eficiente.

* **Aplicaciones t√≠picas:**

  * **NLP moderno:** traducci√≥n autom√°tica (Google Translate), modelos de texto (ChatGPT, BERT, GPT-4).
  * **Visi√≥n por computador:** *Vision Transformers (ViT)* para reconocimiento de im√°genes.
  * **IA generativa:** modelos de texto, imagen, m√∫sica y v√≠deo.

---


## Limitaciones del Deep Learning

Aunque el **Deep Learning** ha impulsado avances impresionantes, tambi√©n tiene limitaciones importantes que conviene conocer:

1. **Necesita muchos datos**

   * Cuanto m√°s grande y variado es el dataset, mejor aprende la red.
   * En problemas con pocos ejemplos, el modelo puede fallar porque no generaliza bien.
   * üëâ Ejemplo: entrenar un sistema de diagn√≥stico m√©dico con solo 100 radiograf√≠as ser√≠a insuficiente; har√≠an falta miles o millones para que funcione bien.

2. **Gran consumo computacional**

   * Entrenar redes profundas requiere **GPUs o TPUs**, que son caras y consumen mucha energ√≠a.
   * Esto limita a empresas o instituciones con gran infraestructura, dejando fuera a quienes no tienen esos recursos.
   * üëâ Ejemplo: entrenar GPT-3 cost√≥ millones de d√≥lares en hardware y electricidad.

3. **Falta de explicabilidad**

   * Las redes neuronales son a menudo una ‚Äúcaja negra‚Äù: sabemos la entrada y la salida, pero no siempre entendemos bien c√≥mo lleg√≥ a esa decisi√≥n.
   * Esto es un problema en √°mbitos cr√≠ticos (medicina, justicia, finanzas), donde es necesario justificar una predicci√≥n.
   * üëâ Ejemplo: un banco que niega un pr√©stamo debe poder explicar el motivo, pero un modelo profundo solo da el resultado sin un razonamiento claro.

4. **Sesgos en los datos**

   * Si los datos de entrenamiento contienen prejuicios o desigualdades, el modelo los reproducir√° (e incluso los amplificar√°).
   * üëâ Ejemplo: un sistema de reconocimiento facial entrenado mayoritariamente con rostros de personas blancas puede fallar m√°s con rostros de personas de otras etnias.

5. **Riesgo de sobreajuste (*overfitting*)**

   * Si el modelo es demasiado complejo para los datos disponibles, puede aprender de memoria los ejemplos en vez de generalizar.
   * üëâ Ejemplo: un modelo que acierta el 100% en los datos de entrenamiento pero falla con datos nuevos.

6. **Dependencia del contexto y cambios en los datos** (*data drift*)

   * El modelo puede dejar de ser v√°lido si las condiciones cambian con el tiempo.
   * üëâ Ejemplo: un detector de spam entrenado en 2020 puede fallar en 2023 porque los spammers inventaron nuevas t√©cnicas.



