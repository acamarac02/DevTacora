---
title: "Random Forest"
sidebar_position: 3
toc_max_heading_level: 5
description: "Introducci√≥n a Random Forest aplicado a problemas de clasificaci√≥n. C√≥mo funciona un bosque de √°rboles, por qu√© mejora a un √Årbol de Decisi√≥n individual, sus hiperpar√°metros principales y c√≥mo entrenarlo y evaluarlo en Python utilizando el dataset Iris."
keywords: [Random Forest, Bagging, Ensamble, Clasificaci√≥n, Machine Learning, scikit-learn, iris, √°rboles, hiperpar√°metros]
---

<div class="justify-text">

Los **Random Forest** (Bosques Aleatorios) son una mejora directa de los √Årboles de Decisi√≥n.
En lugar de entrenar **un solo √°rbol**, que puede ser inestable y propenso al sobreajuste, Random Forest crea **muchos √°rboles diferentes** y combina sus predicciones mediante **votaci√≥n**.

La idea es sencilla pero muy poderosa:

> **Un grupo de modelos simples y diversos suele predecir mejor que un √∫nico modelo complejo.**

Este principio se conoce como **sabidur√≠a de la multitud**: cuando muchas ‚Äúopiniones‚Äù independientes se combinan, los errores individuales se compensan y la predicci√≥n final es m√°s estable.

:::tip Video recomendado
Si quieres una explicaci√≥n muy clara e intuitiva de c√≥mo funciona Random Forest, te recomiendo este v√≠deo:

üëâ [C√≥mo funciona Random Forest (YouTube)](https://www.youtube.com/watch?v=v6VJ2RO66Ag)

En menos de 10 minutos muestra visualmente la idea de los √°rboles aleatorios, el bagging y la votaci√≥n del bosque.
:::


---

## ¬øPor qu√© surge Random Forest?

Los √Årboles de Decisi√≥n tienen muchas ventajas (interpretabilidad, facilidad de uso‚Ä¶), pero tambi√©n un problema importante:

> **Si los dejas crecer sin l√≠mites, tienden a sobreajustar los datos.**

Un √∫nico √°rbol aprende reglas demasiado espec√≠ficas, se vuelve muy sensible a peque√±as variaciones y generaliza mal. Random Forest aparece como soluci√≥n:

* crea muchos √°rboles **distintos**
* cada uno ve una parte diferente de los datos
* cada √°rbol tambi√©n usa un subconjunto aleatorio de caracter√≠sticas
* despu√©s, todos los √°rboles votan la clase final

El resultado:   
‚úî menos sobreajuste   
‚úî modelo m√°s estable   
‚úî mejores predicciones en datos nuevos

Los Random Forest se basan en el concepto de **sabidur√≠a de la multitud**:

![Random Forest](../../0-img/random-forest-sabiduria.png)

Imagina que un √Årbol de Decisi√≥n es una persona haciendo una predicci√≥n. 

Si solo preguntas a una persona, puede equivocarse.   
Pero si preguntas a **100 personas independientes** y haces una votaci√≥n:

* los errores se compensan
* la predicci√≥n final suele ser muy acertada

Random Forest funciona igual:
**cada √°rbol es un ‚Äúvotante‚Äù**, y la predicci√≥n final es la decisi√≥n mayoritaria.

---

## ¬øPor qu√© usar Random Forest vs √Årboles de decisi√≥n?

Random Forest supera a un √Årbol de Decisi√≥n individual en casi todos los aspectos importantes:

* **M√°s robusto**   
    Peque√±os cambios en los datos no afectan tanto.   
    Cada √°rbol ve una versi√≥n diferente del dataset, as√≠ que el modelo final es estable.

* **Menos sobreajuste**  
    Promediar muchos √°rboles reduce la varianza del modelo.   
    Es decir, **memorizan menos y generalizan mejor**.

* **Mejor rendimiento en general**   
    En la mayor√≠a de datasets, Random Forest obtiene:
    * mayor accuracy
    * menor error
    * mejores predicciones en test

* **Combina varios modelos d√©biles en un modelo fuerte**   
    Cada √°rbol es simple y puede cometer errores.   
    Pero juntos ‚Äîgracias al m√©todo de *bagging*‚Äî forman un modelo m√°s potente y preciso.   

---

## Funcionamiento del modelo

Random Forest no es un modelo misterioso:
es simplemente un conjunto de **muchos √Årboles de Decisi√≥n**, entrenados de manera inteligente para que sean **diferentes entre s√≠** y, al combinarlos, formen un modelo final m√°s robusto.

La clave est√° en dos ideas:
**bagging** y **aleatoriedad**.

---

### Bagging (Bootstrap Aggregation)

El primer ingrediente de Random Forest es el **bagging**, una t√©cnica cuyo objetivo es **reducir la varianza** del modelo.

El proceso es:

1. Se crea un **subconjunto de datos** tomando muestras *con reemplazo* (bootstrap).
2. Con ese subconjunto, se entrena un √°rbol.
3. Se repite el proceso muchas veces.
4. Cada √°rbol aprende cosas ligeramente diferentes.
5. Al final, todas las predicciones se combinan (votaci√≥n).

Esto hace que cada √°rbol sea imperfecto en una forma distinta, pero **la combinaci√≥n final sea muy estable**.

---

### Sampling aleatorio de datos

Para cada √°rbol del bosque:

* El modelo elige de forma aleatoria un subconjunto de observaciones del dataset original.
* Como es sampling *con reemplazo*, algunas observaciones se repiten y otras no aparecen en ese √°rbol.

Resultado:

> **Cada √°rbol ve una versi√≥n diferente del dataset**, lo que introduce diversidad en el bosque.

---

### Sampling aleatorio de features

Adem√°s de elegir datos distintos, Random Forest tambi√©n elige **features distintas** en cada split de cada √°rbol.

Por ejemplo:

* Un √°rbol puede basarse en *petal_length* y *sepal_width*
* Otro puede usar *petal_width* y *sepal_length*
* Otro puede ignorar completamente *sepal_width*

Esta aleatoriedad adicional es fundamental:

‚úî evita que todos los √°rboles aprendan lo mismo  
‚úî fuerza a que los √°rboles busquen caminos alternativos   
‚úî mejora la generalizaci√≥n del modelo   

**Cada √°rbol ve un mundo diferente**, combinando:

* bootstrap (datos diferentes)
* selecci√≥n aleatoria de columnas (features diferentes)

se consigue que **cada √°rbol tenga su propia perspectiva** sobre el problema.

> Ning√∫n √°rbol es perfecto, pero sus errores NO son los mismos.
> Esto permite que la votaci√≥n final sea mucho m√°s precisa.

---

### Votaci√≥n del bosque: predicci√≥n final

Una vez entrenados todos los √°rboles:

1. Cada √°rbol hace su predicci√≥n de forma independiente.
2. En clasificaci√≥n, se utiliza **votaci√≥n mayoritaria**:
   ‚Üí la clase m√°s votada por todos los √°rboles es la predicci√≥n final.
3. En regresi√≥n, se utiliza la **media** de las predicciones.

Esta combinaci√≥n final es el coraz√≥n del Random Forest:

> **promediar varios modelos d√©biles produce un modelo fuerte, estable y dif√≠cil de sobreajustar.**

![Random Forest](../../0-img/rf-votacion-final.png)


---

## Importancia del preprocesamiento en Random Forest

Aunque Random Forest es uno de los modelos **menos exigentes** en preprocesamiento, conviene recordar algunas reglas b√°sicas para evitar errores y garantizar un buen rendimiento.

<div class="texto-sin-justificar">

| Aspecto                                  | ¬øEs necesario?                               | Explicaci√≥n                                                                                                                                                            |
| ---------------------------------------- | -------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Escalado (StandardScaler / MinMax)**   | **‚ùå No**                                     | Igual que los √°rboles individuales, Random Forest solo compara valores del tipo ‚Äú¬øfeature ‚â§ umbral?‚Äù, as√≠ que las escalas no afectan al modelo.                        |
| **Codificaci√≥n de categ√≥ricas**          | **‚úî S√≠**                                     | El modelo solo acepta n√∫meros. Puedes usar **Label Encoding o One-Hot**, ambas funcionan porque el √°rbol no interpreta orden.                                          |
| **Tratamiento de outliers**              | **‚úî Opcional** (recomendado si son extremos) | Random Forest es robusto a ruido, pero outliers muy extremos pueden influir en splits poco √≥ptimos en algunos √°rboles. No es tan cr√≠tico como en KNN, SVM o regresi√≥n. |
| **Eliminaci√≥n de nulos**                 | **‚úî S√≠**                                     | Random Forest no admite valores nulos. Deben rellenarse (mean/median/mode) o eliminarse filas.                                                                         |
| **Eliminaci√≥n de features irrelevantes** | **‚úî Recomendado**                            | Un exceso de features irrelevantes hace que los √°rboles prueben divisiones menos √∫tiles. No es grave, pero puede empeorar la precisi√≥n.                                |
| **One-Hot vs Label Encoding**            | **‚úî Cualquiera**                             | A diferencia de KNN, la elecci√≥n no cambia el significado para el modelo. Label Encoding funciona bien incluso sin orden real.                                         |

</div>

---

## Ejemplo b√°sico en Python

Para ver c√≥mo funciona un **Random Forest** en la pr√°ctica, puedes ejecutar este ejemplo sencillo utilizando el dataset **Iris**.
Entrenaremos un Random Forest **sin ajustar hiperpar√°metros**.

üëâ **Puedes abrir el cuaderno aqu√≠:**
[Colab: Random Forest con Iris](../../0-datasets/ejemplo_random_forest_iris.ipynb)

üëâ **Dataset utilizado:**
[iris.csv](../../0-datasets/iris.csv)

---

## Visualizaci√≥n de los √°rboles

En un Random Forest se entrenan **decenas o cientos de √°rboles**, cada uno ligeramente distinto gracias al *bootstrap* y a la selecci√≥n aleatoria de features.

Por ello:

* **No tiene sentido visualizar todo el bosque** ‚Üí ser√≠a imposible de interpretar.
* **S√≠ es √∫til visualizar un par de √°rboles individuales** para ver c√≥mo cada uno aprende reglas diferentes.

Esto ayuda a entender que:

> **El bosque no depende de un √∫nico √°rbol, sino de la votaci√≥n de muchos modelos diversos.**

Si quieres, puedes mostrar solo los primeros √°rboles:

```python
# Importamos las librer√≠as necesarias
import matplotlib.pyplot as plt
from sklearn import tree

# Creamos una figura con 3 subplots (uno por cada √°rbol)
plt.figure(figsize=(30, 15))

# Recorremos los primeros 3 √°rboles del Random Forest
for i in range(3):
    # Seleccionamos el subplot en la posici√≥n i+1
    plt.subplot(1, 3, i + 1)
    
    # Dibujamos el √°rbol n√∫mero i dentro del bosque
    tree.plot_tree(
        rf.estimators_[i],      # √Årbol individual dentro de Random Forest
        feature_names=X.columns,  # Nombres de las columnas para legibilidad
        filled=True,              # Colorear nodos seg√∫n clase predominante
        max_depth=3               # Limitar profundidad para que el √°rbol no sea gigantesco
    )
    
    # T√≠tulo para identificar qu√© √°rbol estamos viendo
    plt.title(f"√Årbol {i}")

# Mostramos todos los gr√°ficos
plt.show()
```

![Random Forest](../../0-img/random-forest-example.png)


---

## Hiperpar√°metros principales

Despu√©s de ver el funcionamiento b√°sico del Random Forest, es importante entender c√≥mo podemos controlar su comportamiento para evitar sobreajuste, mejorar la estabilidad del modelo y ajustar su rendimiento.

En un Random Forest, los hiperpar√°metros ya no controlan **un √∫nico √°rbol**, sino **c√≥mo se comportan todos los √°rboles del bosque**.
Los m√°s importantes son los que permiten regular:

* cu√°ntos √°rboles se entrenan
* cu√°n complejos pueden ser esos √°rboles
* cu√°nta aleatoriedad a√±adimos
* cu√°n homog√©neas deben ser las hojas del bosque

---

### `n_estimators` ‚Äî n√∫mero de √°rboles del bosque

Es uno de los hiperpar√°metros clave del modelo.

Indica **cu√°ntos √°rboles individuales** va a entrenar el Random Forest.

**Idea intuitiva:**

* M√°s √°rboles ‚Üí modelo m√°s estable
* Menos √°rboles ‚Üí modelo m√°s r√°pido, pero menos robusto
* No existe riesgo real de sobreajuste por poner demasiados √°rboles

Ejemplo:

* `n_estimators=10` ‚Üí bosque muy peque√±o, poco estable
* `n_estimators=100` ‚Üí valor t√≠pico
* `n_estimators=300` ‚Üí excelente estabilidad en la mayor√≠a de casos

**Regla pr√°ctica:**

> ‚ÄúPon m√°s √°rboles si quieres un modelo m√°s estable. El √∫nico coste es el tiempo de entrenamiento.‚Äù

---

### `max_depth` ‚Äî profundidad m√°xima de cada √°rbol

Este hiperpar√°metro funciona de manera casi id√©ntica a como lo viste en √Årbol de Decisi√≥n.

Controla **cu√°n profundos** pueden ser los √°rboles del bosque.

* √Årboles muy profundos ‚Üí riesgo de *overfitting* individual
* √Årboles muy poco profundos ‚Üí bosque demasiado simple (*underfitting*)
* Lo normal es permitir √°rboles relativamente profundos, pero no infinitos

En Random Forest, incluso con √°rboles profundos, el modelo **no sobreajusta tanto** gracias a la aleatoriedad del bosque.
A√∫n as√≠, limitar la profundidad suele mejorar la generalizaci√≥n.

**Regla pr√°ctica:**

> ‚ÄúPara datasets peque√±os como Iris, profundidades entre 4 y 6 funcionan muy bien.‚Äù

---

### `min_samples_leaf` ‚Äî muestras m√≠nimas en cada hoja

Indica el **n√∫mero m√≠nimo de observaciones** que debe contener una hoja del √°rbol.

* Valores muy peque√±os (1‚Äì2) ‚Üí √°rboles muy espec√≠ficos
* Valores m√°s altos ‚Üí reglas m√°s estables y menos sensibles al ruido

Es uno de los hiperpar√°metros m√°s importantes para **suavizar** un Random Forest.

**Regla pr√°ctica:**

> ‚ÄúValores 1‚Äì2 funcionan bien para datasets peque√±os. Para datasets ruidosos, usa 3‚Äì5.‚Äù

---

### `max_features` ‚Äî n√∫mero de columnas que cada √°rbol puede usar

Este hiperpar√°metro es **la clave que diferencia un Bagging normal de un Random Forest**.

Determina cu√°ntas **features** puede usar cada √°rbol en **cada divisi√≥n del √°rbol**:

* Valores bajos ‚Üí √°rboles m√°s distintos entre s√≠ ‚Üí m√°s diversidad ‚Üí **menos sobreajuste**
* Valores altos ‚Üí √°rboles m√°s parecidos ‚Üí **m√°s riesgo de sobreajuste**

Valores t√≠picos:

* `"sqrt"` ‚Üí usa la ra√≠z cuadrada del n√∫mero de features
* `"log2"` ‚Üí usa el logaritmo
* o elegir un n√∫mero fijo (`1`, `2`, `3`)

Para Iris (4 features), `"sqrt"` equivale a 2 columnas por split ‚Üí suele ir muy bien.

**Regla pr√°ctica:**

> ‚Äú`max_features="sqrt"` es el valor est√°ndar y suele funcionar de maravilla.‚Äù

---

### ¬øC√≥mo saber si hay overfitting o underfitting?

Al igual que en √Årbol de Decisi√≥n, la forma m√°s sencilla de detectar si un Random Forest est√° sobreajustando o infraajustando es comparar la **accuracy en train** con la **accuracy en test**.

**OVERFITTING (sobreajuste)**

El modelo aprende demasiado bien el entrenamiento y generaliza peor.

Se√±ales:

* Accuracy **muy alta en train** (a veces 1.00)
* Accuracy **claramente m√°s baja en test**

Ejemplo t√≠pico:

| Conjunto | Accuracy |
| -------- | -------- |
| Train    | 1.00     |
| Test     | 0.90     |

Interpretaci√≥n:
Los √°rboles del bosque son demasiado complejos. Aunque el Random Forest reduce el sobreajuste respecto a un solo √°rbol, todav√≠a puede memorizar parte de los datos.

**UNDERFITTING (subajuste)**

El modelo es demasiado simple.

Se√±ales:

* Accuracy **baja en train**
* Accuracy **similarmente baja en test**

Ejemplo:

| Conjunto | Accuracy |
| -------- | -------- |
| Train    | 0.80     |
| Test     | 0.78     |

Interpretaci√≥n:
El bosque es peque√±o (`n_estimators` muy bajo) o los √°rboles son demasiado poco profundos.

**Buen ajuste**

Lo ideal:

* train y test **parecidos**
* ambos valores **altos**

Ejemplo:

| Conjunto | Accuracy |
| -------- | -------- |
| Train    | 0.97     |
| Test     | 0.95     |

Interpretaci√≥n:
El modelo es estable, generaliza bien y no memoriza en exceso.
Esto es lo habitual en Random Forest bien configurado.

---

**C√≥digo para comprobarlo**

Exactamente lo mismo que estudiamos para los √°rboles de decisi√≥n:

```python
# Accuracy en entrenamiento y en test
print("Accuracy TRAIN:", rf.score(X_train, y_train))
print("Accuracy TEST :", rf.score(X_test, y_test))
```

---

## Ejemplo con ajuste de hiperpar√°metros

En este segundo ejemplo trabajaremos con el mismo **Google Colab** y el mismo **dataset Iris** utilizados en el ejemplo b√°sico.
El objetivo ahora es observar c√≥mo los hiperpar√°metros m√°s importantes de un Random Forest (`n_estimators`, `max_depth`, `min_samples_leaf`, `max_features`) influyen en el rendimiento del modelo.

En el Colab encontrar√°s un apartado espec√≠fico llamado **‚ÄúAjuste de hiperpar√°metros‚Äù**.

---

## M√©tricas de evaluaci√≥n

Las m√©tricas que utilizamos para evaluar un Random Forest son **exactamente las mismas** que ya estudiamos en √Årbol de Decisi√≥n y en KNN:

* **Accuracy**
* **Matriz de confusi√≥n**
* **Precision, Recall y F1-score** (classification_report)

---

## An√°lisis de importancia

Igual que con los √Årboles de Decisi√≥n, un Random Forest permite obtener la **importancia de cada variable** mediante el atributo:

```python
rf_tuned.feature_importances_
```

La idea y la interpretaci√≥n son exactamente las mismas que ya estudiamos.

---

## Automatizaci√≥n b√∫squeda de hiperpar√°metros (GridSearchCV)

Igual que en el √Årbol de Decisi√≥n, podemos utilizar **GridSearchCV** para encontrar la mejor combinaci√≥n de hiperpar√°metros de un **Random Forest**.
El funcionamiento es el mismo: GridSearchCV prueba todas las combinaciones posibles del diccionario de par√°metros y selecciona la que obtiene mejor rendimiento mediante validaci√≥n cruzada.

La √∫nica diferencia es que en Random Forest los hiperpar√°metros que ajustamos suelen ser otros (como `n_estimators`, `max_depth`, `max_features` o `min_samples_leaf`).

---

### Ejemplo de diccionario de hiperpar√°metros

Ajustamos solo los hiperpar√°metros m√°s importantes:

```python
param_grid = {
    "n_estimators": [100, 200, 300],   # n¬∫ de √°rboles
    "max_depth": [3, 4, 5, None],      # profundidad m√°xima
    "min_samples_leaf": [1, 2, 3],     # hojas m√≠nimas
    "max_features": [1, 2, "sqrt"]     # n¬∫ de columnas usadas en cada split
}
```

‚ö†Ô∏è Igual que antes, **no pongas listas enormes**:
cada valor extra multiplica el n√∫mero de modelos que se entrenan.

---

### Ejecutar GridSearchCV con Random Forest

```python
from sklearn.model_selection import GridSearchCV

# Modelo base
rf = RandomForestClassifier(random_state=42)

# Configuramos GridSearchCV
grid = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    cv=5,                  # Validaci√≥n cruzada 5-fold
    scoring="accuracy",    # M√©trica a optimizar
    n_jobs=-1              # Usa todos los n√∫cleos disponibles (opcional)
)

# Entrenamos la b√∫squeda de hiperpar√°metros
grid.fit(X_train, y_train)

# Mostramos la mejor combinaci√≥n encontrada
print("Mejores hiperpar√°metros:", grid.best_params_)

# Obtenemos directamente el Random Forest √≥ptimo
best_rf = grid.best_estimator_
```

`best_rf` es el modelo √≥ptimo ya entrenado, listo para evaluar en test o para analizar importancias de variables.

---

## Flujo recomendado en un problema de Random Forest

El proceso para resolver un problema con **Random Forest** se basa en los mismos pasos que los √Årboles de Decisi√≥n, pero con el a√±adido de ajustar los hiperpar√°metros que controlan el comportamiento del bosque completo. Random Forest es robusto, maneja bien el ruido y suele generalizar mejor que un √∫nico √°rbol.

<div class="texto-sin-justificar">

| Paso                                                 | ¬øQu√© se hace?                                                                                                         | ¬øPor qu√© es importante?                                                                                              |
| ---------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- |
| **Paso 1. EDA**                                      | Analizar cada variable, distribuciones, relaci√≥n con la target, detectar variables relevantes.                        | Permite entender el dataset y anticipar qu√© columnas pueden tener mayor importancia en el bosque.                    |
| **Paso 2. Preprocesamiento**                         | Imputar nulos, codificar categ√≥ricas (One-Hot o LabelEncoding), eliminar columnas irrelevantes. No requiere escalado. | Random Forest no acepta nulos y necesita variables num√©ricas. El exceso de columnas in√∫tiles puede introducir ruido. |
| **Paso 3. Ajuste de hiperpar√°metros (GridSearchCV)** | Probar valores de `n_estimators`, `max_depth`, `min_samples_leaf`, `max_features`.                                    | Permite equilibrar el modelo: demasiada profundidad ‚Üí sobreajuste, demasiada simplicidad ‚Üí bajo rendimiento.         |
| **Paso 4. Entrenamiento del modelo final**           | Entrenar un Random Forest con los **mejores hiperpar√°metros** encontrados.                                            | Garantiza que usamos el bosque m√°s estable y efectivo para ese dataset.                                              |
| **Paso 5. An√°lisis overfitting / underfitting**      | Comparar accuracy en **train** y **test**.                                                                            | Si train ‚â´ test ‚Üí sobreajuste. Si ambos bajos ‚Üí underfitting. Random Forest normalmente generaliza muy bien.         |
| **Paso 6. Importancia de las variables**             | Revisar `feature_importances_` y representar sus valores.                                                             | Permite entender qu√© variables han sido m√°s utilizadas por el bosque para tomar decisiones.                          |
| **Paso 7. M√©tricas de evaluaci√≥n**                   | Accuracy y matriz de confusi√≥n.                                                     | Permite evaluar el rendimiento final del modelo y ver qu√© clases confunde.                                           |

</div>

---

## Actividad de seguimiento: Titanic

Realiza un proyecto completo con el dataset **Titanic** siguiendo los pasos vistos en clase.

1. **EDA b√°sico** (ya realizado en entregas anteriores)
2. **Preprocesamiento** (ya realizado en entregas anteriores)
3. **Modelado:**
   * Entrena un **√Årbol de Decisi√≥n**
   * Entrena un **Random Forest**
   * Busca los mejores par√°metros, muestras los √°rboles generados, etc.
4. **Evaluaci√≥n:**
   * Accuracy
   * Matriz de confusi√≥n
   * Comparaci√≥n entre ambos modelos

**Entrega:** Notebook (Colab) con todos los pasos y conclusiones.

</div>