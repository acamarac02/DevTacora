---
title: "Logistic Regression"
sidebar_position: 4
toc_max_heading_level: 5
description: "Introducci√≥n a la Regresi√≥n Log√≠stica aplicada a problemas de clasificaci√≥n binaria. Qu√© es, por qu√© surge, c√≥mo funciona la funci√≥n sigmoide, cu√°ndo usarla, preprocesamiento necesario, hiperpar√°metros principales y entrenamiento paso a paso utilizando el dataset Titanic."
keywords: [Regresi√≥n Log√≠stica, Logistic Regression, Clasificaci√≥n Binaria, ML, Machine Learning, Sigmoide, Regularizaci√≥n, scikit-learn, Titanic]
---


La **Regresi√≥n Log√≠stica** es uno de los modelos m√°s utilizados en Machine Learning para resolver **problemas de clasificaci√≥n**.
A pesar de su nombre, **no sirve para predecir valores continuos**, sino **clases** (por ejemplo, *sobrevive / no sobrevive*, *spam / no spam*, *cliente fiel / abandona*‚Ä¶).

Por todo ello, suele ser el **primer modelo de referencia** (‚Äúbaseline‚Äù) en muchos proyectos reales, es decir, el primer modelo sencillo con el que medimos el rendimiento m√≠nimo razonable.

Sirve para decidir si necesitamos modelos m√°s complejos o si una soluci√≥n simple ya es suficiente.

La idea central es muy sencilla:

> La Regresi√≥n Log√≠stica **no predice directamente una clase**, sino la **probabilidad** de pertenecer a ella.

Primero calcula una combinaci√≥n lineal de las variables, y luego aplica una funci√≥n sigmoide que convierte ese valor en una probabilidad entre 0 y 1.
Finalmente, se decide una clase usando un umbral (normalmente 0.5).

![Gr√°fico EDA](../../0-img/regresion_logistica.png)

Esta forma de trabajar la convierte en un modelo:

* interpretable (ves qu√© variables aumentan o disminuyen la probabilidad)
* estable
* f√°cil de entrenar
* especialmente √∫til cuando las relaciones son *aproximadamente lineales*

La encontrar√°s en multitud de aplicaciones reales, como:

* **Titanic** ‚Üí predecir si una persona sobrevive (0/1)
* **Churn** ‚Üí si un cliente abandonar√° la empresa
* **Salud** ‚Üí si un paciente presenta una enfermedad
* **Finanzas** ‚Üí si una operaci√≥n es fraudulenta
* **Marketing** ‚Üí si un usuario har√° clic en un anuncio

En todos estos casos, el objetivo final es el mismo: **predecir una probabilidad y clasificar seg√∫n ella**.

---

## Regresi√≥n Log√≠stica Binaria vs. Multiclase

Aunque se estudia casi siempre como un modelo para dos clases, la Regresi√≥n Log√≠stica tiene **dos variantes**:

‚úî **Regresi√≥n Log√≠stica Binaria**

Se usa cuando solo existen **dos clases posibles**:

* sobrevivir vs. no sobrevivir (Titanic)
* spam vs. no spam
* cliente abandona vs. cliente permanece
* fraudulento vs. no fraudulento

Es la forma m√°s sencilla y la base sobre la que se construye la multiclase.

‚úî **Regresi√≥n Log√≠stica Multiclase (multinomial)**

Es una extensi√≥n para cuando hay **m√°s de dos categor√≠as**, por ejemplo:

* Tipo de flor (*setosa*, *versicolor*, *virginica*) en Iris
* Tipo de prenda (*camiseta*, *pantal√≥n*, *abrigo*)
* Nivel de riesgo (*bajo*, *medio*, *alto*)

Funciona de forma similar a la binaria, pero usa la funci√≥n **softmax** en lugar de la sigmoide para obtener probabilidades para cada clase.

:::info DIFERENCIAS A NIVEL DE USUARIO
Regresi√≥n Log√≠stica Binaria y Multiclase funcionan igual para el usuario.   
La diferencia est√° por dentro: la versi√≥n binaria usa sigmoide y la multinomial usa softmax para calcular probabilidades.   
En la pr√°ctica, solo cambias un par√°metro en Scikit-Learn.
:::

---

## Funcionamiento del modelo 

:::tip V√≠deo recomendado
Si quieres una explicaci√≥n muy clara e intuitiva de la Regresi√≥n Log√≠stica ‚Äîtanto binaria como multiclase‚Äî te recomiendo este v√≠deo:

üëâ https://youtu.be/KbK_Nb9OS70

Hasta el **minuto 7:42** es perfecto para nuestro nivel: explica la idea de probabilidad, la sigmoide y c√≥mo extiende al caso multiclase sin entrar en matem√°ticas avanzadas.  
A partir de ah√≠ ya entra en detalles que no necesitamos ver en este curso.
:::


### Modelo binario

La Regresi√≥n Log√≠stica binaria se basa en una idea muy sencilla:

1. Primero combina las variables de forma **lineal**.
2. Despu√©s transforma ese resultado en una **probabilidad** usando la funci√≥n sigmoide.
3. Finalmente decide la clase comparando con un **umbral** (normalmente 0.5).

Este proceso hace que el modelo sea interpretable y muy adecuado para problemas donde la relaci√≥n entre *features* y *probabilidad* es aproximadamente lineal.

En el ejemplo partimos de este dataset, que representa el valor de PSA en pacientes varones, que permite determinar (junto con muchos m√°s factores), si parece c√°ncer de pr√≥stata.

![Gr√°fica](../../0-img/dataset.png)

---

#### Combinaci√≥n lineal de variables

El modelo empieza calculando una combinaci√≥n lineal de las variables:

$$
z = w_0 + w_1x_1 + w_2x_2 + \dots + w_nx_n
$$

Donde:

* (x_i) = valores de las features
* (w_i) = coeficientes que el modelo aprende
* (w_0) = intercept (sesgo)

Este valor (z) por s√≠ solo **no es una probabilidad**, ya que puede ser negativo, positivo o muy grande.
Por eso necesitamos una transformaci√≥n.

![Gr√°fica](../../0-img/grafica-lineal.png)

---

#### Funci√≥n sigmoide

Para convertir ese valor lineal en una probabilidad entre 0 y 1 usamos la **funci√≥n sigmoide**:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

La sigmoide ‚Äúaprieta‚Äù valores muy grandes o muy peque√±os hacia el rango ([0, 1]).

**Interpretaci√≥n gr√°fica t√≠pica:**

* Si (z) es muy negativo ‚Üí probabilidad ‚âà 0
* Si (z) es 0 ‚Üí probabilidad = 0.5
* Si (z) es muy positivo ‚Üí probabilidad ‚âà 1

El resultado es:

$$
p = \sigma(z)
$$

Y ahora s√≠:

> **p es la probabilidad de pertenecer a la clase 1.**

![Gr√°fica](../../0-img/funcion-sigmoide.png)

---

#### Probabilidad como salida

La Regresi√≥n Log√≠stica **no devuelve directamente una clase**, sino una probabilidad:

* Si (p = 0.83) ‚Üí 83% de probabilidad de ser clase 1
* Si (p = 0.12) ‚Üí 12% de probabilidad de ser clase 1

Esto se puede obtener con:

```python
model.predict_proba(X)
```

Por ejemplo, supongamos que llega un nuevo registro de PSA con valor 2.3ng/mL y debemos determinar si el paciente es susceptible de tener c√°ncer o no.

![Gr√°fica](../../0-img/probabilidad.png)

Seg√∫n la funci√≥n obtenida, tiene un 60% de probabilidades de padecer c√°ncer. La clase en la que se clasifica depende del umbral establecido.

---

#### Umbral de decisi√≥n

La decisi√≥n final se toma comparando la probabilidad con un **umbral**:

$$
\hat{y} =
\begin{cases}
1 & \text{si } p \ge 0.5 \\
0 & \text{si } p < 0.5
\end{cases}
$$

El umbral por defecto es **0.5**, pero se puede modificar.

**¬øPor qu√© cambiarlo?**

* Si quieres **menos falsos negativos** ‚Üí baja el umbral (p. ej. 0.3).
* Si quieres **menos falsos positivos** ‚Üí s√∫belo (p. ej. 0.7).

Esto es muy √∫til en problemas desequilibrados.

![Gr√°fica](../../0-img/falsos-positivos.png)

---

#### Interpretaci√≥n b√°sica de coeficientes

Una de las ventajas clave de la Regresi√≥n Log√≠stica es que **sus coeficientes son interpretables**.

* Si (w_i > 0):
  aumentar (x_i) **incrementa** la probabilidad de clase 1.
* Si (w_i < 0):
  aumentar (x_i) **reduce** la probabilidad.
* Si (w_i = 0):
  la variable **no influye**.

> Cuanto mayor sea el valor absoluto del coeficiente ‚Üí m√°s importante es la variable.

**Ejemplos t√≠picos:**

* En Titanic:
  `Sex_female` suele tener un coeficiente **muy positivo** (ser mujer aumenta la probabilidad de sobrevivir).
* `Pclass` suele tener coeficiente **negativo** (clases m√°s bajas ‚Üí menos probabilidad).
* `Age` suele tener coeficiente **negativo** (m√°s edad ‚Üí menor probabilidad).

Para verlos en Python:

```python
model.coef_
model.intercept_
```

---

### Modelo multiclase

La Regresi√≥n Log√≠stica no solo sirve para clasificar entre dos clases (0 y 1).
Tambi√©n puede utilizarse cuando existen **tres o m√°s categor√≠as**, como en el cl√°sico dataset **Iris**:

* *Setosa*
* *Versicolor*
* *Virginica*

En estos casos hablamos de **Regresi√≥n Log√≠stica Multiclase** o **multinomial**.

Lo m√°s importante es entender que:

> **El modelo funciona igual para el usuario, pero por dentro calcula una probabilidad para cada clase, no solo para una.**

---

#### Generalizaci√≥n al caso multiclase

En la versi√≥n binaria, el modelo calcula **una √∫nica probabilidad**:
la de pertenecer a la clase 1, usando la sigmoide.

En el caso multiclase:

* El modelo calcula **una probabilidad por cada clase**.
* Estas probabilidades siempre suman **1**.
* La clase predicha es la que tiene **mayor probabilidad**.

Para conseguir esto, se usa una funci√≥n llamada **softmax**, que es equivalente a la sigmoide pero adaptada a varios valores.

> No necesitamos entrar en su f√≥rmula: basta saber que convierte varios valores en probabilidades que suman 1.


#### Ejemplo

Supongamos que tenemos este dataset:

![Gr√°fica](../../0-img/dataset-multiclase.png)

El modelo aprende **tres combinaciones lineales**, una por clase:

* una para *no infecci√≥n*
* una para *infecci√≥n v√≠rica*
* una para *infecci√≥n bacteriana*

Cada una de ellas pasa por la funci√≥n softmax para generar probabilidades:

* Probabilidad de no infecci√≥n
* Probabilidad de infecci√≥n v√≠rica
* Probabilidad de infecci√≥n bacteriana

![Gr√°fica](../../0-img/graficas-multiclase.png)

Si llega un paciente con CRP de 25, la salida de softmax ser√° la siguiente:

| Clase      | Probabilidad |
| ---------- | ------------ |
| No infecci√≥n     | 0.016         |
| Infecci√≥n v√≠rica | 0.982         |
| Infecci√≥n bacteriana  | 0.002         |

El modelo clasifica como **infecci√≥n v√≠rica** (98.2% de probabilidad)

---

## Cu√°ndo usar Regresi√≥n Log√≠stica

La Regresi√≥n Log√≠stica es un modelo muy √∫til, pero no sirve para cualquier tipo de problema.

Es ideal cuando la probabilidad de pertenecer a una clase **aumenta o disminuye de forma m√°s o menos lineal** seg√∫n las variables.

Ejemplos t√≠picos:

* Cuanto mayor es el PSA ‚Üí m√°s probabilidad de c√°ncer.
* Cuanto menor es la edad ‚Üí m√°s probabilidad de sobrevivir en Titanic.
* Cuanto mayor es la tarifa (`Fare`) ‚Üí m√°s probabilidad de salvarse.

No necesitamos que la relaci√≥n sea perfectamente lineal, solo **que no sea radicalmente curva o ca√≥tica**.


:::info Exploratory Data Analysis
Recuerda que las relaciones entre las variables y la target las analiz√°bamos en la fase de EDA. En ese primer momento, podremos intuir si la Regresi√≥n Log√≠stica ser√° correcta para nuestro problema o no.
:::

---

### Criterios para saber si la Regresi√≥n Log√≠stica ‚Äúno rinde bien‚Äù

#### Criterio 1: comparar con modelos m√°s flexibles

En la pr√°ctica, la forma m√°s sencilla y fiable de saber si la Regresi√≥n Log√≠stica es adecuada es:

> **Entrenar tambi√©n un modelo m√°s flexible** (Random Forest, KNN, √Årbol de Decisi√≥n)
> **y comparar resultados.**

Interpretaci√≥n:

* Si Logistic Regression obtiene **rendimiento similar** ‚Üí el problema es casi lineal ‚úî
* Si modelos m√°s flexibles lo superan claramente ‚Üí el problema NO es lineal ‚ùå

Ejemplo:

| Modelo              | Accuracy test |
| ------------------- | ------------- |
| Logistic Regression | 0.78          |
| Random Forest       | 0.90          |

Interpretaci√≥n:
**LR no rinde bien ‚Üí el problema NO es lineal.**

Esta regla es muy utilizada en ciencia de datos real: la LR sirve como **baseline**.

---

#### Criterio 2: Accuracy razonablemente baja en test

```python
from sklearn.metrics import accuracy_score

# Predicciones
y_pred = model.predict(X_test)

# Accuracy en test
test_acc = accuracy_score(y_test, y_pred)

print("Accuracy TEST:", test_acc)
```

No existen valores universales, pero como gu√≠a:

* **< 70%** ‚Üí probablemente el modelo no es adecuado
* **70‚Äì80%** ‚Üí aceptable seg√∫n el dataset
* **> 80%** ‚Üí suele ser buen rendimiento para problemas tabulares cl√°sicos

**OJO:** depende del dataset.
En Titanic, por ejemplo, una LR con **75‚Äì80%** ya se considera buena.

---

#### Criterio 3: Mucha diferencia entre train y test

```python
# Accuracy en entrenamiento
train_acc = model.score(X_train, y_train)

# Accuracy en test
test_acc = model.score(X_test, y_test)

print("Accuracy TRAIN:", train_acc)
print("Accuracy TEST :", test_acc)
```

Si ves algo como:

| M√©trica        | Valor |
| -------------- | ----- |
| Accuracy TRAIN | 0.92  |
| Accuracy TEST  | 0.72  |

‚Üí **mala generalizaci√≥n**.
En LR esto suele ocurrir cuando las relaciones no son lineales y el modelo no consigue ajustar ambas partes.

---

## Importancia del preprocesamiento

Aunque la Regresi√≥n Log√≠stica es un modelo sencillo y estable, s√≠ requiere ciertos pasos de preprocesamiento para funcionar correctamente.

| Aspecto                                    | ¬øEs necesario?        | Explicaci√≥n                                                                                                                                                                                                                                     |
| ------------------------------------------ | --------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Escalado (StandardScaler / MinMax)**     | **‚úî Obligatorio**     | La Regresi√≥n Log√≠stica optimiza sus par√°metros mediante m√©todos num√©ricos (gradiente), y si las variables est√°n en escalas muy diferentes, la optimizaci√≥n se vuelve inestable. Escalar mejora la convergencia y evita que una variable domine. |
| **Codificaci√≥n de categ√≥ricas**            | **‚úî S√≠**    | La LR no admite texto. Adem√°s, al ser lineal, es importante usar **One-Hot** y **LabelEncoder** de forma correcta.                                                 |
| **Tratamiento de outliers**                | **‚úî Recomendado**     | Outliers muy grandes pueden empujar la recta de decisi√≥n de forma exagerada, afectando los coeficientes y la probabilidad final. No es tan grave como en KNN, pero s√≠ afecta.                                                                   |
| **Eliminar o imputar nulos**               | **‚úî S√≠**              | La LR no permite valores nulos.                                                                                                                             |
| **Eliminar columnas irrelevantes**         | **‚úî Muy recomendado** | En modelos lineales, cada variable tiene un peso directo. Variables in√∫tiles a√±aden ruido y empeoran la estabilidad del modelo, especialmente cuando hay pocas muestras.                                                                        |



:::warning
La Regresi√≥n Log√≠stica es uno de los modelos m√°s sensibles al preprocesamiento. Un mal preprocesado suele dar lugar a coeficientes inestables y mala generalizaci√≥n.
:::

---

## Regularizaci√≥n e hiperpar√°metros principales

La Regresi√≥n Log√≠stica es un modelo muy sencillo, pero tiene un aspecto clave para que funcione bien:
la **regularizaci√≥n**.
Este mecanismo evita que los coeficientes crezcan demasiado y que el modelo **sobreajuste** los datos de entrenamiento.

En Scikit-Learn, la regularizaci√≥n se controla principalmente mediante los hiperpar√°metros **`C`**, **`penalty`**, **`solver`** y **`max_iter`**.

A continuaci√≥n explicamos los m√°s importantes.

---

### ¬øQu√© es la regularizaci√≥n y por qu√© se usa?

Cuando el modelo intenta ajustarse demasiado a los datos de entrenamiento (ruido, valores extremos, peque√±as fluctuaciones), sus coeficientes pueden hacerse **muy grandes**.
Esto provoca:

* sobreajuste,
* mala generalizaci√≥n,
* inestabilidad en las predicciones.

La **regularizaci√≥n** a√±ade una ‚Äúpenalizaci√≥n‚Äù al tama√±o de los coeficientes, oblig√°ndolos a mantenerse razonables.

> **Objetivo:** evitar que el modelo memorice y conseguir que generalice mejor.

En la pr√°ctica:
**m√°s regularizaci√≥n ‚Üí coeficientes m√°s peque√±os ‚Üí modelo m√°s estable.**

---

### Par√°metro `C`

`C` es el hiperpar√°metro m√°s importante de la Regresi√≥n Log√≠stica.

* **C grande** ‚Üí **poca** regularizaci√≥n ‚Üí el modelo se ajusta mucho ‚Üí riesgo de *overfitting*
* **C peque√±o** ‚Üí **mucha** regularizaci√≥n ‚Üí el modelo se suaviza ‚Üí riesgo de *underfitting*

En Scikit-Learn:

```python
LogisticRegression(C=1.0)
```

Ten en cuenta que **no existe un ‚ÄúC correcto‚Äù universal**, pero s√≠ hay rangos recomendados y comportamientos t√≠picos seg√∫n el dataset.

| Tipo de dataset                     | Valores t√≠picos de `C` | Explicaci√≥n                                               |
| ----------------------------------- | ---------------------- | --------------------------------------------------------- |
| Peque√±o, ruidoso, con outliers      | **0.1 ‚Äì 1**            | M√°s regularizaci√≥n evita que los coeficientes se disparen |
| Limpio, tabular, relaciones claras  | **1 ‚Äì 10**             | C est√°ndar para buena generalizaci√≥n                      |
| Muy complejo o con muchas variables | **‚â•10**                | Solo si buscas ajustar m√°s (riesgo de overfitting)        |
| Datasets enormes                    | **0.1 ‚Äì 1**            | M√°s regularizaci√≥n ayuda a la estabilidad                 |

En la pr√°ctica:

> **Empieza siempre con `C = 1`.   
> Si rinde mal, prueba `C = 0.1` o `C = 10`.**

:::tip Detectar overfitting o underfitting
Puedes analizar si la Regresi√≥n Log√≠stica est√° sobreajustando o subajustando comparando la **accuracy en entrenamiento** y en **test**:

```python
print("Accuracy TRAIN:", model.score(X_train, y_train))
print("Accuracy TEST :", model.score(X_test, y_test))
```

**Interpretaci√≥n r√°pida:**

* TRAIN ‚â´ TEST ‚Üí **overfitting** (el modelo memoriza, no generaliza).
* TRAIN ‚âà TEST pero ambas bajas ‚Üí **underfitting** (el modelo es demasiado simple).
* TRAIN ‚âà TEST y ambas altas ‚Üí **buen ajuste**.

:::

---

### Par√°metro `max_iter`

En algunos datasets, especialmente cuando:

* las variables no est√°n bien escaladas, o
* el modelo necesita m√°s pasos de optimizaci√≥n,

puede aparecer un aviso:

```
ConvergenceWarning: Increase max_iter
```

Solo significa que el algoritmo necesita m√°s iteraciones.

Basta con subirlo:

```python
LogisticRegression(max_iter=500)
```

---

## M√©tricas de evaluaci√≥n

Las m√©tricas que utilizamos para evaluar una **Regresi√≥n Log√≠stica** son **exactamente las mismas** que ya estudiamos en los otros problemas de clasificaci√≥n:

* **Accuracy**
* **Matriz de confusi√≥n**
* **Precision, Recall y F1-score** (`classification_report`)

---

## Atributos √∫tiles

La Regresi√≥n Log√≠stica dispone de **tres elementos muy √∫tiles** para entender c√≥mo est√° tomando decisiones:

1. **Predicci√≥n de clases (`predict`)**
2. **Predicci√≥n de probabilidades (`predict_proba`)**
3. **Coeficientes del modelo (`coef_` e `intercept_`)**

Estos atributos permiten interpretar mejor el modelo y tomar decisiones basadas en probabilidades, algo especialmente importante en problemas cl√≠nicos, financieros o desbalanceados.

---

### `predict()` ‚Äî Predice la clase final

Devuelve la **clase asignada** (0 o 1) usando el umbral por defecto 0.5.

```python
# Predecimos la CLASE final (0 o 1) para los registros del conjunto de test
# Esta predicci√≥n usa el umbral por defecto de 0.5.
y_pred = model.predict(X_test)

# Mostramos las primeras 5 predicciones para ver el formato del resultado
print(y_pred[:5])
```

Interpretaci√≥n:

* El modelo asigna 1 si la probabilidad ‚â• 0.5.
* Asigna 0 si la probabilidad < 0.5.

Es la predicci√≥n ‚Äúdirecta‚Äù, pero **no es la m√°s informativa**.

---

### `predict_proba()` ‚Äî Devuelve probabilidades

Este es el atributo **m√°s importante** de la Regresi√≥n Log√≠stica.

```python
probs = model.predict_proba(X_test)
print(probs[:5]) # Mostramos solo la probabilidad de los 5 primeros
```

Salida t√≠pica:

```
[[0.32, 0.68],
 [0.71, 0.29],
 ...]
```

Interpretaci√≥n:

* La primera columna es la probabilidad de **clase 0**.
* La segunda columna es la probabilidad de **clase 1**.

Esto permite cambiar el **umbral** manualmente y evitar errores en problemas desbalanceados.

---

### `coef_` ‚Äî Importancia (lineal) de cada feature

`coef_` contiene los coeficientes del modelo.
Es equivalente a una ‚Äúimportancia lineal‚Äù.

```python
print(model.coef_)
```

Interpretaci√≥n (muy simple):

* `coef_` **positivo** ‚Üí aumenta la probabilidad de clase 1.
* `coef_` **negativo** ‚Üí reduce la probabilidad de clase 1.
* valor cercano a 0 ‚Üí poca influencia.

Ejemplo t√≠pico (Titanic):

* `Sex_female` ‚Üí coeficiente **positivo** (ser mujer aumenta supervivencia)
* `Pclass` ‚Üí coeficiente **negativo** (clase baja reduce supervivencia)

Esto ayuda a entender **c√≥mo** est√° aprendiendo el modelo.

Puedes representarlo gr√°ficamente con el siguiente c√≥digo para **regresi√≥n log√≠stica binaria**:

```python
import matplotlib.pyplot as plt
import numpy as np

coef = model.coef_[0]
features = X_train.columns

plt.figure(figsize=(8,5))
plt.barh(features, coef)
plt.axvline(0, color='black')
plt.title("Coeficientes del modelo (impacto lineal)")
plt.show()
```

Interpretaci√≥n sencilla:
* barras > 0 ‚Üí aumenta la probabilidad de clase 1
* barras < 0 ‚Üí reduce la probabilidad de clase 1
* barras peque√±as ‚Üí poca influencia sobre la probabilidad de clase 1

![Gr√°fica](../../0-img/coef-binaria.png)

O con el siguiente c√≥digo para **regresi√≥n log√≠stica multiclase**:

```python
# Cada fila de coef_ corresponde a una clase (en el mismo orden que best_model.classes_)
# Cada columna corresponde a una feature
coef_df = pd.DataFrame(
    best_model.coef_,
    index=best_model.classes_,   # nombres de las clases
    columns=X_train.columns      # nombres de las variables
)

plt.figure(figsize=(8,5))
sns.heatmap(
    coef_df,
    annot=True,          # mostrar valores num√©ricos
    cmap="coolwarm",     # azul = negativo, rojo = positivo
    center=0             # que los colores se centren en 0, ideal para interpretar coeficientes
)

plt.title("Coeficientes del modelo de Regresi√≥n Log√≠stica (multiclase)")
plt.xlabel("Features")
plt.ylabel("Clases")
plt.show()
```

Interpretaci√≥n sencilla:
* Filas ‚Üí cada especie (setosa, versicolor, virginica)
* Columnas ‚Üí cada una de las 4 features
* Color rojo ‚Üí coeficiente positivo ‚Üí aumenta probabilidad de esa clase
* Color azul ‚Üí coeficiente negativo ‚Üí reduce probabilidad de esa clase
M√°s intenso = mayor impacto

![Gr√°fica](../../0-img/coef-multiclase.png)

---

## Ajuste de hiperpar√°metros (GridSearchCV)

En Regresi√≥n Log√≠stica, el hiperpar√°metro m√°s importante es **`C`**, que controla la regularizaci√≥n del modelo.
Como no sabemos de antemano qu√© valor funcionar√° mejor, podemos usar **GridSearchCV** para probar varios y seleccionar autom√°ticamente el que obtiene mejor rendimiento.

---

### Diccionario recomendado

Usamos una peque√±a rejilla de valores t√≠picos para `C` y un valor c√≥modo para `max_iter` (por si el modelo necesita m√°s iteraciones para ajustarse):

```python
param_grid = {
    "C": [0.01, 0.1, 1, 10],
    "max_iter": [300]   # suficiente para asegurar convergencia
}
```

* Valores **peque√±os** (0.01, 0.1) ‚Üí m√°s regularizaci√≥n ‚Üí modelo m√°s suave
* Valor **medio** (1) ‚Üí equilibrio (suele funcionar muy bien)
* Valor **grande** (10) ‚Üí menos regularizaci√≥n ‚Üí modelo m√°s flexible

GridSearchCV probar√° todos estos valores y elegir√° el que obtenga **mejor accuracy**.

---

### Ejecutar GridSearchCV

```python
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# Modelo base
log_reg = LogisticRegression()

# Configuraci√≥n del GridSearch
grid = GridSearchCV(
    estimator=log_reg,
    param_grid=param_grid,
    cv=5,                # validaci√≥n cruzada 5-fold
    scoring="accuracy"   # m√©trica a optimizar
)

# Entrenar la b√∫squeda
grid.fit(X_train, y_train)

# Mostrar los mejores hiperpar√°metros
print("Mejores par√°metros:", grid.best_params_)
```

---

### Obtenci√≥n del mejor modelo

Una vez finalizada la b√∫squeda, puedes obtener directamente el modelo √≥ptimo ya entrenado:

```python
best_model = grid.best_estimator_
```

`best_model` es una Regresi√≥n Log√≠stica con los mejores valores de `C` y `max_iter`, lista para usar.

---

## Flujo recomendado en un problema de Regresi√≥n Log√≠stica

| Paso                                                 | ¬øQu√© se hace?                                                                         | ¬øPor qu√© es importante?                                                                           |
| ---------------------------------------------------- | ------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------- |
| **Paso 1. EDA**                                           | Analizar cada variable, revisar distribuciones, relaci√≥n con la target.               | Detecta patrones lineales, outliers, columnas in√∫tiles, nulos‚Ä¶ y permite estimar si LR puede funcionar. |
| **Paso 2. Preprocesamiento**                              | Imputaci√≥n de nulos, escalado, One-Hot, eliminaci√≥n de variables irrelevantes...        | La LR necesita datos limpios y escalados para ser estable y generalizar bien.                     |
| **Paso 3. Entrenamiento (modelo base o GridSearch)**      | Entrenar la LR. | Permite obtener el modelo que mejor se ajusta al dataset.                                         |
| **Paso 4. An√°lisis de overfitting/underfitting**          | Comparar accuracy en train y test.                                                    | Permite saber si el modelo est√° memorizando o si es demasiado simple. Si hemos usado GridSearch deber√≠amos tener un modelo equilibrado.                             |
| **Paso 5. Atributos √∫tiles**                              | Revisar `predict_proba`, `coef_` y la interpretaci√≥n de los signos.                   | Ayuda a entender c√≥mo est√° tomando decisiones el modelo.                                          |
| **Paso 6. M√©tricas de evaluaci√≥n**                        | Accuracy, matriz de confusi√≥n.                                                        | Mide el rendimiento real del modelo.                                                              |
| **Paso 7. Comparaci√≥n con un modelo baseline** | Comparar con √Årbol o KNN simples.                                                     | √ötil para ver si el problema es lineal o si LR est√° limitada.                                     |

---

## Ejemplo Regresi√≥n Lineal binaria

Para ver c√≥mo funciona un **Linear Regression binario** en la pr√°ctica, puedes ejecutar este ejemplo utilizando el dataset **Titanic sucio**.

üëâ **Puedes abrir el cuaderno aqu√≠:**
[Colab: Linear Regression con Titanic](../../0-datasets/4_LR_Titanic.ipynb)

üëâ **Dataset utilizado:**
[titanic_sucio.csv](../../0-datasets/titanic_sucio.csv)

---

## Ejemplo Regresi√≥n Lineal multiclase

Para ver c√≥mo funciona un **Linear Regression multiclase** en la pr√°ctica, puedes ejecutar este ejemplo utilizando el dataset **iris**.

üëâ **Puedes abrir el cuaderno aqu√≠:**
[Colab: Linear Regression con iris](../../0-datasets/4_1_LR_iris.ipynb)

üëâ **Dataset utilizado:**
[iris.csv](../../0-datasets/iris.csv)