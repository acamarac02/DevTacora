---
title: "Decision Trees"
sidebar_position: 2
toc_max_heading_level: 5
description: "Introducci√≥n a los √Årboles de Decisi√≥n aplicados a problemas de clasificaci√≥n. C√≥mo funcionan, c√≥mo toman decisiones, sus hiperpar√°metros principales y c√≥mo entrenarlos y evaluarlos en Python utilizando el dataset Iris."
keywords: [√Årboles de Decisi√≥n, Decision Tree, Clasificaci√≥n, Machine Learning, scikit-learn, iris, reglas, hiperpar√°metros]
---


Los **√Årboles de Decisi√≥n** son uno de los modelos m√°s intuitivos y visuales de todo el Machine Learning.
En lugar de usar f√≥rmulas o distancias, toman decisiones siguiendo **reglas tipo ‚Äúsi‚Ä¶ entonces‚Ä¶‚Äù** que dividen los datos en grupos cada vez m√°s homog√©neos.


Un √Årbol de Decisi√≥n es un modelo que:

* divide el espacio de datos en regiones seg√∫n preguntas simples (*‚Äú¬øpetal_length ‚â§ 2.45?‚Äù‚Äù*)
* llega a una **hoja**, donde asigna una clase (en clasificaci√≥n)
* aprende autom√°ticamente qu√© preguntas hacer y en qu√© orden

Se usan para **clasificaci√≥n** (por ejemplo, qu√© especie de flor es Iris) y para **regresi√≥n** (predecir valores num√©ricos).

![Gr√°fico EDA](../../0-img/ejemplo-decision-tree.png)


Un √°rbol decide paso a paso:

1. ¬øLos ingresos del cliente superan los 50.000‚Ç¨?
2. Si no, ¬øtiene m√°s de 30 a√±os?
3. Finalmente, ¬øa qu√© clase pertenecen los datos que quedan?

Cada decisi√≥n crea una **rama**.
El camino completo desde la ra√≠z hasta la hoja es una **regla** del tipo:

> ‚ÄúSi *age* > 30 entonces No compra‚Äù.

Esto hace que el modelo sea f√°cil de explicar y de leer.

---

## Ventajas principales

Los √Årboles de Decisi√≥n destacan por:

* **Interpretabilidad:** puedes ver exactamente qu√© reglas est√° usando el modelo.
* **Visualizaci√≥n sencilla:** se pueden dibujar y entender en segundos.
* **No requieren escalado:** esto se debe a que no miden distancias, a diferencia de KNN o SVM.
* **Pueden capturar relaciones no lineales** sin esfuerzo.
* **Funcionan bien como modelo base** y preparan el camino para Random Forest y Gradient Boosting.

---

## Dataset de ejemplo: Iris

En estos apuntes utilizaremos el **dataset Iris**, uno de los conjuntos de datos m√°s conocidos en Machine Learning por su simplicidad y porque permite visualizar muy bien c√≥mo funcionan los modelos de clasificaci√≥n.

El dataset contiene **150 observaciones**, cada una correspondiente a una flor del g√©nero *Iris*, y est√° dividido en **3 especies**:

* **Iris setosa**
* **Iris versicolor**
* **Iris virginica**

![Gr√°fico EDA](../../0-img/resumen-iris.png)

Cada flor est√° descrita mediante **4 caracter√≠sticas num√©ricas**, medidas en cent√≠metros:

| sepal_length | sepal_width | petal_length | petal_width | species    |
| ------------ | ----------- | ------------ | ----------- | ---------- |
| 5.1          | 3.5         | 1.4          | 0.2         | setosa     |
| 7.0          | 3.2         | 4.7          | 1.4         | versicolor |
| 6.3          | 3.3         | 6.0          | 2.5         | virginica  |
| 4.9          | 3.1         | 1.5          | 0.1         | setosa     |
| 5.8          | 2.7         | 5.1          | 1.9         | virginica  |

Estas variables son ideales para aprender √Årboles de Decisi√≥n porque:

* Las clases est√°n **bien separadas**, sobre todo usando *petal_length* y *petal_width*.
* Se pueden generar √°rboles simples que explican muy bien c√≥mo funciona el modelo.
* Todo el dataset es **num√©rico y no tiene valores nulos**, por lo que no requiere preprocesamiento.

:::info
En los ejemplos visuales utilizaremos √∫nicamente **petal_length** y **petal_width** para que se entienda claramente c√≥mo el √°rbol divide los datos.
:::

---

## Funcionamiento y componentes

Los √Årboles de Decisi√≥n funcionan dividiendo progresivamente los datos mediante **preguntas simples**, hasta que cada grupo final contiene observaciones mayoritariamente de una sola clase.

![Gr√°fico EDA](../../0-img/componentes-ad.png)

---

### Componentes

Un √°rbol est√° formado por:

* **Nodo ra√≠z**. Es el punto de partida. Por este nodo pasan todos los elementos del dataset.   
    Aqu√≠ el modelo formula la **primera pregunta**, por ejemplo:

    ```
    ¬øpetal_length ‚â§ 2.45?
    ```

* **Ramas**. Cada respuesta a la condici√≥n crea una rama:

    * Si se cumple ‚Üí rama izquierda
    * Si NO se cumple ‚Üí rama derecha

* **Hojas**. Son los nodos finales. Aqu√≠ el √°rbol **asigna la clase** (setosa, versicolor, virginica‚Ä¶).    
    En una hoja ya no se hacen m√°s preguntas.

---

### Split

Un ***split*** es simplemente una **divisi√≥n del nodo actual** basada en una pregunta.

Ejemplo:

```
petal_width ‚â§ 1.75
```

Cada split:

* reduce la mezcla de clases
* produce nodos m√°s ‚Äúpuros‚Äù
* mejora la capacidad del modelo para separar especies

El √°rbol prueba muchos splits posibles y selecciona el que mejor separa las clases.

---

### Criterios de divisi√≥n: Gini

Para decidir **qu√© pregunta hacer** en cada nodo, el √°rbol prueba distintas posibles divisiones (*splits*) y elige la que mejor separa las clases.
La forma m√°s com√∫n de medir qu√© tan ‚Äúbuena‚Äù es una divisi√≥n es usando la **impureza Gini**, que indica cu√°n mezcladas est√°n las clases dentro de un nodo.

![Gini](../../0-img/gini.png)


La **impureza Gini** mide **lo mezcladas que est√°n las clases** en un nodo.

* **Gini = 0** ‚Üí nodo completamente puro (solo una clase)
* **Gini alto** ‚Üí mucha mezcla de clases

En el ejemplo de la imagen:

* Nodo ra√≠z ‚Üí `gini = 0.667` (m√°xima mezcla: 50 setosa, 50 versicolor, 50 virginica)
* Nodo naranja ‚Üí `gini = 0.0` (todas son setosa ‚Üí pureza total)
* Nodo verde ‚Üí `gini = 0.168` (casi todas versicolor)
* Nodo morado ‚Üí `gini = 0.043` (casi todas virginica)

El √°rbol elige el split que **reduce m√°s la impureza**, es decir, el que consigue nodos hijos m√°s homog√©neos.

:::info
Existe otro criterio de decisi√≥n llamado **entrop√≠a**, que mide la mezcla de clases de forma similar, pero es m√°s avanzado.
En este curso solo usaremos **Gini**, que es m√°s f√°cil de interpretar y es el valor por defecto en scikit-learn.
:::

---

### Ejemplo funcionamiento

Cuando llega una nueva observaci√≥n, el √°rbol:

1. Empieza en la **ra√≠z**.
2. Eval√∫a la condici√≥n del nodo:
   * Si se cumple ‚Üí rama izquierda
   * Si no ‚Üí rama derecha
3. Repite el proceso en los nodos siguientes.
4. Termina en una **hoja**, que contiene la clase predicha.

Es literalmente seguir un camino de decisiones **‚Äúsi‚Ä¶ entonces‚Ä¶‚Äù**.

---

## Importancia del preprocesamiento

Aunque los √Årboles de Decisi√≥n son modelos muy flexibles y funcionan bien incluso sin un preprocesamiento complejo, es importante conocer sus necesidades b√°sicas para evitar errores y obtener buenos resultados.

Las reglas principales son las siguientes:

| Aspecto                                | ¬øEs necesario?    | Explicaci√≥n                                                                                                                                                  |
| -------------------------------------- | ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Escalado (StandardScaler / MinMax)** | **‚ùå No**          | Los √°rboles no usan distancias, solo comparaciones del tipo ‚Äú¬øfeature ‚â§ umbral?‚Äù. La escala no afecta a las decisiones.                                      |
| **Codificaci√≥n de categ√≥ricas**        | **‚úî S√≠**          | Los √°rboles solo aceptan n√∫meros. Se puede usar **Label Encoding** o **One-Hot**, ambas funcionan porque el √°rbol **no interpreta orden** entre los valores. |
| **Tratamiento de outliers**            | **‚úî A veces**     | Los √°rboles son robustos, pero outliers muy extremos pueden causar splits raros. Menos cr√≠tico que en modelos basados en distancia.                          |
| **Eliminar nulos o imputarlos**        | **‚úî S√≠**          | Los √Årboles de Decisi√≥n de scikit-learn **no aceptan nulos**. Es obligatorio imputar o eliminar filas.                                                       |
| **Eliminar columnas irrelevantes**     | **‚úî Recomendado** | Features totalmente in√∫tiles pueden generar splits basados en ruido. No es cr√≠tico, pero mejora la calidad del √°rbol.                                        |

---

## Ejemplo b√°sico en Python

Para entender mejor c√≥mo se comporta un √Årbol de Decisi√≥n, puedes ejecutar este ejemplo pr√°ctico con el dataset **Iris**.
En √©l entrenamos un √°rbol **sin limitar profundidad** (muy grande ‚Üí sobreajuste).

üëâ **Puedes abrir el cuaderno aqu√≠:**
[Colab: √Årbol de Decisi√≥n con Iris](../../0-datasets/ejemplo_decision_tree_iris.ipynb)

üëâ **Dataset utilizado:**
[iris.csv](../../0-datasets/iris.csv)

Este ejemplo ser√° la base para entender los conceptos de **profundidad**, **sobreajuste**, **subajuste**, y los **hiperpar√°metros principales** que veremos a continuaci√≥n.

---

## Visualizaci√≥n del √°rbol

Una de las ventajas m√°s grandes de los √Årboles de Decisi√≥n es que podemos **ver el modelo completo**: todas sus decisiones, separaciones y clases finales. Esto hace que el aprendizaje sea muy visual y ayuda a entender c√≥mo ‚Äúpiensa‚Äù el √°rbol.

Scikit-learn permite dibujar el √°rbol directamente con `plot_tree`:

```python
# Creamos una figura grande para que el √°rbol se vea bien
plt.figure(figsize=(22, 12))

# Dibujamos el √°rbol de decisi√≥n entrenado
plot_tree(
    tree_full,                # El modelo ya entrenado
    feature_names=features,   # Nombres de las columnas que estamos usando como X
    class_names=iris["species"].unique(),  # Nombres de las clases (setosa, versicolor, virginica)
    filled=True,              # Colorea los nodos seg√∫n la clase predominante
    rounded=True,             # Esquinas redondeadas para mejorar la visualizaci√≥n
    fontsize=10               # Tama√±o del texto dentro del √°rbol
)

# Mostramos la figura en pantalla
plt.show()
```

Par√°metros importantes:

* **feature_names** ‚Üí nombres de las columnas usadas para X (features)
* **class_names** ‚Üí nombres de las clases
* **filled=True** ‚Üí colorea los nodos seg√∫n la especie predominante
* **rounded=True** ‚Üí mejora la legibilidad

![√Årbol complejo](../../0-img/tree-full.png)

üß† **Interpretaci√≥n del gr√°fico**

En cada nodo puedes ver:

* **la condici√≥n** que divide los datos (por ejemplo, `petal_length ‚â§ 2.45`)
* **el valor de Gini**, que indica la mezcla de clases
* **cu√°ntas muestras** llegan a ese nodo (`samples`)
* **la distribuci√≥n por clases** (`value = [...]`)
* y la **clase asignada** si es una hoja

Gracias a esto podemos seguir el flujo de decisiones desde la ra√≠z hasta cada hoja y entender **exactamente** por qu√© el √°rbol clasifica una flor como *setosa*, *versicolor* o *virginica*.

---

## Profundidad e hiperpar√°metros

Si observas el √°rbol que hemos generado en el ejemplo anterior, ver√°s que tiene **muchos nodos**, algunas hojas con **muy pocas muestras** (incluso 1 o 2) y un gran n√∫mero de decisiones encadenadas.

Este tipo de √°rbol ocurre cuando dejamos que el modelo crezca **sin ning√∫n l√≠mite**, intentando capturar todos los detalles de los datos. El problema es que esto suele llevar a un comportamiento llamado **sobreajuste** (*overfitting*): el modelo se adapta tanto al conjunto de entrenamiento que pierde capacidad para generalizar en datos nuevos.

Para evitarlo, los √Årboles de Decisi√≥n permiten ajustar distintos **hiperpar√°metros**, que act√∫an como ‚Äúfrenos‚Äù para controlar cu√°nto puede crecer el √°rbol.

---

### Profundidad: sobreajuste vs subajuste

La **profundidad** de un √°rbol es el n√∫mero de decisiones (splits) que hay desde la ra√≠z hasta una hoja.

* **√Årbol muy profundo ‚Üí Overfitting**

  * Aprende reglas demasiado espec√≠ficas.
  * Crea nodos con muy pocas muestras.
  * Captura ruido en lugar de patrones.
  * Es exactamente lo que ocurre en el √°rbol grande de la imagen.

* **√Årbol poco profundo ‚Üí Underfitting**

  * Modelo demasiado simple.
  * No captura la estructura real de los datos.
  * Su accuracy suele ser baja.

* **Objetivo:**
  Encontrar un equilibrio: un √°rbol **lo bastante profundo** para aprender, pero **no tanto** como para memorizar.

---

### Hiperpar√°metros principales

Estos hiperpar√°metros permiten controlar el tama√±o del √°rbol y evitar el sobreajuste. Son muy f√°ciles de entender si pensamos que todos ellos **limitan cu√°nto puede dividirse el √°rbol**.

#### `max_depth`

Es el hiperpar√°metro m√°s importante. Controla la **profundidad m√°xima** del √°rbol.

* Si es muy grande ‚Üí √°rbol enorme ‚Üí *overfitting*.
* Si es muy peque√±o ‚Üí √°rbol demasiado simple ‚Üí *underfitting*.
* Es la herramienta principal para equilibrar el modelo.

#### `min_samples_split`

N√∫mero m√≠nimo de muestras necesarias para dividir un nodo.

* Si lo aumentas ‚Üí el √°rbol solo divide cuando hay suficientes datos.
* Evita splits innecesarios o aleatorios.

Ejemplo intuitivo:

> ‚ÄúNo dividas este nodo si solo hay 3 muestras, espera a tener m√°s.‚Äù

#### `min_samples_leaf`

N√∫mero m√≠nimo de muestras que debe contener una hoja.

* Evita hojas con 1 o 2 datos (muy inestables).
* Suaviza el √°rbol y reduce ruido.
* Mejora la generalizaci√≥n.

Ejemplo intuitivo:

> ‚ÄúNo permitas hojas que representen casos demasiado espec√≠ficos.‚Äù

---

### ¬øC√≥mo saber si hay overfitting o underfitting?

Es muy f√°cil usando **train vs test accuracy**. La regla general es esta:

**OVERFITTING (sobreajuste)**

> El modelo aprende demasiado bien el conjunto de *train* y generaliza mal.

Se√±ales claras:

* Accuracy **muy alta en train**
* Accuracy **claramente m√°s baja en test**

Ejemplo t√≠pico:

| Conjunto | Accuracy |
| -------- | -------- |
| Train    | 1.00     |
| Test     | 0.88     |

üîé Interpretaci√≥n:   
El modelo **memoriza** los datos de entrenamiento (√°rbol muy profundo, nodos con 1 muestra, reglas muy espec√≠ficas).

**UNDERFITTING (subajuste)**

> Modelo demasiado simple: no aprende los patrones.

Se√±ales claras:

* Accuracy **baja en train**
* Accuracy **similarmente baja en test**

Ejemplo t√≠pico:

| Conjunto | Accuracy |
| -------- | -------- |
| Train    | 0.75     |
| Test     | 0.72     |

üîé Interpretaci√≥n:   
El modelo no est√° capturando bien la estructura de los datos (√°rbol demasiado peque√±o, `max_depth` muy bajo).

**Buen ajuste (lo ideal)**

> Train y test tienen accuracy **parecida** y **ambas son razonablemente altas**.

Ejemplo:

| Conjunto | Accuracy |
| -------- | -------- |
| Train    | 0.94     |
| Test     | 0.92     |

‚úî No memoriza   
‚úî Generaliza bien  
‚úî No est√° infraentrenado

---

**C√≥digo Python para detectar overfitting o underfitting**

A nivel de c√≥digo, podemos comprobar si hay overfitting o underfitting comparando la **accuracy en train** con la **accuracy en test**.
Para ello basta con evaluar el modelo con `score()` sobre ambos conjuntos:

```python
# Accuracy en entrenamiento y en test
print("Accuracy TRAIN:", tree_full.score(X_train, y_train))
print("Accuracy TEST :", tree_full.score(X_test, y_test))
```

**¬øQu√© hace este c√≥digo?**

* `tree_full.score(X_train, y_train)`
  ‚Üí Calcula el porcentaje de aciertos del modelo sobre los datos **con los que se entren√≥**.
  ‚Üí Si esta accuracy es muy alta (por ejemplo 1.00), el modelo est√° ‚Äúmemorizando‚Äù.

* `tree_full.score(X_test, y_test)`
  ‚Üí Calcula el porcentaje de aciertos del modelo sobre **datos nuevos** que no ha visto antes.
  ‚Üí Si esta accuracy es claramente m√°s baja que la de train, indica **sobreajuste**.

---

## Ejemplo con ajuste de hiperpar√°metros

En este segundo ejemplo trabajaremos con el mismo **Google Colab** y el mismo **dataset Iris** utilizados anteriormente.
El objetivo es ver c√≥mo los hiperpar√°metros (`max_depth`, `min_samples_split`, `min_samples_leaf`) permiten controlar el tama√±o del √°rbol y mejorar su capacidad de generalizaci√≥n.

En el Colab encontrar√°s un apartado espec√≠fico llamado ‚ÄúAjuste de hiperpar√°metros ‚Äì Decision Trees‚Äù.

---

## M√©tricas de evaluaci√≥n

Para evaluar un √Årbol de Decisi√≥n podemos utilizar **las mismas m√©tricas de clasificaci√≥n** que ya estudiamos en el apartado de **KNN**.

* **Accuracy**
  Proporci√≥n de aciertos del modelo.

* **Matriz de confusi√≥n**
  Permite ver qu√© clases se confunden entre s√≠.

* **Classification report** - **Precision, Recall y F1-score** (M√°s avanzadas)
  √ötiles especialmente cuando las clases est√°n desbalanceadas.

Estas m√©tricas te permitir√°n saber si el √°rbol est√° funcionando bien, si confunde especies entre s√≠ y si necesita un mejor ajuste de hiperpar√°metros.

---

## An√°lisis de importancia

Una de las grandes ventajas de los √Årboles de Decisi√≥n es que no solo permiten visualizar las reglas que aprende el modelo, sino tambi√©n **medir qu√© variables han sido m√°s importantes** para tomar decisiones. Esto hace que los √°rboles sean modelos muy **interpretables**, especialmente en comparaci√≥n con otros algoritmos.

**Obtener la importancia de las variables**

Una vez entrenado el √°rbol, scikit-learn nos da un atributo llamado `.feature_importances_`, que indica cu√°nto ha contribuido cada variable a las divisiones del √°rbol.

```python
importances = tree_test.feature_importances_

for name, value in zip(features, importances):
    print(name, ":", value)
```

Los valores est√°n entre **0 y 1** y **suman 1**.
Cuanto mayor es el valor, m√°s ha utilizado el √°rbol esa feature para separar las clases.

Podemos visualizar estas importancias con un **gr√°fico** muy sencillo:

```python
plt.figure(figsize=(6, 4))
plt.barh(features, importances)
plt.xlabel("Importancia")
plt.title("Importancia de las variables")
plt.show()
```

Este tipo de gr√°fico es ideal para ver de un vistazo cu√°les son las variables clave.

![Gr√°fico EDA](../../0-img/importancia-features.png)

üß† **Interpretaci√≥n**

La importancia de una variable indica:

* **cu√°ntas veces** se ha usado esa feature para dividir el dataset
* **cu√°nto ha mejorado** la pureza del √°rbol cuando se ha usado
* **qu√© papel juega** en la separaci√≥n de las clases

Por ejemplo, si `petal_length` tiene la mayor importancia, significa que es la variable m√°s √∫til para separar *setosa*, *versicolor* y *virginica*.

:::warning Importante
La *importancia* de una variable **no implica causalidad**.  
Solo indica **lo √∫til que ha sido esa feature para que el √°rbol separe las clases**.  
El modelo usa esa variable porque funciona bien **en este dataset**, no porque provoque el resultado.
:::

---

## Automatizaci√≥n b√∫squeda hiperpar√°metros (GridSearchCV)

Para encontrar la mejor combinaci√≥n de hiperpar√°metros de un √Årbol de Decisi√≥n, podemos utilizar **GridSearchCV**, una herramienta de scikit-learn que prueba autom√°ticamente m√∫ltiples configuraciones y selecciona la que ofrece mejor rendimiento.

Este m√©todo es especialmente √∫til cuando no sabemos qu√© valores de `max_depth`, `min_samples_split` o `min_samples_leaf` son los m√°s adecuados.

**C√≥mo hacer una b√∫squeda de hiperpar√°metros**

GridSearchCV funciona as√≠:

1. Le damos un modelo base (por ejemplo, `DecisionTreeClassifier()`).
2. Le pasamos un **diccionario de hiperpar√°metros** con los valores que queremos probar.
3. Realiza validaci√≥n cruzada para todas las combinaciones posibles. Ver 5-fold Cross-Validation en los apuntes de KNN.
4. Devuelve el modelo que obtuvo mejor rendimiento.

---

### Ejemplo de diccionario de par√°metros

```python
param_grid = {
    "max_depth": [2, 3, 4, 5, None],        # 5 opciones
    "min_samples_split": [2, 5, 10],        # 3 opciones
    "min_samples_leaf": [1, 2, 5]           # 3 opciones
}
```

**GridSearchCV va a probar absolutamente todas las combinaciones posibles** de los valores que pongas en `param_grid`.
Es decir: hace una **b√∫squeda exhaustiva** (*grid search*) por la rejilla de hiperpar√°metros.

GridSearchCV har√°:

```
5 √ó 3 √ó 3 = 45 combinaciones
```

Es decir, **45 √°rboles distintos**. Si adem√°s usamos `cv=5`, entonces entrena cada uno **5 veces**, as√≠ que:

```
45 modelos √ó 5 validaciones = 225 entrenamientos
```

Por eso **no debes poner listas enormes**, porque puede tardar mucho.

---

### C√≥mo obtener el mejor √°rbol

```python
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

# Creamos un √°rbol base (sin configurar hiperpar√°metros)
# GridSearchCV ser√° quien pruebe distintas combinaciones
tree = DecisionTreeClassifier(random_state=42)

# Configuramos GridSearchCV
grid = GridSearchCV(
    estimator=tree,      # Modelo base que queremos optimizar
    param_grid=param_grid,  # Diccionario con las combinaciones de hiperpar√°metros a probar
    cv=5,                # Validaci√≥n cruzada de 5 particiones
    scoring="accuracy"   # M√©trica que usaremos para elegir el mejor modelo
)

# Entrenamos el GridSearch: prueba todas las combinaciones en param_grid
grid.fit(X_train, y_train)

# Mostramos la mejor combinaci√≥n encontrada
print("Mejores hiperpar√°metros:", grid.best_params_)

# Obtenemos directamente el √°rbol entrenado con los mejores par√°metros
best_tree = grid.best_estimator_
```

`best_tree` contiene el modelo √≥ptimo encontrado por GridSearchCV, ya entrenado y listo para evaluar en el conjunto de test.

:::tip C√≥mo elegir los valores para GridSearch
No hay una f√≥rmula m√°gica para elegir los valores de GridSearch.  
Empieza con rangos **peque√±os y realistas** seg√∫n el dataset,  
observa el comportamiento del √°rbol y ampl√≠a solo si hace falta.

En datasets sencillos como **Iris** suelen bastar profundidades entre **2 y 5**.  
En datasets reales y m√°s complejos puedes probar valores algo mayores, como **5 a 12**.
:::

---

## Flujo recomendado en un problema de √Årbol de Decisi√≥n

El proceso para resolver un problema con **Decision Trees** es muy intuitivo y permite interpretar las reglas que el modelo aprende. Adem√°s, es importante ajustar sus hiperpar√°metros para evitar que el √°rbol crezca demasiado y sobreajuste.

| Paso                                                 | ¬øQu√© se hace?                                                                                            | ¬øPor qu√© es importante?                                                                                  |
| ---------------------------------------------------- | -------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- |
| **Paso 1. EDA**                                      | Analizar cada variable, distribuciones, relaci√≥n con la target, identificar variables relevantes.        | Permite anticipar qu√© columnas podr√≠an ser √∫tiles para los splits del √°rbol.                             |
| **Paso 2. Preprocesamiento**                         | Imputar nulos, codificar categ√≥ricas (One-Hot o LabelEncoding), eliminar columnas irrelevantes.          | Los √°rboles no aceptan nulos y necesitan todas las variables en formato num√©rico. No requieren escalado. |
| **Paso 3. Ajuste de hiperpar√°metros (GridSearchCV)** | Usar una rejilla con `max_depth`, `min_samples_split`, `min_samples_leaf` para encontrar el mejor √°rbol. | Evita que el √°rbol crezca sin control, reduce el sobreajuste y mejora la generalizaci√≥n.                 |
| **Paso 4. Entrenamiento del modelo final**           | Entrenar un √°rbol con los **mejores hiperpar√°metros** encontrados en el paso anterior.                   | Garantiza que usamos el √°rbol √≥ptimo para ese dataset.                                                   |
| **Paso 5. An√°lisis overfitting / underfitting**      | Comparar accuracy en **train** y **test**. Si se buscaron los mejores hiperpar√°metros, el modelo deber√≠a estar equilibrado.                                                              | √Årbol demasiado profundo ‚Üí overfitting. √Årbol muy limitado ‚Üí underfitting.                               |
| **Paso 6. Visualizaci√≥n y an√°lisis del √°rbol**                  | Mostrar y analizar el √°rbol entrenado con `plot_tree` (con profundidad limitada para que sea legible).              | Permite ver c√≥mo decide el modelo, qu√© splits realiza y qu√© reglas aprende.                              |
| **Paso 7. Importancia de las variables**             | Revisar `feature_importances_` y representar sus valores.                                                | Muestra qu√© variables son las m√°s relevantes para el √°rbol y permite interpretar el modelo.              |
| **Paso 8. M√©tricas de evaluaci√≥n**                   | Accuracy, matriz de confusi√≥n.                                      | Permite medir el rendimiento final del modelo en datos nuevos.                                           |

---

## Actividad de seguimiento: Iris

Realiza un peque√±o proyecto completo con el dataset **Iris** completo siguiendo los pasos vistos en clase.

1. **EDA b√°sico**
2. **Preprocesamiento**
3. **Modelado:**
   * Entrena **KNN** (usando GridSearchCV)
   * Entrena un **√Årbol de Decisi√≥n**: busca la mejor selecci√≥n de hiperpar√°metros, muestra la importancia de las features, el √°rbol de decisi√≥n generado, etc.
4. **Evaluaci√≥n:**
   * Accuracy
   * Matriz de confusi√≥n
   * Breve comparaci√≥n entre ambos modelos

**Entrega:** Notebook (Colab) con todos los pasos y conclusiones.