---
title: "K-Nearest Neighbors (KNN)"
sidebar_position: 1
toc_max_heading_level: 5
description: "Introducci√≥n al algoritmo K-Nearest Neighbors (KNN) aplicado a problemas de clasificaci√≥n. C√≥mo funciona, por qu√© depende del preprocesamiento, c√≥mo elegir el mejor valor de k y c√≥mo evaluarlo con m√©tricas de clasificaci√≥n."
keywords: [KNN, Clasificaci√≥n, Machine Learning, vecinos m√°s cercanos, scikit-learn, m√©tricas, distancia, hiperpar√°metros]
---


El algoritmo **K-Nearest Neighbors (KNN)** es uno de los modelos m√°s sencillos e intuitivos de todo el aprendizaje supervisado.
Su idea principal es muy f√°cil de entender:

> **Para predecir la clase de una nueva observaci√≥n, el modelo ‚Äúmira‚Äù a los k puntos m√°s cercanos y decide seg√∫n lo que digan sus vecinos.**

No construye una f√≥rmula, ni ajusta par√°metros, ni aprende pesos.
Simplemente **compara distancias** entre observaciones. Por eso se le llama un modelo **basado en instancias**.

## C√≥mo funciona KNN

El algoritmo **K-Nearest Neighbors (KNN)** toma sus decisiones bas√°ndose en una idea muy sencilla:
**si un punto nuevo se parece a otros puntos que ya conocemos, deber√≠a pertenecer a la misma clase que ellos.**

Para aplicar esta idea, KNN utiliza tres elementos clave:

1. **Una medida de distancia** para saber qu√© observaciones est√°n cerca.
2. **Un par√°metro k**, que indica cu√°ntos vecinos mirar.
3. **Un proceso de votaci√≥n**, donde los vecinos ‚Äúdeciden‚Äù la clase final.

A continuaci√≥n explicamos cada uno de estos elementos.

---

### Distancias

Para que el modelo pueda saber cu√°les son los puntos ‚Äúm√°s cercanos‚Äù, es necesario medir la distancia entre observaciones.
La m√°s utilizada por defecto es la **distancia eucl√≠dea**, que se corresponde con la distancia recta entre dos puntos:

$$
d(p, q) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + \ldots + (p_n - q_n)^2}
$$

Esto significa que dos observaciones ser√°n consideradas ‚Äúsimilares‚Äù si:

* sus valores num√©ricos est√°n cerca entre s√≠,
* y sus variables categ√≥ricas han sido codificadas adecuadamente.

---

### El par√°metro k y su efecto en el modelo

El valor de **k** determina cu√°ntos vecinos toma en cuenta KNN para votar la clase final. Elegir un buen valor es fundamental porque controla el equilibrio entre **sobreajuste (overfitting)** y **subajuste (underfitting)** en el modelo:

**k muy peque√±o ‚Üí Overfitting (sobreajuste)**.  
El modelo se fija demasiado en los vecinos m√°s cercanos.
- Es muy sensible al ruido.
- Cambiar un solo punto puede modificar la predicci√≥n.
- Se adapta demasiado al conjunto de entrenamiento y generaliza peor.

‚û° Resultado: **accuracy muy alta en train, baja en test**.

**k muy grande ‚Üí Underfitting (subajuste)**.  
El modelo suaviza demasiado las decisiones.
- Considera tantos vecinos que pierde detalles importantes.
- Mezcla puntos de clases diferentes.
- La frontera de decisi√≥n es demasiado simple.

‚û° Resultado: **accuracy baja tanto en train como en test**.

El **objetivo** es encontrar un k intermedio que permita:
- un modelo estable,
- que generalice bien,
- sin sensibilidad excesiva al ruido.

Ese valor se suele encontrar probando varios valores de k y comparando el error en test. M√°s adelante haremos un gr√°fico ‚Äú*accuracy vs k*‚Äù que lo muestra visualmente.

---

### Proceso de predicci√≥n paso a paso

Cuando KNN tiene que decidir la clase de una nueva observaci√≥n, sigue tres pasos muy claros:

**1. Calcular todas las distancias**

Compara la observaci√≥n nueva con **todas** las del conjunto de entrenamiento.
Para cada observaci√≥n calcula una distancia (eucl√≠dea, Manhattan, etc.).

![Ejemplo KNN](../../0-img/knn-ejemplo.png)

**2. Seleccionar los k vecinos m√°s cercanos**

Ordena todas las distancias de menor a mayor y se queda con los **k puntos m√°s cercanos**.

Ejemplo:
Si k = 5 ‚Üí se toman los 5 puntos m√°s pr√≥ximos.

![Ejemplo KNN](../../0-img/knn-ejemplo-2.png)

**3. Votar la clase mayoritaria**

Los vecinos ‚Äúvotan‚Äù su clase.
La clase que m√°s se repita entre esos k vecinos es la que el modelo asigna.

Ejemplos:

* Si los 5 vecinos son `[1, 0, 1, 1, 0]` ‚Üí la clase predicha es **1**.
* Si est√°n empatados, KNN utiliza criterios internos para romper el empate (dependiendo de la implementaci√≥n).

![Ejemplo KNN](../../0-img/knn-ejemplo-3.png)

:::info PARTICULARIDAD DE KNN
üí° **KNN no entrena un modelo**
KNN **no ajusta par√°metros** como hacen la regresi√≥n log√≠stica o las redes neuronales.

> Solo guarda los datos de entrenamiento y los usa para comparar distancias durante la predicci√≥n.

Esto hace que:

* El entrenamiento sea pr√°cticamente instant√°neo.
* Pero la predicci√≥n sea m√°s costosa si hay muchos datos.
:::

---

## Cu√°ndo se utiliza y principales caracter√≠sticas

KNN es un modelo √∫til cuando:

* Queremos un primer modelo sencillo, r√°pido de implementar.
* El dataset no es muy grande (KNN calcula distancias para cada predicci√≥n).
* Las variables son significativas en t√©rminos de **similitud** (lo similar debe estar cerca).

:::tip ¬øQu√© significa la similitud?

KNN funciona bien cuando se cumple la siguiente idea:

> **Si dos observaciones son similares en sus features, deber√≠an pertenecer a la misma clase.**

Ejemplos (Titanic):
* Pasajeros con **Pclass = 1**, **Sex = female**, **Fare alto** ‚Üí suelen estar en la clase ‚Äú*sobrevivi√≥*‚Äù.
* Pasajeros **varones** en **tercera clase** ‚Üí suelen estar en ‚Äú*no sobrevivi√≥*‚Äù.

Puedes comprobarlo visualmente utilizando un **scatterplot coloreado por clase**, representando dos variables num√©ricas relevantes:

```python
sns.scatterplot(x="Age", y="Fare", hue="Survived", data=df)
```

![Gr√°fico EDA](../../0-img/scatterplot-knn.png)

**Interpretaci√≥n:**

* Si los puntos de la misma clase forman **grupos visibles** ‚Üí la similitud es √∫til y KNN probablemente funcionar√° bien.
* Si las clases est√°n totalmente **mezcladas** ‚Üí la distancia no ayuda y KNN no ser√° un buen modelo.

Este gr√°fico es lo m√°s parecido a ‚Äúver‚Äù c√≥mo funcionar√° KNN antes de ejecutarlo.

Ten en cuenta que:
üëâ ‚ÄúCon estas dos features, la similitud NO es √∫til.‚Äù
üëâ ‚ÄúSi a√±ades variables categ√≥ricas transformadas, la cosa cambia.‚Äù
:::

Caracter√≠sticas importantes de KNN:

* **No aprende una ecuaci√≥n**, solo almacena los datos.
* El entrenamiento es pr√°cticamente inmediato.
* El coste computacional ocurre durante la **predicci√≥n**, no durante el entrenamiento.
* Funciona especialmente bien cuando el dataset est√° **limpio, bien escalado y sin ruido**.

---

## Ventajas

* **Muy f√°cil de entender**.
* Sirve como modelo base para comparar con modelos m√°s complejos.
* Capaz de capturar relaciones no lineales.
* Suele ofrecer buenos resultados si el preprocesamiento es correcto.

## Limitaciones

* **Depende completamente de las distancias**, as√≠ que necesita:
  * escalado de variables,
  * ausencia de outliers muy fuertes,
  * buena codificaci√≥n categ√≥rica.
* Es lento cuando el dataset tiene miles o millones de registros (tiene que calcular muchas distancias).
* Sensible al ruido: un solo vecino ‚Äúraro‚Äù puede afectar la predicci√≥n.
* Elegir un buen valor de **k** es crucial (muy peque√±o ‚Üí sobreajuste, muy grande ‚Üí subajuste).

---

## Importancia del preprocesamiento

KNN es uno de los modelos m√°s sensibles al preprocesamiento de los datos.
Esto se debe a que **todas sus decisiones se basan en distancias**, y cualquier detalle en las features puede alterar completamente el resultado.

A continuaci√≥n vemos por qu√© el preprocesamiento es especialmente cr√≠tico en este modelo.

### Escalado de variables

En KNN, cada variable contribuye a la distancia.
Si una columna tiene valores mucho m√°s grandes que otra, dominar√° la distancia aunque no sea la m√°s relevante.

Ejemplo t√≠pico en Titanic:

* `Fare` puede llegar a 500
* `Age` solo llega a 80

Si no escalamos:

* Fare ‚Äúarrastra‚Äù la distancia,
* Age deja de tener peso,
* y KNN toma decisiones distorsionadas.

üí° **Conclusi√≥n:**
El escalado (StandardScaler o MinMaxScaler) es **obligatorio** en KNN.
Sin √©l, el modelo no funciona bien.

---

### Codificaci√≥n de variables categ√≥ricas

KNN calcula distancias entre filas. Para que pueda hacerlo, **todas las variables deben ser num√©ricas**.

Pero adem√°s:

* Una codificaci√≥n incorrecta puede inventar relaciones que no existen.
* Por ejemplo, si `Embarked` se codifica como `S=0`, `C=1`, `Q=2`, KNN interpretar√° que Q est√° ‚Äúm√°s lejos‚Äù de S que C, lo cual **no tiene sentido**.

Por eso, en variables **nominales** se requiere One-Hot Encoding.
Esto evita introducir un orden artificial que arruinar√≠a la distancia.

üí° **Conclusi√≥n:**
La codificaci√≥n correcta evita que KNN aprenda relaciones geom√©tricas que no son reales.

---

## Implementaci√≥n en Python

Partimos del punto en el que el **preprocesamiento ya est√° completado**.
Recuerda que en el **Paso 2** del preprocesamiento divid√≠amos el dataset en entrenamiento y prueba:

```python
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y
)
```

Y en el **Paso 3** aplic√°bamos todas las transformaciones necesarias √∫nicamente sobre el conjunto de *train* (imputaci√≥n de nulos, codificaci√≥n, escalado‚Ä¶) y luego las traslad√°bamos a *test* utilizando los par√°metros aprendidos.

En este apartado asumimos que **todo ese proceso ya est√° hecho**, por lo que comenzamos directamente a trabajar con los datos **ya preprocesados** para entrenar y evaluar el modelo KNN.

---

### Paso 1. B√∫squeda de la mejor *k*

Antes de entrenar definitivamente el modelo KNN, necesitamos elegir el valor √≥ptimo de **k**, es decir, cu√°ntos vecinos tendr√° en cuenta el algoritmo para tomar decisiones. Recuerda que la elecci√≥n de *k* influye directamente en el rendimiento del modelo.

Para determinar qu√© valor funciona mejor, utilizaremos un proceso llamado **validaci√≥n cruzada** (*cross-validation*).
Aqu√≠ veremos dos m√©todos:

* **Opci√≥n 1: 5-Fold Cross-Validation** (b√∫squeda manual de k)
* **Opci√≥n 2: GridSearchCV** (m√©todo m√°s avanzado)

---

#### Opci√≥n 1. 5-Fold Cross-Validation

La **validaci√≥n cruzada** es una t√©cnica para evaluar un modelo de manera m√°s fiable y evitar depender de una √∫nica divisi√≥n train/test.

En lugar de entrenar el modelo una sola vez, hacemos:

1. Dividir el conjunto de entrenamiento (X_train) en **5 subconjuntos** (folds).
2. Entrenar el modelo con 4 de ellos.
3. Validar con el fold restante.
4. Repetir el proceso cambiando el fold de validaci√≥n hasta completar las 5 combinaciones.
5. Calcular la **media de accuracy** obtenida.

Esto nos da una estimaci√≥n mucho m√°s robusta del rendimiento del modelo.

![5 Fold](../../0-img/knn-crossval.png)

El objetivo es:

> Probar varios valores de **k**, medir su rendimiento medio con cross-validation y elegir el que obtenga el mejor resultado.

---

##### Aplicaci√≥n de 5-FOLD CV con varias k

A continuaci√≥n se muestra un ejemplo desde k = 1 hasta k = 30:

```python
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
import numpy as np

# Rango de valores de k que queremos probar (del 1 al 30)
k_values = range(1, 31)

# Aqu√≠ iremos guardando la accuracy media de cada k
accuracy_scores = []

# Recorremos todos los posibles valores de k
for k in k_values:
    # Creamos un modelo KNN con ese valor de k
    knn = KNeighborsClassifier(n_neighbors=k)
    
    # Aplicamos validaci√≥n cruzada (5-fold) solo sobre el conjunto de entrenamiento
    #   - cross_val_score entrena y valida el modelo 5 veces (cv=5)
    #   - devuelve un array con 5 valores de accuracy (uno por cada fold)
    scores = cross_val_score(knn, X_train, y_train, cv=5)
    
    # Calculamos la media de esos 5 valores y la guardamos
    accuracy_scores.append(scores.mean())
```

Despu√©s de ejecutar este c√≥digo, habremos evaluado **30 modelos diferentes**, uno por cada valor de *k* (desde 1 hasta 30).
Para cada uno de esos modelos se realiza un proceso llamado **5-Fold Cross-Validation**, que funciona as√≠:

1. El conjunto de entrenamiento (*X_train_scaled*, *y_train*) se divide en **5 partes iguales** llamadas *folds*.
2. Para un valor concreto de k:
   * Se entrena el modelo **5 veces**, cada vez utilizando 4 folds para entrenar y **1 fold distinto** para validar.
   * En cada una de esas 5 repeticiones se obtiene una accuracy diferente.
3. Al final, calculamos la **media** de esas 5 accuracies.
   Esa media representa lo bien que funciona ese valor de k.
4. Repetimos el proceso para **cada k** del rango 1‚Äì30.

Este procedimiento nos da una estimaci√≥n mucho m√°s fiable que entrenar el modelo una sola vez, ya que utiliza diferentes particiones del conjunto de entrenamiento y reduce el riesgo de depender de un √∫nico train/validation.

---

##### Gr√°fico: Accuracy vs k

Una vez obtenida la accuracy media para cada valor de **k** mediante 5-Fold Cross-Validation, es muy √∫til visualizar c√≥mo cambia esa accuracy en funci√≥n de k.
Este gr√°fico nos permite ver:
* cu√°ndo el modelo mejora,
* cu√°ndo deja de mejorar,
* y cu√°ndo empieza a empeorar si usamos un k demasiado grande.

En lugar de representar el error, esta vez representamos directamente la **accuracy media**:

```python
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(k_values, accuracy_scores, marker='o')
plt.xlabel("Valor de k")
plt.ylabel("Accuracy medio (5-Fold)")
plt.title("Accuracy vs k (5-Fold Cross-Validation)")
plt.grid(True)
plt.show()
```

![Accuracy 5-Fold](../../0-img/accuracy-5fold.png)

El comportamiento t√≠pico de la accuracy es:

* **k peque√±o ‚Üí accuracy baja**
  El modelo usa pocos vecinos y es demasiado sensible al ruido.

* **Zona intermedia ‚Üí accuracy m√°xima**
  Aqu√≠ el modelo logra un equilibrio entre simplicidad y estabilidad.
  Este suele ser el **mejor valor de k**.

* **k grande ‚Üí la accuracy vuelve a bajar**
  El modelo empieza a mezclar demasiados vecinos y las clases se confunden.

---

##### Selecci√≥n de la mejor *k*

El valor √≥ptimo de k es aquel que:

> **consigue la mayor accuracy media en validaci√≥n cruzada**,
> sin que la curva empiece a caer despu√©s.

En otras palabras:

* buscamos el **pico m√°ximo** del gr√°fico,
* pero siempre comprobando que no sea un valor extremo (por ejemplo, k=1 suele dar accuracy artificialmente alta en train y mala en test).

En c√≥digo, podemos recuperar el valor as√≠:

```python
best_k = k_values[np.argmax(accuracy_scores)]
print("Mejor valor de k:", best_k)
```

---

#### Opci√≥n 2. GridSearchCV

`GridSearchCV` realiza **autom√°ticamente** todo el proceso anterior:

* Prueba varios valores de k
* Eval√∫a el modelo con cross-validation
* Devuelve el mejor

Sin embargo, es una t√©cnica m√°s avanzada que veremos m√°s adelante.

<!--

`GridSearchCV` es una herramienta de scikit-learn que **automatiza todo el proceso de b√∫squeda de hiperpar√°metros**, incluido el valor √≥ptimo de *k* en KNN.  
Hace exactamente lo mismo que el m√©todo manual anterior, pero de manera m√°s c√≥moda y estructurada:

- Prueba autom√°ticamente una lista de valores de k.  
- Aplica validaci√≥n cruzada para cada uno (por ejemplo, 5-fold).  
- Calcula la m√©trica media (accuracy por defecto).  
- Devuelve el mejor modelo y el mejor valor de k.

Es decir:

> **GridSearchCV = Validaci√≥n cruzada + B√∫squeda de hiperpar√°metros en un solo paso.**


```python
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier

# Definimos los valores de k que queremos probar
param_grid = {
    'n_neighbors': range(1, 31)
}

# Creamos el modelo base
knn = KNeighborsClassifier()

# Configuramos GridSearchCV:
# - knn: el modelo
# - param_grid: valores de k a probar
# - cv=5: validaci√≥n cruzada 5-fold
# - scoring='accuracy': m√©trica que optimiza
grid = GridSearchCV(
    estimator=knn,
    param_grid=param_grid,
    cv=5,
    scoring='accuracy'
)

# Entrenamos GridSearchCV con el conjunto de entrenamiento preprocesado
grid.fit(X_train, y_train)

# Extraemos el mejor valor de k
print("Mejor k encontrado:", grid.best_params_['n_neighbors'])

# Extraemos el mejor modelo ya entrenado
knn_best = grid.best_estimator_
```

**Ventajas de GridSearchCV**

* Automatiza la validaci√≥n cruzada.
* Devuelve directamente el modelo √≥ptimo.
* Facilita probar m√∫ltiples hiperpar√°metros (no solo k).
* Muy √∫til cuando los modelos son m√°s complejos (SVM, Random Forest, etc.).

:::info Nota sobre GridSearchCV

Si utilizas **GridSearchCV**, no necesitas realizar el **Paso 2 (Entrenamiento con la mejor k)**.

¬øPor qu√©?  
Porque el m√©todo:

1. Prueba todos los valores de k especificados.  
2. Aplica validaci√≥n cruzada para cada uno.  
3. Selecciona el mejor valor de k.  
4. **Entrena autom√°ticamente el modelo final** con ese mejor valor usando todo el conjunto de entrenamiento.

El modelo final ya est√° disponible en:

```python
knn_best = grid.best_estimator_
```

Este modelo se encuentra **totalmente entrenado y listo para predecir**, por lo que no es necesario volver a llamarlo a `fit()`.

:::

-->

---

### Paso 2. Entrenamiento con la mejor *k*

Una vez identificado el valor √≥ptimo de **k** mediante validaci√≥n cruzada, el siguiente paso es entrenar el modelo KNN definitivo utilizando **todo el conjunto de entrenamiento preprocesado** (`X_train` y `y_train`).

La l√≥gica es simple:

1. Elegimos `best_k` (el valor de k que obtuvo la mayor accuracy media en el paso anterior).
2. Creamos un modelo KNN con ese valor.
3. Lo entrenamos utilizando **todos los datos de entrenamiento**, no solo los folds.

De esta forma aprovechamos al m√°ximo la informaci√≥n disponible antes de evaluar en test.

```python
# Entrenar el modelo definitivo con toda la parte de entrenamiento
from sklearn.neighbors import KNeighborsClassifier

knn_final = KNeighborsClassifier(n_neighbors=best_k)
knn_final.fit(X_train, y_train)
```

**¬øPor qu√© entrenamos de nuevo?**

Durante la validaci√≥n cruzada, el modelo se entren√≥ **muchas veces**, pero:

* cada entrenamiento us√≥ solo **4/5** del conjunto de entrenamiento,
* y los folds iban cambiando.

Ahora que ya sabemos cu√°l es el mejor *k*, debemos entrenar el modelo final usando **el 100% del conjunto de entrenamiento**, para que aprenda con la m√°xima cantidad de datos posible antes de evaluar en test.

**Resultado de este paso**

Ahora ya tenemos un modelo KNN completamente entrenado con el mejor valor de *k*, listo para:

* hacer predicciones sobre `X_test`,
* y evaluar su rendimiento con las m√©tricas del siguiente apartado.

---

### Paso 3. Evaluaci√≥n mediante m√©tricas

Una vez entrenado el modelo con el mejor valor de **k**, el siguiente paso es evaluar su rendimiento en el conjunto de **test**.
Para ello utilizaremos las m√©tricas m√°s importantes en problemas de **clasificaci√≥n**:

* **Accuracy**
* **Matriz de confusi√≥n**
* **Precision**
* **Recall**
* **F1-score**
* **Support**

Estas m√©tricas nos permiten saber **qu√© tan bien est√° funcionando el modelo**, si se est√° equivocando m√°s en una clase que en otra, y si tiene problemas de equilibrio entre clases.

Antes de evaluar el modelo, necesitamos generar las predicciones sobre el conjunto de test:

```python
y_pred = knn_final.predict(X_test)
```

---

#### Accuracy

La **accuracy** mide el porcentaje de predicciones correctas sobre el total:

$$
\text{accuracy} = \frac{\text{aciertos}}{\text{total}}
$$

```python
from sklearn.metrics import accuracy_score

y_pred = knn_final.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print("Accuracy:", accuracy)
```

Salida conseguida:

```
Accuracy: 0.8549618320610687
```

**Interpretaci√≥n**

* **Accuracy muy alta (‚âà 0.95 o superior)** ‚Üí puede parecer excelente, pero hay que tener cuidado: podr√≠a indicar sobreajuste (el modelo memoriza el entrenamiento y no generaliza bien), o un fuerte desbalanceo de clases (el modelo acierta porque predice siempre la clase mayoritaria).
* **Accuracy alta (‚âà 0.85 - 0.95)** ‚Üí indica buen rendimiento global, pero no garantiza que el modelo funcione bien para *todas* las clases.
* **Accuracy moderada (‚âà 0.70 ‚Äì 0.85)** ‚Üí el modelo capta parte del patr√≥n, pero a√∫n puede estar fallando en clases minoritarias o con datos ruidosos.
* **Accuracy baja (< 0.70)** ‚Üí el modelo no est√° identificando correctamente la estructura del problema o las clases est√°n muy mezcladas.

üí° **Pero cuidado:**
La accuracy por s√≠ sola puede ser enga√±osa, especialmente si las clases est√°n desbalanceadas o si el modelo tiende a predecir siempre la clase mayoritaria.

---

#### Matriz de confusi√≥n

La **matriz de confusi√≥n** es una herramienta fundamental en clasificaci√≥n, porque permite ver exactamente **c√≥mo** y **d√≥nde** se equivoca el modelo.
No solo indica cu√°ntos aciertos tiene, sino *qu√© tipo de errores* comete:

* **TP (True Positives)** ‚Üí predijo positivo y era positivo
* **TN (True Negatives)** ‚Üí predijo negativo y era negativo
* **FP (False Positives)** ‚Üí predijo positivo pero era negativo
* **FN (False Negatives)** ‚Üí predijo negativo pero era positivo

Representarla como un **mapa de calor** facilita enormemente la interpretaci√≥n.

```python
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Calcular la matriz de confusi√≥n
cm = confusion_matrix(y_test, y_pred)

# Representaci√≥n como mapa de calor
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["Predicci√≥n: 0", "Predicci√≥n: 1"],
            yticklabels=["Real: 0", "Real: 1"])
plt.title("Matriz de Confusi√≥n")
plt.xlabel("Predicciones")
plt.ylabel("Valores reales")
plt.show()
```

**C√≥mo interpretar el mapa de calor**

![Gr√°fico EDA](../../0-img/matriz-confusion.png)

**Interpretaci√≥n general:**

* **TN (arriba izquierda)**: n√∫mero de negativos bien clasificados
* **TP (abajo derecha)**: n√∫mero de positivos bien clasificados
* **FP (arriba derecha)**: el modelo predijo positivo pero era negativo
  * error t√≠pico cuando el modelo es demasiado ‚Äúoptimista‚Äù
* **FN (abajo izquierda)**: el modelo predijo negativo pero era positivo
  * error cr√≠tico en muchos problemas (fraude, enfermedad, fallos, etc.)

**¬øQu√© nos indica una buena matriz de confusi√≥n?**

* Los n√∫meros grandes deben estar en la **diagonal principal** (TN y TP).
* Los valores **fuera de la diagonal** deben ser lo m√°s peque√±os posible.
* Muchos **FN** ‚Üí el modelo est√° ‚Äúdejando pasar‚Äù positivos ‚Üí problema si queremos detectar casos sensibles (fraudes, enfermedades‚Ä¶).
* Muchos **FP** ‚Üí el modelo dispara demasiadas alarmas falsas.


**üí° Conclusi√≥n**

> La matriz de confusi√≥n no solo dice ‚Äúcu√°nto acierta el modelo‚Äù,
> sino **en qu√© acierta y en qu√© falla**, lo cual es esencial para entender si el modelo es realmente √∫til para la tarea.

<!--

---


#### Classification report

Estas m√©tricas aparecen juntas en el **classification_report**, una herramienta muy √∫til que resume el rendimiento por clase.

```python
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))
```

La salida t√≠pica es as√≠:

```
              precision    recall  f1-score   support
0              0.86        0.92      0.89       163
1              0.85        0.75      0.80        99
```

Vamos a ver qu√© significa cada m√©trica:

---

##### Precision

$$
\text{precision} = \frac{TP}{TP + FP}
$$

**Qu√© mide:**
De todas las predicciones positivas y negativas realizadas por el modelo, ¬øcu√°ntas son correctas?

**Precision baja** ‚Üí el modelo da **falsos positivos**.

**Conclusi√≥n de precisi√≥n en la tabla anterior:**

Los valores de **precision** obtenidos para ambas clases son altos y muy similares:

* **Clase 0**: precision = **0.86**
* **Clase 1**: precision = **0.85**

Esto significa que:

> Cuando el modelo predice una clase (ya sea 0 o 1), suele acertar con una alta fiabilidad.

En otras palabras:

* **El modelo comete pocos falsos positivos** en ambas clases.
* Las predicciones positivas (para cada clase) son bastante ‚Äúde confianza‚Äù.
* No existe un sesgo claro hacia una de las dos clases en t√©rminos de precision.

---

##### Recall

$$
\text{recall} = \frac{TP}{TP + FN}
$$

**Qu√© mide:**
De todos los supervivientes reales, ¬øcu√°ntos detecta el modelo?

**Recall baja** ‚Üí el modelo falla y comete **falsos negativos** (los m√°s graves en este contexto).

**Conclusi√≥n recall**:

Los valores de **recall** muestran un comportamiento m√°s desigual entre las dos clases:

* **Clase 0**: recall = **0.92**
* **Clase 1**: recall = **0.75**

Esto significa que:

> El modelo identifica muy bien los casos de la **clase 0**, pero deja escapar una parte m√°s significativa de los casos reales de la **clase 1**.

En otras palabras:

* El modelo comete **pocos falsos negativos** en la clase 0 (predice correctamente la mayor√≠a de los ‚Äú0‚Äù reales).
* En la clase 1, el modelo tiene m√°s dificultades para detectar todos los positivos reales ‚Üí **hay m√°s falsos negativos**.
* Esta diferencia indica que la **sensibilidad del modelo es mayor para la clase 0** que para la clase 1.

No es necesariamente un problema grave, pero se√±ala que:

* La clase 1 es m√°s dif√≠cil de identificar.
* El modelo puede beneficiarse de ajustes (k distinto, o modelos m√°s complejos) si la clase positiva fuera especialmente importante en el problema.

:::tip Precision vs Recall

Vamos a partir de esta idea:

**Precision = de lo que predije como positivo, ¬øcu√°ntos eran realmente positivos?**  
**Recall = de todos los positivos reales, ¬øcu√°ntos detect√© como positivos?**

Aunque ambas m√©tricas parecen parecidas, miden aspectos muy distintos del rendimiento de un modelo de clasificaci√≥n.

Imagina una cesta llena de manzanas rojas y verdes. Tu tarea es identificar solo las **manzanas rojas** (clase positiva).

**PRECISION: ‚ÄúDe las que saqu√© como rojas, ¬øcu√°ntas eran realmente rojas?‚Äù**
- Si saco 10 manzanas y 9 son rojas ‚Üí **precision = 0.9 (alta)**  
- Si saco 10 y solo 3 son rojas ‚Üí **precision baja**

üëâ La precision mide **cu√°n fiables son tus predicciones positivas**.

**RECALL: ‚ÄúDe TODAS las manzanas rojas que hab√≠a en la cesta, ¬øcu√°ntas encontr√©?‚Äù**
- Si hab√≠a 20 rojas y saco 18 ‚Üí **recall = 0.9 (alto)**  
- Si saco solo 5 ‚Üí **recall = 0.25 (bajo)**

üëâ El recall mide **cu√°ntos positivos reales has logrado detectar**.

**Resumen f√°cil**

- **Precision** ‚Üí calidad de los positivos predichos (evitar falsos positivos).  
- **Recall** ‚Üí cantidad de positivos encontrados (evitar falsos negativos).

:::

---

##### F1-score

$$
\text{F1} = 2 \cdot \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}
$$

**Qu√© mide:**
Equilibrio entre precision y recall.
Muy √∫til cuando:

* las clases est√°n desbalanceadas
* hay un coste diferente de los errores

**Conclusi√≥n sobre la tabla anterior**

El **F1-score** combina precision y recall en una sola m√©trica, de forma que solo ser√° alto cuando **ambas** lo sean.
En la tabla anterior:

* **Clase 0 (F1 = 0.89)** ‚Üí el modelo logra un excelente equilibrio entre acertar cuando predice 0 y detectar correctamente los 0 reales.
* **Clase 1 (F1 = 0.80)** ‚Üí el equilibrio es bueno, pero peor que en la clase 0, porque el modelo **deja escapar algunos positivos reales** (recall m√°s bajo).

Esto significa que:

* El modelo tiene un rendimiento muy **equilibrado y s√≥lido** en la clase 0, ya que tanto su precision como su recall son altos.
* En la clase 1, aunque la precision es buena, el recall es m√°s bajo, y eso hace que el F1-score tambi√©n baje. Esta clase es **m√°s dif√≠cil** para el modelo, normalmente porque hay menos ejemplos o m√°s variabilidad.
* Aun as√≠, un F1-score de 0.80 sigue siendo **un buen resultado** para un modelo b√°sico como KNN.

---

##### Support

Indica cu√°ntos ejemplos reales hay de cada clase en el test.

Ejemplo:

* support of 0 = 105 pasajeros ‚Äúno sobrevivi√≥‚Äù
* support of 1 = 74 pasajeros ‚Äúsobrevivi√≥‚Äù

**Conclusi√≥n**
Si una clase tiene menos support (como ‚Äú1‚Äù en Titanic), el modelo suele tener m√°s dificultades para predecirla bien.

-->

---

## Actividad de Seguimiento: Retenci√≥n de empleados

El objetivo de la actividad es realizar el **Preprocesamiento** sobre el dataset **Employee Attrition** y aplicar el modelo KNN sobre √©l, con el fin de **predecir la rotaci√≥n laboral** (abandono o permanencia en la empresa).

### Dataset

**Fichero:** [`employee.csv`](./0-datasets/employee.csv)    
**M√°s informaci√≥n:** [Kaggle](https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset)   
**Variable objetivo (target):** `Attrition`
* `Yes` ‚Üí el empleado dej√≥ la empresa
* `No` ‚Üí el empleado sigue trabajando   

**Nota**: recuerda que ya hiciste el EDA de este dataset, parte de ese an√°lisis para no repetir trabajo.


### Entregable

Un **cuaderno de Google Colab** con:

* **Preprocesamiento completo** del dataset
* **Aplicaci√≥n del modelo KNN**:
  * B√∫squeda de la mejor k
  * Entrenamiento con la mejor k
  * Evaluaci√≥n mediante m√©tricas
* Recuerda incluir **conclusiones y comentarios** sobre las tareas que vas realizando.
