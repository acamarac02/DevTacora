---
title: "Ridge y Lasso Regression"
sidebar_position: 6
toc_max_heading_level: 5
description: "Introducci√≥n a Ridge y Lasso Regression. Regularizaci√≥n en modelos lineales, funcionamiento interno, hiperpar√°metro alpha, diferencias entre L1 y L2, preprocesamiento y evaluaci√≥n."
keywords: [Ridge, Lasso, Regularizaci√≥n, Regresi√≥n Lineal, Machine Learning, scikit-learn, L1, L2]
---

Los modelos **Ridge Regression** y **Lasso Regression** son extensiones de la **Regresi√≥n Lineal** que incorporan un mecanismo de **regularizaci√≥n**, es decir, un t√©rmino adicional que penaliza modelos demasiado complejos.

Ambos modelos se utilizan para:

- Reducir **overfitting**
- Controlar coeficientes grandes
- Mejorar la **generalizaci√≥n**
- (En el caso de Lasso) realizar **selecci√≥n autom√°tica de variables**

La idea central es sencilla:

> ‚ÄúPreferimos modelos m√°s simples, incluso si encajan un poco peor en entrenamiento, porque generalizan mejor.‚Äù

---

## Idea principal de la regularizaci√≥n

En una **regresi√≥n lineal cl√°sica**, el modelo busca los coeficientes que **minimizan el error de predicci√≥n en entrenamiento**, sin tener en cuenta la complejidad del modelo.

Cuando el dataset tiene **muchas variables**, **ruido** o **variables correlacionadas**, esto puede provocar:

* coeficientes excesivamente grandes
* modelos muy sensibles a peque√±as variaciones de los datos
* **overfitting** (buen rendimiento en entrenamiento, peor en test)

La **regularizaci√≥n** a√±ade un t√©rmino de penalizaci√≥n que limita el tama√±o de los coeficientes, bas√°ndose en las siguientes premisas:

* Coeficientes grandes ‚Üí modelo sensible e inestable
* Coeficientes peque√±os ‚Üí modelo m√°s suave y robusto

La regularizaci√≥n act√∫a como una **fuerza que empuja los coeficientes hacia 0**:

* sin anularlos completamente (Ridge)
* o permitiendo eliminar algunos (Lasso)

---

## Funcionamiento interno del modelo

### Regresi√≥n Lineal (recordatorio)

La regresi√≥n lineal minimiza la siguiente funci√≥n de coste:

$$
\text{Error} = \text{MSE}
$$

Es decir, solo se preocupa por ajustar lo mejor posible los datos de entrenamiento.

---

## Ridge Regression (Regularizaci√≥n L2)

Ridge a√±ade un t√©rmino de penalizaci√≥n basado en el **cuadrado de los coeficientes**.

### Funci√≥n de coste (Ridge)

$$
\text{Error} = \text{MSE} + \alpha \sum \beta_j^2
$$

Donde:

- $ beta_j $ son los coeficientes del modelo
- $ alpha $ controla la fuerza de la regularizaci√≥n (hiperpar√°metro cuyo valor especificamos nosotros)

### Qu√© hace Ridge en la pr√°ctica

- Reduce el valor de los coeficientes
- Evita coeficientes extremadamente grandes
- **No elimina variables** (los coeficientes rara vez llegan a 0)

Ridge es especialmente √∫til cuando:

- hay **multicolinealidad**
- muchas variables aportan informaci√≥n parcial
- se quiere estabilidad en el modelo

---

## Lasso Regression (Regularizaci√≥n L1)

Lasso introduce una penalizaci√≥n basada en el **valor absoluto de los coeficientes**.

### Funci√≥n de coste (Lasso)

$$
\text{Error} = \text{MSE} + \alpha \sum |\beta_j|
$$

### Qu√© hace Lasso en la pr√°ctica

- Reduce coeficientes
- **Fuerza a que algunos coeficientes sean exactamente 0**
- Realiza **selecci√≥n autom√°tica de variables**

---

## Diferencia clave entre Ridge y Lasso

| Aspecto | Ridge | Lasso |
|-------|-------|-------|
| Tipo de penalizaci√≥n | L2 (cuadrado) | L1 (valor absoluto) |
| Reduce coeficientes | ‚úî S√≠ | ‚úî S√≠ |
| Elimina variables | ‚ùå No | ‚úî S√≠ |
| Selecci√≥n de features | ‚ùå No | ‚úî S√≠ |
| Estabilidad | Muy alta | Menor si variables est√°n correlacionadas |


:::tip ¬øC√ìMO FUNCIONAN RIDGE Y LASSO EN LA PR√ÅCTICA?

Supongamos un modelo de **regresi√≥n lineal** con tres variables:

* `x1`: superficie de la vivienda
* `x2`: n√∫mero de habitaciones
* `x3`: una variable poco relevante (ruido)

**Regresi√≥n lineal (sin regularizaci√≥n)**

Tras entrenar el modelo, obtenemos estos coeficientes:

| Variable          | Coeficiente |
| ----------------- | ----------- |
| x1 (superficie)   | 0.85        |
| x2 (habitaciones) | 1.20        |
| x3 (ruido)        | **4.50**    |

üìå Observaci√≥n:

* El coeficiente de `x3` es muy grande
* El modelo est√° usando una variable poco importante para ajustar mejor el entrenamiento
* Esto suele indicar **overfitting**

---

**Ridge Regression (regularizaci√≥n L2)**

Entrenamos ahora un modelo Ridge con un valor moderado de `alpha`.

| Variable          | Coeficiente |
| ----------------- | ----------- |
| x1 (superficie)   | 0.72        |
| x2 (habitaciones) | 0.98        |
| x3 (ruido)        | **0.60**    |

üìå Qu√© ha ocurrido:

* Todos los coeficientes se han reducido
* El coeficiente de la variable ruidosa ha bajado mucho
* **Ninguna variable se elimina completamente**
* El modelo es m√°s estable y menos sensible al ruido

---

**Lasso Regression (regularizaci√≥n L1)**

Entrenamos ahora un modelo Lasso con un valor similar de `alpha`.

| Variable          | Coeficiente |
| ----------------- | ----------- |
| x1 (superficie)   | 0.70        |
| x2 (habitaciones) | 0.95        |
| x3 (ruido)        | **0.00**    |

üìå Qu√© ha ocurrido:

* Lasso ha reducido los coeficientes
* La variable `x3` ha sido **eliminada autom√°ticamente**
* El modelo es m√°s simple e interpretable

---

üëâ Todos los modelos pueden predecir razonablemente bien, pero:

* Ridge y Lasso suelen **generalizar mejor**
* Lasso produce modelos m√°s simples
* Ridge es m√°s estable cuando las variables est√°n correlacionadas

:::

---

## El hiperpar√°metro `alpha`

El par√°metro **`alpha`** controla la intensidad de la regularizaci√≥n.

- `alpha = 0` ‚Üí regresi√≥n lineal cl√°sica
- `alpha` peque√±o ‚Üí regularizaci√≥n suave
- `alpha` grande ‚Üí modelo muy simple

Efecto de alpha:

- Si es **demasiado peque√±o** ‚Üí no soluciona overfitting
- Si es **demasiado grande** ‚Üí underfitting

Por eso **alpha debe ajustarse**, normalmente con validaci√≥n cruzada.

---

## Entrenamiento vs predicci√≥n

### Entrenamiento

Durante el entrenamiento:

1. El modelo busca coeficientes que minimicen **una funci√≥n de coste compuesta por dos partes**:

   * el error de predicci√≥n (por ejemplo, MSE)
   * un t√©rmino de penalizaci√≥n que castiga coeficientes grandes
2. La importancia de la penalizaci√≥n viene controlada por el hiperpar√°metro `alpha`
3. El modelo aprende un equilibrio entre:

   * ajustar bien los datos
   * y mantener un modelo simple y estable


---

### Predicci√≥n

Una vez el modelo ha sido entrenado, la **regularizaci√≥n ya no interviene directamente** en el c√°lculo de las predicciones.

En la fase de predicci√≥n:

* Se utiliza la **misma ecuaci√≥n lineal** que en una regresi√≥n lineal cl√°sica:
  $$
  \hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots
  $$
* No se a√±ade ning√∫n t√©rmino de penalizaci√≥n al calcular la predicci√≥n.
* La regularizaci√≥n ha actuado **solo durante el entrenamiento**, influyendo en los valores de los coeficientes.

Es decir, el modelo predice igual que una regresi√≥n lineal, pero usando **coeficientes previamente regularizados**, que suelen ser m√°s peque√±os y estables.

---

## Importancia del preprocesamiento (CR√çTICO)

A diferencia de √°rboles o Random Forest, **Ridge y Lasso son muy sensibles a la escala**.

| Aspecto | ¬øEs necesario? | Explicaci√≥n |
|------|------|------|
| Tratamiento de nulos | ‚úî S√≠ | No admiten valores nulos |
| Escalado | ‚úî **Imprescindible** | La penalizaci√≥n depende de la escala |
| Variables categ√≥ricas | ‚úî S√≠ | Requieren encoding |
| Outliers | ‚ö†Ô∏è Importante | Pueden afectar mucho a los coeficientes |

:::warning Escalado obligatorio
Si las variables no est√°n en la misma escala, la regularizaci√≥n penaliza m√°s a unas que a otras de forma incorrecta.
:::

---

## M√©tricas de evaluaci√≥n

Se usan las m√©tricas est√°ndar de regresi√≥n:

- **MAE**
- **MSE**
- **R¬≤**

No hay m√©tricas espec√≠ficas para Ridge o Lasso.

---

## Cu√°ndo usar Ridge o Lasso

### Ridge es buena opci√≥n cuando:

- Hay muchas variables correlacionadas
- Todas las variables aportan algo de informaci√≥n
- Se busca estabilidad y buen rendimiento

### Lasso es buena opci√≥n cuando:

- Se sospecha que muchas variables no son relevantes
- Se quiere un modelo m√°s interpretable
- Se necesita selecci√≥n autom√°tica de features

---

## Flujo recomendado en un problema de Ridge / Lasso

| Paso | Qu√© se hace | Por qu√© |
|----|----|----|
| 1. EDA | Distribuciones, outliers | Detectar problemas |
| 2. Preprocesamiento | Escalado + encoding | Imprescindible |
| 3. Entrenamiento | Ajustar `alpha` | Controlar complejidad |
| 4. Evaluaci√≥n | MAE, MSE, R¬≤ | Medir generalizaci√≥n |
| 5. Interpretaci√≥n | Coeficientes | Entender el modelo |
| 6. Comparaci√≥n | Comparar m√©tricas y resultados de los diferentes modelos | Elegir el mejor |

---

## Ejemplo: Ridge y Lasso

Para ver c√≥mo funcionan **Ridge y Lasso** en la pr√°ctica, puedes ejecutar este ejemplo utilizando el dataset **California Housing**.

üëâ **Puedes abrir el cuaderno aqu√≠:**
[Colab: Ridge y Lasso](../../0-datasets/ejemplo_ridge_lasso.ipynb)

---

## Actividad de seguimiento: Bike Sharing Dataset

Utiliza el **Bike Sharing Dataset** y compara:

- Regresi√≥n Lineal
- Ridge Regression
- Lasso Regression

Incluye:

- Ajuste de `alpha`
- Comparaci√≥n de m√©tricas
- An√°lisis de coeficientes
- Conclusiones razonadas

**Entrega:** Notebook (Colab) con conclusiones claras y justificadas.