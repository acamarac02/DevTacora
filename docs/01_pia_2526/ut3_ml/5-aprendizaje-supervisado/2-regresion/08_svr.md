---
title: "Support Vector Regression (SVR)"
sidebar_position: 8
toc_max_heading_level: 5
description: "Introducci√≥n a Support Vector Regression (SVR) en Machine Learning. Idea del margen epsilon, vectores soporte, kernels, hiperpar√°metros principales y comparaci√≥n con otros modelos de regresi√≥n."
keywords: [SVR, Support Vector Regression, SVM, Regresi√≥n, Machine Learning, scikit-learn, kernel, epsilon]
---

La **Support Vector Regression (SVR)** es un algoritmo de Machine Learning utilizado para **predecir valores num√©ricos**.

A diferencia de otros modelos de regresi√≥n, SVR no intenta minimizar directamente el error total, sino que busca:

> **Encontrar una funci√≥n lo m√°s simple posible que se ajuste a los datos, permitiendo peque√±os errores controlados.**

SVR es un modelo **potente**, especialmente en datasets peque√±os o medianos, pero tambi√©n **sensible al preprocesamiento y a los hiperpar√°metros**.

---

## Idea principal del algoritmo

![Gr√°fico EDA](../../0-img/svr.png)

La idea central de SVR es la siguiente:

> ‚ÄúAjustar una funci√≥n que prediga bien, manteni√©ndose lo m√°s plana posible, y tolerando errores peque√±os.‚Äù

Para ello, SVR introduce el concepto de **margen epsilon (Œµ)**:

* Se define un **tubo alrededor de la funci√≥n**
* Los errores **dentro del tubo no se penalizan**
* Solo se penalizan los puntos que quedan **fuera del margen**

Estos puntos se denominan **vectores soporte** y son los que realmente determinan el modelo.


---

## SVR como modelo basado en m√°rgenes

SVR pertenece a la familia de modelos **basados en m√°rgenes**:

* No intenta pasar exactamente por todos los puntos
* Busca un compromiso entre:
  * buen ajuste
  * simplicidad del modelo
* Solo algunos puntos influyen en la soluci√≥n final

Esto hace que SVR sea:
* robusto frente a cierto ruido
* sensible a outliers si no se regula bien

---

## Funcionamiento interno del modelo

El entrenamiento de SVR se basa en los siguientes conceptos:

### Margen epsilon (Œµ)

* Define una zona de tolerancia alrededor de la funci√≥n
* Errores menores que Œµ **no se penalizan**
* Cuanto mayor es Œµ:
  * m√°s tolerancia al error
  * modelo m√°s simple

---

### Vectores soporte

* Son los puntos que quedan:
  * fuera del margen Œµ
  * o justo en el borde
* Solo estos puntos influyen en la funci√≥n final
* El resto de observaciones **no afectan al modelo**

---

### Regularizaci√≥n (C)

SVR introduce el par√°metro **C**, que controla cu√°nto se penalizan los errores grandes:

* C grande ‚Üí el modelo intenta ajustarse mucho a los datos
* C peque√±o ‚Üí se permite m√°s error para ganar generalizaci√≥n

---

## Entrenamiento vs predicci√≥n

### Entrenamiento

Durante el entrenamiento, SVR:

1. Define un margen Œµ alrededor de la funci√≥n
2. Busca una funci√≥n lo m√°s plana posible
3. Penaliza solo los errores que superan Œµ
4. Ajusta la funci√≥n usando los vectores soporte

El proceso se basa en una **optimizaci√≥n matem√°tica**, no en reglas ni √°rboles.

---

### Predicci√≥n

Para predecir un nuevo dato se eval√∫a la funci√≥n aprendida

---

## Uso de SVR en regresi√≥n

### Cu√°ndo S√ç usarlo

SVR puede ser una buena opci√≥n cuando:

* El dataset es peque√±o o mediano
* Hay relaciones no lineales
* Se dispone de buen preprocesamiento
* Se busca un modelo potente sin usar ensembles

---

### Cu√°ndo NO es la mejor opci√≥n

SVR puede no ser ideal cuando:

* El dataset es muy grande (entrenamiento lento)
* Hay muchas variables sin escalar
* Se requiere interpretabilidad
* Hay muchos outliers no tratados

---

## Importancia del preprocesamiento

En SVR, el preprocesamiento es **cr√≠tico**:

| Aspecto               | ¬øEs necesario? | Explicaci√≥n                               |
| --------------------- | -------------- | ----------------------------------------- |
| Tratamiento de nulos  | ‚úî S√≠           | No admite valores nulos                   |
| Escalado              | ‚úî **Obligatorio** | Usa distancias y productos escalares     |
| Variables categ√≥ricas | ‚úî S√≠           | Deben codificarse                         |
| Outliers              | ‚ö†Ô∏è Muy importante | Afectan directamente al margen          |

> En la pr√°ctica, SVR **siempre debe combinarse con escalado**.

---

## Principales hiperpar√°metros

SVR es **muy sensible a los hiperpar√°metros**.

Los m√°s importantes son:

* `C`
* `epsilon`
* `kernel`
* `gamma` (seg√∫n el kernel)

---

### Par√°metro C

Controla el equilibrio entre:

* ajuste a los datos
* simplicidad del modelo

* C grande ‚Üí riesgo de overfitting
* C peque√±o ‚Üí posible underfitting

---

### Margen epsilon (`epsilon`)

Define cu√°nto error se tolera sin penalizar:

* Œµ peque√±o ‚Üí ajuste m√°s estricto
* Œµ grande ‚Üí modelo m√°s suave

---

### Kernel

El kernel define la forma de la funci√≥n aprendida.

Los m√°s comunes son:

* `linear`
* `rbf` (el m√°s utilizado)
* `poly`

El kernel permite a SVR modelar **relaciones no lineales**.

![Gr√°fico EDA](../../0-img/kernel-svr.png)

En el gr√°fico anterior:

* (a) Linear 
* (b) Polynomial 
* (c) Gaussian RBF 
* (d) Exponential RBF 

La imagen muestra c√≥mo **SVR cambia la forma de la funci√≥n aprendida seg√∫n el kernel utilizado**. Con el **kernel lineal** (a), el modelo ajusta una recta con margen, por lo que solo puede capturar relaciones lineales. Con el **kernel polin√≥mico** (b), la funci√≥n se curva suavemente y permite modelar relaciones no lineales simples. El **kernel Gaussiano RBF** (c) ofrece mayor flexibilidad, adapt√°ndose mejor a patrones no lineales complejos manteniendo una curva suave. Por √∫ltimo, el **kernel RBF exponencial** (d) genera un ajuste muy flexible y local, capaz de seguir variaciones muy finas de los datos, con mayor riesgo de sobreajuste si no se regulan bien los hiperpar√°metros.

---

### Par√°metro gamma (en kernels no lineales)

`gamma` indica **hasta qu√© distancia ‚Äúse nota‚Äù la influencia de cada punto de entrenamiento** en los kernels no lineales (como RBF o polin√≥mico).

* **gamma grande** ‚Üí cada punto solo influye en una zona muy cercana. El modelo se vuelve muy sensible a cambios locales y puede generar curvas muy onduladas, con **riesgo de sobreajuste**.
* **gamma peque√±o** ‚Üí cada punto influye en una regi√≥n amplia. El modelo es m√°s suave y generaliza mejor, pero puede **no capturar detalles importantes** y subajustar.

En la pr√°ctica, `gamma` controla la **flexibilidad del modelo**: valores altos hacen el modelo m√°s complejo y valores bajos lo hacen m√°s simple.

---

## Ajuste de hiperpar√°metros

Es habitual usar:

* validaci√≥n cruzada
* `GridSearchCV` o `RandomizedSearchCV`

Tabla orientativa:

| Dataset | C        | epsilon | kernel | Comentario                  |
| ------ | -------- | ------- | ------ | --------------------------- |
| Peque√±o | 1 ‚Äì 10   | 0.05‚Äì0.2 | rbf    | Buen equilibrio              |
| Mediano | 1 ‚Äì 100  | 0.05‚Äì0.1 | rbf    | Ajustar gamma con cuidado   |
| Grande  | ‚ùå       | ‚ùå       | ‚ùå     | SVR suele ser poco eficiente|

---

## Interpretabilidad del modelo

SVR **no es f√°cilmente interpretable**:

* No tiene coeficientes claros (salvo kernel lineal)
* No hay reglas ni √°rboles
* No ofrece importancia de variables

Su interpretaci√≥n es principalmente **geom√©trica** (m√°rgenes).

---

## M√©tricas de evaluaci√≥n

Se usan las m√©tricas habituales de regresi√≥n:

* **MAE**
* **MSE**
* **R¬≤**

Es importante evaluar el modelo en **train y test** para detectar:

* overfitting (C o gamma demasiado altos)
* underfitting (C o epsilon demasiado bajos)

---

## Flujo recomendado en un problema de SVR (Regresi√≥n)

| Paso                | Qu√© se hace                     | Por qu√©                         |
| ------------------- | ------------------------------- | ------------------------------- |
| 1. EDA              | Nulos, outliers, escalas        | SVR es sensible a la escala     |
| 2. Preprocesamiento | Escalado + encoding             | Requisito fundamental           |
| 3. Entrenamiento    | Ajuste de C, epsilon y kernel   | Modelo sensible                 |
| 4. Evaluaci√≥n       | MAE, MSE, R¬≤ + gr√°ficos         | Detectar overfitting            |
| 5. Comparaci√≥n      | Comparar con otros modelos      | Elegir mejor modelo             |

---

## Ejemplo: SVR para Regresi√≥n

Para ver c√≥mo funciona un **Support Vector Regressor** en la pr√°ctica, puedes ejecutar un ejemplo utilizando el dataset **California Housing**.

üëâ **Puedes abrir el cuaderno aqu√≠:**
[Colab: Support Vector Regression](../../0-datasets/ejemplo_svr.ipynb)


---

## Actividad de seguimiento: Bike Sharing Dataset

Utiliza el **Bike Sharing Dataset** y compara:

- Regresi√≥n Lineal
- KNN Regresi√≥n
- √Årbol de Decisi√≥n (Regresi√≥n)
- Random Forest (Regresi√≥n)
- Gradient Boosting
- SVR

Incluye:

- Ajuste de hiperpar√°metros
- M√©tricas de evaluaci√≥n
- An√°lisis de overfitting
- Conclusiones razonadas

**Entrega:** Notebook (Colab) con conclusiones claras y justificadas.
