---
title: "Random Forest Regresi√≥n"
sidebar_position: 5
toc_max_heading_level: 5
description: "Introducci√≥n a Random Forest Regression en Machine Learning. Funcionamiento del algoritmo (bootstrap y submuestreo de variables), diferencias con √°rboles de clasificaci√≥n, hiperpar√°metros principales, importancia de variables y m√©tricas de evaluaci√≥n."
keywords: [Random Forest, Bosques Aleatorios, Regresi√≥n, Random Forest Regression, Machine Learning, scikit-learn, bootstrap, bagging]
---

Los **Random Forest para Regresi√≥n (Random Forest Regression)** son algoritmos de Machine Learning utilizados para **predecir valores num√©ricos** combinando las predicciones de **muchos √°rboles de decisi√≥n**.

Un Random Forest es, literalmente, un *‚Äúbosque‚Äù* de √°rboles:

- Cada √°rbol aprende reglas *if‚Äìelse* como un Decision Tree
- Pero se entrena con **variaci√≥n** (datos y variables diferentes)
- La predicci√≥n final se obtiene **promediando** las predicciones de todos los √°rboles

En la pr√°ctica, Random Forest suele ser m√°s **robusto** y generaliza mejor que un √∫nico √°rbol, porque reduce el **overfitting** t√≠pico de los Decision Trees.

![Gr√°fico EDA](../../0-img/resumen-rfr.png)

---

## Idea principal del algoritmo

La idea es sencilla:

> ‚ÄúEn vez de confiar en un solo √°rbol (que puede sobreajustar), entrenamos muchos √°rboles diferentes y combinamos sus predicciones.‚Äù

Para conseguir √°rboles **diferentes**, Random Forest introduce dos fuentes de aleatoriedad:

1. **Bootstrap de filas (bagging)**: cada √°rbol se entrena con una muestra aleatoria de los datos
2. **Submuestreo de variables**: en cada split, el √°rbol solo puede probar un subconjunto aleatorio de features

Esta combinaci√≥n hace que los √°rboles no sean copias unos de otros, y por tanto el promedio final sea m√°s estable.

:::info Random Forest es un ensemble
Random Forest es un modelo **ensemble**, es decir, un modelo formado por la combinaci√≥n de muchos modelos simples (en este caso, √°rboles).

- Un √°rbol individual puede ser inestable y sobreajustar
- Un conjunto de √°rboles tiende a ser m√°s robusto
- La combinaci√≥n (media) reduce errores por ‚Äúcasualidades‚Äù del entrenamiento
:::

---

## Funcionamiento interno del modelo

Un Random Forest se entrena generando muchos √°rboles de decisi√≥n, pero **no todos ven exactamente los mismos datos ni las mismas variables**.

### Paso 1: Bootstrap (muestreo con reemplazo)

Para entrenar cada √°rbol:

- Se crea una muestra aleatoria del dataset **con reemplazo**
- Esto significa que:
  - algunas filas aparecen repetidas
  - algunas filas no aparecen en ese √°rbol

Ejemplo: si el dataset tiene 1.000 filas, cada √°rbol suele entrenarse con 1.000 filas muestreadas con reemplazo (por defecto), pero no ser√°n las mismas que en otro √°rbol.

Esto se conoce como **bagging** (*bootstrap aggregating*).

---

### Paso 2: Entrenar un √°rbol con submuestreo de variables

Mientras el √°rbol se construye:

- En cada nodo, el algoritmo **no prueba todas las variables**
- En su lugar, selecciona un subconjunto aleatorio de features (por ejemplo, `sqrt(n_features)`)

Luego:

- Eval√∫a posibles splits usando solo esas variables disponibles
- Elige el split que mejor reduce el error

:::info ¬øC√≥mo se elige la ‚Äúmejor divisi√≥n‚Äù en regresi√≥n?
En Random Forest para **regresi√≥n**, cada √°rbol usa el mismo criterio que un √°rbol de decisi√≥n de regresi√≥n:

- La divisi√≥n se elige buscando **minimizar el error**
- Habitualmente se usa **MSE (Mean Squared Error)** como medida del error

El objetivo de cada split es que, dentro de cada grupo, los valores del target sean lo m√°s parecidos posible.
:::

---

### Paso 3: Repetir para crear muchos √°rboles

El proceso se repite tantas veces como indique el hiperpar√°metro `n_estimators`:

- 100 √°rboles
- 200 √°rboles
- 500 √°rboles‚Ä¶

Cada √°rbol ser√° distinto porque:

- ha visto un bootstrap distinto
- y ha tomado decisiones basadas en subconjuntos aleatorios de variables

---

## Entrenamiento vs predicci√≥n

### Entrenamiento

Durante el entrenamiento, el Random Forest:

1. Genera muchos conjuntos bootstrap
2. Entrena un √°rbol por cada bootstrap
3. Introduce aleatoriedad en las features de cada split
4. Guarda todos los √°rboles

Este proceso puede ser **costoso computacionalmente**, porque no entrenamos un modelo, sino muchos.

---

### Predicci√≥n

Para predecir un nuevo dato:

1. El dato se pasa por **cada √°rbol**
2. Cada √°rbol devuelve un valor num√©rico (su predicci√≥n)
3. El Random Forest devuelve la **media** de todas las predicciones

> En regresi√≥n, Random Forest predice promediando.

Esto suele dar resultados m√°s estables que un √°rbol individual, especialmente si los datos tienen ruido.

---

## Random Forest en regresi√≥n vs clasificaci√≥n

El funcionamiento general es el mismo en ambos casos:

- muchos √°rboles
- bootstrap
- submuestreo de variables
- agregaci√≥n final

La diferencia est√° en **c√≥mo se combinan las predicciones**:

- **Clasificaci√≥n** ‚Üí votaci√≥n mayoritaria
- **Regresi√≥n** ‚Üí media (promedio)

---

## Uso de Random Forest en Regresi√≥n

### Cu√°ndo S√ç usarlos

Random Forest suele funcionar muy bien cuando:

- Hay relaciones no lineales
- El dataset tiene ruido moderado
- Se busca buen rendimiento con poco ajuste
- Se quiere un modelo robusto sin demasiada feature engineering

En muchos problemas tabulares, Random Forest es un modelo ‚Äútodoterreno‚Äù.

---

### Cu√°ndo NO funcionan bien

Puede no ser la mejor opci√≥n cuando:

- Hay much√≠simas variables (muy alta dimensionalidad)
- Se necesita interpretabilidad total (un bosque es menos interpretable que un √°rbol)
- El dataset es enorme y el entrenamiento se vuelve lento
- Se requiere extrapolar fuera del rango observado (no es su punto fuerte)

---

## Importancia del preprocesamiento

| Aspecto               | ¬øEs necesario? | Explicaci√≥n |
| --------------------- | -------------- | ---------- |
| Tratamiento de nulos  | ‚úî S√≠           | No admite valores nulos |
| Escalado              | ‚ùå No           | No usa distancias |
| Variables categ√≥ricas | ‚ö†Ô∏è Depende      | Hay que codificarlas (one-hot, ordinal, etc.) |
| Outliers              | ‚ö†Ô∏è Importante  | Un bosque es m√°s robusto que un √°rbol, pero outliers extremos a√∫n pueden afectar |

---

## Principales hiperpar√°metros

Random Forest suele rendir muy bien ‚Äúpor defecto‚Äù, pero estos hiperpar√°metros son clave para controlar rendimiento, overfitting y coste:

* `n_estimators`
* `max_depth`
* `min_samples_split`
* `min_samples_leaf`
* `max_features`


### N√∫mero de √°rboles (`n_estimators`)

Controla cu√°ntos √°rboles se entrenan. Por lo general, m√°s √°rboles dan predicciones m√°s estables (hasta cierto punto) pero con un mayor coste de entrenamiento

En general:
- 100 suele ser un buen punto de partida
- 200‚Äì500 puede mejorar en datasets complejos

---

### Profundidad m√°xima (`max_depth`)

Controla cu√°n complejos pueden ser los √°rboles. √Årboles muy profundos tiene m√°s riesgo de overfitting en cada √°rbol
pero el promedio del bosque suele reducirlo

Aun as√≠, limitar `max_depth` puede:
- acelerar entrenamiento
- mejorar generalizaci√≥n en datasets peque√±os/ruidosos

---

### Muestras m√≠nimas (`min_samples_split`, `min_samples_leaf`)

Igual que en un √°rbol:

- `min_samples_split`: m√≠nimo para dividir un nodo
- `min_samples_leaf`: m√≠nimo para que una hoja sea v√°lida

Subir estos valores suele:
- suavizar las predicciones
- reducir overfitting
- hacer el modelo m√°s estable

---

### N√∫mero de variables por split (`max_features`)

Este es uno de los hiperpar√°metros m√°s caracter√≠sticos del Random Forest.

Indica cu√°ntas variables se consideran **en cada split**.

- Menos variables ‚Üí √°rboles m√°s diferentes ‚Üí mejor ‚Äúdiversidad‚Äù
- Demasiadas variables ‚Üí √°rboles m√°s parecidos ‚Üí menos beneficio del ensemble

Valores t√≠picos:
- `sqrt` (muy com√∫n)
- `log2`
- un porcentaje (ej. `0.7`)

---

### Ajuste de hiperpar√°metros

Como en otros algoritmos:

- Se puede usar validaci√≥n cruzada
- Es habitual usar `GridSearchCV`

Tabla orientativa de rangos para empezar:

| Tama√±o del dataset | N¬∫ de registros | `n_estimators` | `max_depth` | `min_samples_leaf` | Comentario |
| ------------------ | --------------- | -------------- | ----------- | ------------------ | ---------- |
| Peque√±o            | < 1.000         | 100 ‚Äì 300      | 3 ‚Äì 10      | 2 ‚Äì 20             | Limitar complejidad para evitar overfitting |
| Mediano            | 1.000 ‚Äì 10.000  | 200 ‚Äì 500      | 5 ‚Äì 20      | 1 ‚Äì 10             | Buen equilibrio rendimiento/tiempo |
| Grande             | > 10.000        | 300 ‚Äì 800      | 10 ‚Äì None   | 1 ‚Äì 5              | M√°s √°rboles y profundidad pueden ayudar |

> Estos rangos sirven como punto de partida. Los mejores hiperpar√°metros dependen del dataset y deben ajustarse con validaci√≥n cruzada.

---

## Importancia de variables

Random Forest permite calcular **importancia de variables**, normalmente basada en:

- cu√°nto reduce el error cuando se usa una feature en los splits
- promediado a lo largo de todos los √°rboles

Esto es √∫til para:

- interpretaci√≥n parcial del modelo
- selecci√≥n de variables
- entender qu√© features aportan m√°s informaci√≥n

:::info Importancia ‚â† causalidad
Que una variable sea importante no significa que sea la causa del fen√≥meno.
Indica que el modelo la usa mucho para reducir el error, pero puede haber correlaciones o variables redundantes.
:::

---

## M√©tricas de evaluaci√≥n

En Random Forest Regression se usan las mismas m√©tricas que en otros modelos de regresi√≥n:

- **MAE** (Mean Absolute Error)
- **MSE** (Mean Squared Error)
- **R¬≤** (Coeficiente de determinaci√≥n)

---

## Flujo recomendado en un problema de Random Forest (Regresi√≥n)

| Paso                | Qu√© se hace                        | Por qu√© es importante |
| ------------------- | ---------------------------------- | --------------------- |
| 1. EDA              | Distribuciones, outliers, nulos     | Asegura calidad de datos |
| 2. Preprocesamiento | Limpieza + encoding categ√≥ricas     | No admite nulos, necesita num√©ricos |
| 3. Entrenamiento    | Ajustar hiperpar√°metros             | Controla rendimiento y coste |
| 4. Evaluaci√≥n       | MAE, MSE, R¬≤ + gr√°ficos             | Medir generalizaci√≥n |
| 5. Interpretaci√≥n   | Importancia de variables            | Entender el modelo |
| 6. Comparaci√≥n      | Comparar con otros modelos          | Elegir el mejor para el dataset |

---

## Ejemplo: Random Forest para Regresi√≥n

Para ver c√≥mo funciona un **Random Forest Regressor** en la pr√°ctica, puedes ejecutar este ejemplo utilizando el dataset **California Housing**.

üëâ **Puedes abrir el cuaderno aqu√≠:**
[Colab: Random Forest Regression](../../0-datasets/ejemplo_random_forest_regresion.ipynb)

---

## Actividad de seguimiento: Bike Sharing Dataset

Utiliza el **Bike Sharing Dataset** y compara:

- Regresi√≥n Lineal
- KNN Regresi√≥n
- √Årbol de Decisi√≥n (Regresi√≥n)
- Random Forest (Regresi√≥n)

Incluye:

- Ajuste de hiperpar√°metros (`n_estimators`, `max_depth`, `max_features`, etc.)
- M√©tricas de evaluaci√≥n
- Importancia de variables
- Conclusiones razonadas

**Entrega:** Notebook (Colab) con conclusiones claras y justificadas.
