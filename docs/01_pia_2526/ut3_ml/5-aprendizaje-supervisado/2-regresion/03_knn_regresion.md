---
title: "KNN Regresi√≥n"
sidebar_position: 2
toc_max_heading_level: 5
description: "Introducci√≥n a KNN Regression en Machine Learning. Funcionamiento del algoritmo, diferencias con KNN clasificaci√≥n, hiperpar√°metros principales, importancia del escalado, m√©tricas de evaluaci√≥n y ejemplos pr√°cticos con scikit-learn."
keywords: [KNN, KNN Regression, Regresi√≥n KNN, Machine Learning, vecinos m√°s cercanos, distancia, scikit-learn]
---

La **KNN Regresi√≥n (K-Nearest Neighbors Regression)** es un algoritmo de Machine Learning utilizado para **predecir valores num√©ricos** bas√°ndose en la informaci√≥n de los datos m√°s cercanos.

A diferencia de la Regresi√≥n Lineal, **KNN no aprende una f√≥rmula matem√°tica**, sino que realiza las predicciones **directamente a partir de los datos de entrenamiento**.

Por su simplicidad conceptual, KNN Regresi√≥n suele utilizarse como:

* Primer **modelo no lineal**
* Modelo de comparaci√≥n frente a regresi√≥n lineal
* Algoritmo introductorio para entender modelos basados en distancia

:::info Lectura recomendada
Para reforzar la comprensi√≥n de las diferencias entre **Regresi√≥n Lineal** y **KNN**, se recomienda la siguiente lectura:

üëâ **KNN vs Linear Regression: How to Choose the Right ML Algorithm**  
https://medium.com/@skytoinds/knn-vs-linear-regression-how-to-choose-the-right-ml-algorithm-4f6bf01a4202

El art√≠culo compara ambos modelos desde un punto de vista conceptual, destacando aspectos como la forma de la funci√≥n de predicci√≥n, la flexibilidad del modelo y el papel de los datos en cada enfoque.
:::


---

## Idea principal del algoritmo

La idea de KNN Regresi√≥n es muy intuitiva:

> ‚ÄúSi varios puntos cercanos tienen valores parecidos, un nuevo punto deber√≠a tener un valor similar.‚Äù

Para predecir un nuevo valor:

1. Se buscan los **k puntos m√°s cercanos**
2. Se calcula la **media** de sus valores objetivo
3. Esa media es la predicci√≥n final

![Gr√°fico EDA](../../0-img/knn-regressor.png)

---

## Funcionamiento del modelo

El funcionamiento interno de KNN Regresi√≥n sigue siempre los mismos pasos:

1. Elegir el valor de **k**
2. Calcular la **distancia** entre el punto nuevo y todos los puntos del dataset
3. Seleccionar los **k vecinos m√°s cercanos**
4. Calcular la **media** (o media ponderada) de sus valores objetivo

```text
Nuevo punto ‚Üí buscar vecinos ‚Üí promediar valores ‚Üí predicci√≥n
```

### C√°lculo de la distancia

Para decidir qu√© puntos son ‚Äúcercanos‚Äù, KNN utiliza una **m√©trica de distancia**.
La m√°s habitual es la **distancia eucl√≠dea**, aunque existen otras como Manhattan.

La distancia se calcula teniendo en cuenta **todas las variables de entrada**, por lo que:

* Las variables deben estar en la **misma escala**
* Variables con valores grandes pueden dominar la distancia

Por este motivo, el **escalado de los datos es obligatorio** en KNN.

---

### Entrenamiento vs predicci√≥n

KNN Regresi√≥n es un algoritmo basado en instancias:

* Durante el **entrenamiento**, el modelo **no aprende par√°metros**
* Simplemente **almacena el dataset de entrenamiento**
* El trabajo computacional ocurre en la **fase de predicci√≥n**

Cada vez que se realiza una predicci√≥n, el modelo debe:

* Calcular distancias a todos los puntos
* Buscar los vecinos m√°s cercanos
* Calcular la predicci√≥n final

Esto explica por qu√© KNN es muy barato de entrenar pero costoso en tiempo y memoria al predecir

---

### Interpretaci√≥n geom√©trica

KNN Regresi√≥n **no ajusta una recta ni un plano**.
La predicci√≥n depende √∫nicamente de la **regi√≥n local** del espacio de datos donde cae el nuevo punto.

Esto lo convierte en un modelo:

* Flexible
* No lineal
* Muy dependiente de la distribuci√≥n de los datos

![Gr√°fico EDA](../../0-img/knn-regressor-2.png)

:::info Modelos param√©tricos vs no param√©tricos

En Machine Learning, los modelos de regresi√≥n pueden clasificarse, de forma general, en **modelos param√©tricos** y **modelos no param√©tricos**.

Esta distinci√≥n no tiene que ver con si el modelo es simple o complejo, sino con **c√≥mo representa la relaci√≥n entre las variables de entrada y la variable objetivo**.

---

#### Modelos param√©tricos

Los **modelos param√©tricos** asumen de antemano una **forma concreta** para la funci√≥n de predicci√≥n.
Esa funci√≥n se describe mediante un **n√∫mero fijo de par√°metros**, independientemente de cu√°ntos datos tengamos.

La **Regresi√≥n Lineal** es un ejemplo claro de modelo param√©trico:

$$
\hat{y} = w_0 + w_1x_1 + \dots + w_nx_n
$$

El modelo siempre tiene la misma estructura (una combinaci√≥n lineal de las variables).
Durante el entrenamiento, **solo se ajustan los valores de los coeficientes**.

Aunque aumente la cantidad de datos, **la forma del modelo no cambia**, √∫nicamente cambian sus par√°metros.

---

#### Modelos no param√©tricos

Los **modelos no param√©tricos** no asumen una forma concreta para la funci√≥n de predicci√≥n.
En su lugar, permiten que la **estructura del modelo dependa directamente de los datos**.

**KNN Regresi√≥n** es un modelo no param√©trico:

* No aprende coeficientes
* No ajusta una funci√≥n expl√≠cita
* Utiliza directamente los datos de entrenamiento para realizar las predicciones

La forma de la funci√≥n de predicci√≥n depende de:
* la distribuci√≥n de los datos
* el valor de $ (k) $
* la m√©trica de distancia utilizada

![Gr√°fico EDA](../../0-img/lr-vs-knn.png)

:::

---

## Uso de KNN Regresi√≥n

### Cu√°ndo S√ç usarlo

KNN Regresi√≥n puede funcionar bien cuando:

* La relaci√≥n entre variables es **no lineal**
* El dataset no es excesivamente grande
* Los datos est√°n bien distribuidos
* Se dispone de un buen preprocesamiento

---

### Cu√°ndo NO funciona bien

KNN Regresi√≥n suele rendir mal cuando:

* El dataset es muy grande (coste computacional)
* Hay mucho ruido
* Existen muchos outliers
* El n√∫mero de variables es elevado (curse of dimensionality)

En la pr√°ctica:

> KNN es un buen modelo de referencia, pero rara vez es el mejor modelo final.

---

## Importancia del preprocesamiento

En KNN Regresi√≥n, el preprocesamiento **es obligatorio**.

| Aspecto                | ¬øEs necesario?   | Explicaci√≥n                       |
| ---------------------- | ---------------- | --------------------------------- |
| Tratamiento de nulos   | ‚úî S√≠             | No admite valores nulos           |
| Escalado               | ‚úî Obligatorio    | La distancia depende de la escala |
| Outliers               | ‚úî Muy importante | Pueden dominar la predicci√≥n      |
| Selecci√≥n de variables | ‚úî Recomendado    | Reduce ruido y dimensionalidad    |

:::warning Escalado obligatorio
Si las variables no est√°n en la misma escala, **la distancia no tiene sentido** y el modelo producir√° predicciones err√≥neas.
:::

---

## Principales hiperpar√°metros

KNN Regresi√≥n depende fuertemente de sus hiperpar√°metros, ya que estos determinan **c√≥mo se definen los vecinos y c√≥mo se calcula la predicci√≥n**.

Por este motivo, es habitual **buscar autom√°ticamente la mejor combinaci√≥n de hiperpar√°metros** utilizando **validaci√≥n cruzada**, del mismo modo que ya se ha hecho en los algoritmos de clasificaci√≥n.

En la pr√°ctica, esto se realiza mediante herramientas como **GridSearchCV**, que permiten evaluar distintas combinaciones y seleccionar la que ofrece mejor rendimiento medio.

---

### N√∫mero de vecinos (`n_neighbors`)

* **k peque√±o** ‚Üí modelo muy sensible al ruido (overfitting)
* **k grande** ‚Üí modelo muy suavizado (underfitting)

No existe un valor universal de \(k\).
El valor √≥ptimo debe ajustarse mediante **validaci√≥n cruzada**, evaluando el rendimiento del modelo en distintos subconjuntos de datos.

---

### Pesos (`weights`)

* `uniform` ‚Üí todos los vecinos pesan igual
* `distance` ‚Üí los vecinos m√°s cercanos tienen mayor influencia en la predicci√≥n

En muchos casos, `weights="distance"` mejora el rendimiento, especialmente cuando los datos no est√°n uniformemente distribuidos.

---

### Ajuste de hiperpar√°metros

Al igual que en clasificaci√≥n, en problemas de regresi√≥n es posible combinar **KNN Regresi√≥n**, **validaci√≥n cruzada** y **GridSearchCV** para:

* Probar distintos valores de `n_neighbors`
* Comparar diferentes esquemas de pesos
* Seleccionar autom√°ticamente la mejor configuraci√≥n

Esto permite obtener un modelo m√°s robusto y reducir el riesgo de **overfitting** o **underfitting**.

---

## M√©tricas de evaluaci√≥n

En KNN Regresi√≥n se utilizan las mismas m√©tricas que en otros modelos de regresi√≥n:

* **MAE** (Mean Absolute Error)
* **MSE** (Mean Squared Error)
* **R¬≤** (Coeficiente de determinaci√≥n)

---

## Flujo recomendado en un problema de KNN Regresi√≥n

| Paso                | Qu√© se hace                 | Por qu√© es importante    |
| ------------------- | --------------------------- | ------------------------ |
| 1. EDA              | Analizar escalas y outliers | KNN es sensible a ambos  |
| 2. Preprocesamiento | Limpieza y escalado         | Hace v√°lida la distancia |
| 3. Entrenamiento    | Ajustar hiperpar√°metros         | Controla el sesgo        |
| 4. Evaluaci√≥n       | MAE, MSE, R¬≤ + An√°lisis gr√°fico del rendimiento               | Medir rendimiento        |
| 5. Comparaci√≥n      | Comparar con otros modelos  | Determina qu√© modelo se adapta mejor a nuestro dataset |

---

## Ejemplo KNN Regresi√≥n

Para ver c√≥mo funciona un **KNN Regression** en la pr√°ctica, puedes ejecutar este ejemplo utilizando el dataset **California Housing**.

üëâ **Puedes abrir el cuaderno aqu√≠:**
[Colab: KNN Regression con California Housing](../../0-datasets/ejemplo_knn_regresion.ipynb)

---

## Actividad de seguimiento: Bike Sharing Dataset

Utiliza el **Bike Sharing Dataset** y compara:

* Regresi√≥n Lineal
* KNN Regresi√≥n

Recuerda que debes realizar:

* Entrenamiento con GridSearch
* An√°lisis mejores hiperpar√°metros
* M√©tricas de evaluaci√≥n
* Gr√°fica de an√°lisis de resultados (Valores reales vs predichos; Residuos)

**Usa el mismo Colab que la entrega anterior**.

**Entrega:** Notebook (Colab) con conclusiones razonadas.
