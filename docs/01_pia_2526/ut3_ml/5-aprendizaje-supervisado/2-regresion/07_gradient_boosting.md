---
title: "Gradient Boosting Regresi√≥n"
sidebar_position: 7
toc_max_heading_level: 5
description: "Introducci√≥n a Gradient Boosting Regression en Machine Learning. Funcionamiento del algoritmo (boosting secuencial y correcci√≥n de errores), diferencias con Random Forest, hiperpar√°metros principales como learning rate y n√∫mero de √°rboles, importancia de variables y m√©tricas de evaluaci√≥n."
keywords: [Gradient Boosting, Boosting, Regresi√≥n, Gradient Boosting Regression, Machine Learning, scikit-learn, ensemble, learning rate]
---

Los **Gradient Boosting para Regresi√≥n (Gradient Boosting Regression)** son algoritmos de Machine Learning utilizados para **predecir valores num√©ricos** combinando las predicciones de **muchos √°rboles de decisi√≥n entrenados de forma secuencial**.

A diferencia de Random Forest, donde los √°rboles son independientes, en Gradient Boosting:

* Los √°rboles se entrenan **uno detr√°s de otro**
* Cada nuevo √°rbol aprende a **corregir los errores del modelo anterior**
* La predicci√≥n final es la **suma de las contribuciones de todos los √°rboles**

En la pr√°ctica, Gradient Boosting suele lograr **muy buen rendimiento en problemas tabulares**, especialmente cuando se ajustan bien sus hiperpar√°metros.

---

## Idea principal del algoritmo

La idea central de Gradient Boosting es la siguiente:

> ‚ÄúEntrenar modelos simples de forma secuencial, donde cada nuevo modelo se centra en corregir los errores cometidos hasta ese momento.‚Äù

En lugar de construir muchos √°rboles independientes, Gradient Boosting construye un modelo **paso a paso**, mejor√°ndolo progresivamente.

Este enfoque se conoce como **boosting**.

---

## Gradient Boosting como modelo ensemble

Gradient Boosting es un modelo **ensemble**, pero de tipo **secuencial**, no paralelo.

* Cada √°rbol individual es un **modelo d√©bil** (*weak learner*)
* El conjunto de muchos √°rboles produce un **modelo fuerte**
* La mejora se consigue reduciendo principalmente el **sesgo** del modelo

:::info Boosting vs Bagging

* **Bagging (Random Forest)**: muchos modelos independientes que se promedian
* **Boosting (Gradient Boosting)**: modelos entrenados secuencialmente que corrigen errores
:::

---

## Funcionamiento interno del modelo

El entrenamiento de Gradient Boosting se basa en la idea de **residuos** (errores).

![Gr√°fico EDA](../../0-img/gradient_boosting.png)

### Paso 1: Modelo inicial

El algoritmo comienza con una predicci√≥n muy simple a partir de un **√°rbol de decisi√≥n**. Este primer modelo ya permite hacer una **predicci√≥n inicial**.

---

### Paso 2: C√°lculo de los errores (residuos)

Una vez hecha la predicci√≥n inicial:

* Se calcula el **error** de cada observaci√≥n
* En regresi√≥n, el error suele ser:

$$
\text{residuo} = y_{\text{real}} - y_{\text{predicho}}
$$

Estos residuos indican **d√≥nde y cu√°nto se equivoca el modelo**.

---

### Paso 3: Entrenar un √°rbol para corregir errores

* Se entrena un **√°rbol de decisi√≥n** usando como target los **residuos**
* El √°rbol aprende patrones sobre **qu√© tipo de datos generan m√°s error**

Este √°rbol no predice el valor final, sino **una correcci√≥n**.

---

### Paso 4: Actualizar la predicci√≥n

La predicci√≥n del modelo se actualiza:

$$
\text{nueva predicci√≥n} = \text{predicci√≥n anterior} + \text{learning rate} \times \text{predicci√≥n del √°rbol}
$$

El **learning rate** controla cu√°nto influye cada nuevo √°rbol.

---

### Paso 5: Repetir el proceso

El proceso se repite tantas veces como indique `n_estimators`:

* Cada √°rbol intenta corregir los errores que a√∫n quedan
* El modelo mejora progresivamente

---

### Resultado final

Una vez entrenados todos los √°rboles, la predicci√≥n final se obtiene **sumando las contribuciones de cada √°rbol**.

La predicci√≥n del modelo viene dada por la siguiente expresi√≥n:

$$
y_{\text{pred}} = y_1 + \eta \cdot r_1 + \eta \cdot r_2 + \cdots + \eta \cdot r_N
$$

donde:

* $ (y_1) $ es la **predicci√≥n inicial** del modelo (normalmente la media del target),
* $ (\eta) $ (*learning rate*) controla **cu√°nto aporta cada √°rbol**,
* $ (r_1, r_2, \ldots, r_N) $ son las **correcciones (errores o residuos)** predichas por cada uno de los √°rboles entrenados.

Cada √°rbol no predice directamente el valor final, sino una **correcci√≥n** que se a√±ade a la predicci√≥n anterior.
La suma de todas estas peque√±as correcciones da lugar a la **predicci√≥n final** del modelo.

---

## Entrenamiento vs predicci√≥n

### Entrenamiento

Durante el entrenamiento, Gradient Boosting:

1. Parte de una predicci√≥n inicial simple
2. Calcula errores
3. Entrena un √°rbol para corregirlos
4. Actualiza el modelo
5. Repite el proceso

Este entrenamiento es **secuencial**, por lo que **no puede paralelizarse f√°cilmente**.

---

### Predicci√≥n

Para predecir un nuevo dato:

1. Se aplica la predicci√≥n inicial
2. Se suman las correcciones de todos los √°rboles (ponderadas por el learning rate)
3. Se obtiene la predicci√≥n final

Es decir, se aplica la f√≥rmula descrita anteriormente:

$$
y_{\text{pred}} = y_1 + \eta \cdot r_1 + \eta \cdot r_2 + \cdots + \eta \cdot r_N
$$

> En regresi√≥n, Gradient Boosting predice sumando peque√±as correcciones sucesivas.

---

## Gradient Boosting en regresi√≥n vs clasificaci√≥n

La estructura general es la misma:

* entrenamiento secuencial
* correcci√≥n de errores
* combinaci√≥n de m√∫ltiples √°rboles

La diferencia est√° en:

* **Regresi√≥n** ‚Üí se corrigen errores num√©ricos (residuos)
* **Clasificaci√≥n** ‚Üí se corrigen errores de probabilidad/clase

---

## Uso de Gradient Boosting en Regresi√≥n

### Cu√°ndo S√ç usarlo

Gradient Boosting suele funcionar muy bien cuando:

* Hay relaciones complejas y no lineales
* Se busca **alto rendimiento predictivo**
* Se acepta un mayor coste de entrenamiento a cambio de precisi√≥n

Es habitual que supere a Random Forest si est√° bien ajustado.

---

### Cu√°ndo NO es la mejor opci√≥n

Puede no ser ideal cuando:

* El dataset es muy grande (entrenamiento lento)
* No se quiere ajustar muchos hiperpar√°metros
* Se necesita un modelo muy interpretable

---

## Importancia del preprocesamiento

| Aspecto               | ¬øEs necesario? | Explicaci√≥n                   |
| --------------------- | -------------- | ----------------------------- |
| Tratamiento de nulos  | ‚úî S√≠           | No admite valores nulos       |
| Escalado              | ‚ùå No           | No usa distancias             |
| Variables categ√≥ricas | ‚úî S√≠     | Deben codificarse             |
| Outliers              | ‚ö†Ô∏è Importante  | Puede sobreajustar a outliers |

---

## Principales hiperpar√°metros

Gradient Boosting es **m√°s sensible a los hiperpar√°metros** que Random Forest.

Los m√°s importantes son:

* `n_estimators`
* `learning_rate`
* `max_depth`
* `min_samples_split`
* `min_samples_leaf`
* `subsample`

---

### N√∫mero de √°rboles (`n_estimators`)

Indica cu√°ntos √°rboles se entrenan secuencialmente.

* M√°s √°rboles ‚Üí mayor capacidad
* Demasiados √°rboles ‚Üí riesgo de overfitting si no se regula

---

### Learning rate (`learning_rate`)

Controla cu√°nto contribuye cada √°rbol al modelo final.

* Valores peque√±os (0.01‚Äì0.1) ‚Üí aprendizaje lento y robusto
* Valores grandes (>0.2) ‚Üí aprendizaje r√°pido pero inestable

Existe una relaci√≥n clara:

> **learning rate peque√±o ‚Üí m√°s √°rboles necesarios**

---

### Profundidad del √°rbol (`max_depth`)

Los √°rboles en Gradient Boosting suelen ser **poco profundos**.

* Profundidades t√≠picas: 2‚Äì5
* √Årboles simples ‚Üí mejor generalizaci√≥n

---

### Submuestreo (`subsample`)

Indica qu√© porcentaje de datos se usa para entrenar cada √°rbol.

* Valores < 1 introducen aleatoriedad
* Ayuda a reducir overfitting

---

## Ajuste de hiperpar√°metros

Es habitual usar:

* validaci√≥n cruzada
* `GridSearchCV` o `RandomizedSearchCV`

Tabla orientativa:

| Dataset | `learning_rate` | `n_estimators` | `max_depth` | Comentario         |
| ------- | --------------- | -------------- | ----------- | ------------------ |
| Peque√±o | 0.05 ‚Äì 0.1      | 100 ‚Äì 300      | 2 ‚Äì 4       | Evitar overfitting |
| Mediano | 0.05 ‚Äì 0.1      | 200 ‚Äì 500      | 3 ‚Äì 5       | Buen equilibrio    |
| Grande  | 0.01 ‚Äì 0.05     | 500 ‚Äì 1000     | 3 ‚Äì 6       | Mayor capacidad    |

---

## Importancia de variables

Gradient Boosting permite calcular **importancia de variables**, basada en:

* cu√°nto reduce el error cada variable
* frecuencia de uso en los splits

Esto ayuda a:

* entender parcialmente el modelo
* detectar variables relevantes

---

## M√©tricas de evaluaci√≥n

Se usan las m√©tricas habituales de regresi√≥n:

* **MAE**
* **MSE**
* **R¬≤**

Es especialmente importante **evaluar si hay overfitting**. Se eval√∫a comparando las m√©tricas obtenidas en entrenamiento y en test:

* Si el rendimiento es muy bueno en train pero empeora claramente en test, el modelo est√° sobreajustando.
* Si los resultados son similares en ambos conjuntos, el ajuste es adecuado.

Como normal general podemos decir:

| Diferencia de (R^2) (train ‚àí test) | Interpretaci√≥n                         |
| ---------------------------------- | -------------------------------------- |
| (> 0.10)                           | **Overfitting claro**                  |
| (0.05 ‚Äì 0.10)                      | **Overfitting leve / moderado**        |
| (< 0.05)                           | **Buen ajuste y buena generalizaci√≥n** |

En la pr√°ctica, se comprueban tambi√©n las diferencias entre los MSE y MAE de ambos conjuntos.

---

## Flujo recomendado en un problema de Gradient Boosting (Regresi√≥n)

| Paso                | Qu√© se hace                     | Por qu√©               |
| ------------------- | ------------------------------- | --------------------- |
| 1. EDA              | Nulos, outliers, distribuciones | Evitar errores graves |
| 2. Preprocesamiento | Limpieza + encoding             | Requisito del modelo  |
| 3. Entrenamiento    | Ajuste de hiperpar√°metros       | Modelo sensible       |
| 4. Evaluaci√≥n       | MAE, MSE, R¬≤ + gr√°ficos         | Detectar overfitting  |
| 5. Interpretaci√≥n   | Importancia de variables        | Entender el modelo    |
| 6. Comparaci√≥n      | Comparar con RF y otros         | Elegir mejor modelo   |

---

## Comparaci√≥n r√°pida: Random Forest vs Gradient Boosting

| Aspecto                        | Random Forest  | Gradient Boosting        |
| ------------------------------ | -------------- | ------------------------ |
| Entrenamiento                  | Paralelo       | Secuencial               |
| √Årboles                        | Independientes | Dependientes             |
| Riesgo principal               | Subajuste      | Sobreajuste              |
| Sensibilidad a hiperpar√°metros | Baja           | Alta                     |
| Rendimiento m√°ximo             | Bueno          | Muy alto (bien ajustado) |

---

## Ejemplo: Gradient Boosting para Regresi√≥n

Para ver c√≥mo funciona un **Gradient Boosting Regressor** en la pr√°ctica, puedes ejecutar este ejemplo utilizando el dataset **California Housing**.

üëâ **Puedes abrir el cuaderno aqu√≠:**
[Colab: Gradient Boosting Regression](../../0-datasets/ejemplo_gradient_boosting.ipynb)

---

## Actividad de seguimiento: Bike Sharing Dataset

Utiliza el **Bike Sharing Dataset** y compara:

- Regresi√≥n Lineal
- KNN Regresi√≥n
- √Årbol de Decisi√≥n (Regresi√≥n)
- Random Forest (Regresi√≥n)
- Gradient Boosting

Incluye:

- Ajuste de hiperpar√°metros (`n_estimators`, `max_depth`, `max_features`, etc.)
- M√©tricas de evaluaci√≥n
- Importancia de variables
- An√°lisis de overfitting
- Conclusiones razonadas

**Entrega:** Notebook (Colab) con conclusiones claras y justificadas.