---
title: "Regresi√≥n Lineal"
sidebar_position: 1
toc_max_heading_level: 5
description: "Introducci√≥n a la Regresi√≥n Lineal en Machine Learning. Regresi√≥n lineal simple y m√∫ltiple, funcionamiento del modelo, interpretaci√≥n de coeficientes, m√©tricas de evaluaci√≥n (MSE, MAE, R¬≤), preprocesamiento necesario, supuestos del modelo y ejemplos pr√°cticos con Scikit-Learn."
keywords: [Regresi√≥n Lineal, Linear Regression, ML, Machine Learning, Regresi√≥n Simple, Regresi√≥n M√∫ltiple, MSE, MAE, R2, scikit-learn]
---

La **Regresi√≥n Lineal** es uno de los modelos m√°s simples y fundamentales de Machine Learning. 

Por su sencillez y estabilidad, la Regresi√≥n Lineal suele utilizarse como **modelo baseline** en problemas de regresi√≥n, es decir, el **primer modelo** que entrenamos para tener una referencia m√≠nima de rendimiento antes de probar modelos m√°s complejos.

---

## Regresi√≥n Lineal Simple vs. M√∫ltiple

La Regresi√≥n Lineal puede presentarse en dos formas principales, seg√∫n el n√∫mero de variables explicativas.

![Tipos regresi√≥n lineal](../../0-img/tipos_regresion_lineal.png)

### Regresi√≥n Lineal Simple

La **Regresi√≥n Lineal Simple** se utiliza cuando existe **una √∫nica variable independiente** (*feature*) que explica la variable objetivo.

Ejemplo cl√°sico:

* horas de estudio ‚Üí nota obtenida
* metros cuadrados ‚Üí precio de una vivienda
* antig√ºedad ‚Üí salario

El modelo aprende una **recta** que relaciona la variable de entrada con la salida.

Conceptualmente:

> ‚ÄúA medida que aumenta X, ¬øc√≥mo cambia Y?‚Äù

![Regresi√≥n lineal](../../0-img/regresion-lineal.png)

:::info
Aunque en la pr√°ctica real **la regresi√≥n lineal simple rara vez se utiliza como modelo final**, es fundamental estudiarla por varias razones:

* Es la **base conceptual** de la regresi√≥n lineal m√∫ltiple (es el mismo modelo con m√°s variables).
* Permite **visualizar** f√°cilmente la relaci√≥n entre una variable y la target.
* Se utiliza con frecuencia en **EDA** para analizar relaciones individuales y detectar tendencias u outliers.
* Facilita la **interpretaci√≥n de los coeficientes**, que luego se extiende al caso multivariable.

Por ello, aunque los modelos reales suelen ser **multivariables**, la regresi√≥n lineal simple es clave para **entender, interpretar y construir** modelos de regresi√≥n m√°s complejos.
:::

---

### Regresi√≥n Lineal M√∫ltiple

La **Regresi√≥n Lineal M√∫ltiple** es una extensi√≥n natural del modelo simple.
Se utiliza cuando la variable objetivo depende de **varias variables explicativas**.

Ejemplos reales:

* Precio vivienda = m¬≤ + n¬∫ habitaciones + zona + antig√ºedad
* Salario = experiencia + estudios + edad
* Ventas = publicidad + estacionalidad + precio

Aunque el modelo es m√°s complejo internamente, **para el usuario funciona igual**:
solo cambia el n√∫mero de variables que entran al modelo.

---

## Funcionamiento del modelo

La Regresi√≥n Lineal se basa en una idea muy sencilla:
**combinar las variables de forma lineal para predecir un valor num√©rico**.

El modelo aprende una funci√≥n de la forma:

$$
\hat{y} = w_0 + w_1x_1 + w_2x_2 + \dots + w_nx_n
$$

Donde:

* $ x_i $ ‚Üí valores de las variables de entrada (features)
* $ w_i $ ‚Üí coeficientes aprendidos por el modelo
* $ w_0 $ ‚Üí intercept o t√©rmino independiente
* $ \hat{y} $ ‚Üí valor predicho

El objetivo del entrenamiento es encontrar los valores de $ w_i $ que **minimicen el error** entre los valores reales y los predichos.

---

### Interpretaci√≥n geom√©trica

Seg√∫n el n√∫mero de variables:

* 1 variable ‚Üí **recta**
* 2 variables ‚Üí **plano**
* m√°s variables ‚Üí **hiperplano**

No es necesario profundizar en la geometr√≠a:
lo importante es entender que el modelo busca la **mejor superficie lineal** que se ajuste a los datos.

![Interpretaci√≥n geom√©trica](../../0-img/interpretacion-geometrica.png)

---

### Interpretaci√≥n de coeficientes

Una de las grandes ventajas de la Regresi√≥n Lineal es que es **altamente interpretable**.

* Si (w_i > 0):
  aumentar (x_i) **incrementa** el valor predicho.
* Si (w_i < 0):
  aumentar (x_i) **reduce** el valor predicho.
* Si (w_i = 0):
  la variable **no influye** en la predicci√≥n.

> El coeficiente indica **cu√°nto cambia la predicci√≥n cuando la variable aumenta una unidad**, manteniendo el resto constante.

---

## Uso de la Regresi√≥n Lineal

La Regresi√≥n Lineal se apoya en varios supuestos te√≥ricos, cuyo cumplimiento determinar√°n si el problema es adecuado para este algoritmo:

1. **Linealidad**: la relaci√≥n entre X e Y es aproximadamente lineal

![Linealidad](../../0-img/linealidad.png)

2. **Homoscedasticidad**: el error tiene varianza constante

![Homoscedasticidad](../../0-img/Homoscedasticidad.png)

3. **Baja multicolinealidad**: las variables no est√°n altamente correlacionadas. Si en la matriz de correlaci√≥n observas dos features con una correlaci√≥n muy alta, deber√°s tratarlas mediante feature engineering, eliminando una de ellas, etc.


### Cu√°ndo S√ç usarla

La Regresi√≥n Lineal es adecuada cuando:

* Existe una **relaci√≥n aproximadamente lineal** entre variables y target
* El dataset no es excesivamente complejo
* Se necesita **interpretabilidad**
* Se busca un **modelo base r√°pido y estable**

Ejemplos t√≠picos:

* Precio aumenta con metros cuadrados
* Ventas aumentan con inversi√≥n publicitaria
* Salario aumenta con experiencia

---

### Cu√°ndo NO funciona bien

La Regresi√≥n Lineal suele rendir mal cuando:

* Las relaciones son claramente **no lineales**
* Hay muchos **outliers**
* Existen **interacciones complejas** entre variables

En la pr√°ctica:

> Siempre conviene comparar la Regresi√≥n Lineal con modelos m√°s flexibles
> (√Årboles, Random Forest, KNN).

---

## Importancia del preprocesamiento

La Regresi√≥n Lineal es muy sensible a la calidad de los datos.

| Aspecto                | ¬øEs necesario?   | Explicaci√≥n                           |
| ---------------------- | ---------------- | ------------------------------------- |
| Tratamiento de nulos   | ‚úî S√≠             | El modelo no admite valores nulos     |
| Escalado               | ‚úî Recomendado    | Facilita interpretaci√≥n y estabilidad |
| Outliers               | ‚úî Muy importante | Distorsionan la recta                 |
| Eliminar variables irrelevantes | ‚úî Recomendado    | A√±aden ruido                          |

---

## Principales hiperpar√°metros

La Regresi√≥n Lineal cl√°sica **no tiene hiperpar√°metros relevantes**, ya que calcula directamente la soluci√≥n √≥ptima que minimiza el error cuadr√°tico medio.
Los hiperpar√°metros aparecen en extensiones regularizadas como Ridge o Lasso.

---

## M√©tricas de evaluaci√≥n

Son las m√©tricas estudiadas en el apartado Ideas generales:

* **MAE** (Mean Absolute Error)
* **MSE** (Mean Squared Error)
* **R¬≤**

:::tip Resumen r√°pido
* Cuanto **menor** sea el MSE, **mejor** es el modelo
* Cuanto **menor** sea el MAE, **mejor** es el modelo
* R¬≤ Cuanto **m√°s cercano a 1**, **mejor** es el modelo.
:::

---

## Atributos √∫tiles del modelo

La Regresi√≥n Lineal proporciona varios atributos clave:

* `coef_` ‚Üí coeficientes del modelo
* `intercept_` ‚Üí t√©rmino independiente

Estos atributos permiten **entender c√≥mo el modelo est√° tomando decisiones**.

---

## Flujo recomendado en un problema de Regresi√≥n Lineal

| Paso                | Qu√© se hace                     | Por qu√© es importante         |
| ------------------- | ------------------------------- | ----------------------------- |
| 1. EDA              | Analizar variables y relaciones | Detecta linealidad y outliers |
| 2. Preprocesamiento | Limpieza, escalado, selecci√≥n   | Mejora estabilidad            |
| 3. Entrenamiento    | Ajustar el modelo               | Obtener coeficientes          |
| 4. Evaluaci√≥n       | MAE, MSE, R¬≤  + An√°lisis gr√°fico del rendimiento                   | Medir rendimiento             |
| 5. Interpretaci√≥n   | Analizar coeficientes           | Entender el modelo            |
| 6. Comparaci√≥n      | Probar modelos flexibles        | Ver si el problema es lineal  |

---

## Ejemplo Regresi√≥n Lineal

Para ver c√≥mo funciona un **Linear Regression** en la pr√°ctica, puedes ejecutar este ejemplo utilizando el dataset **California Housing**.

üëâ **Puedes abrir el cuaderno aqu√≠:**
[Colab: Linear Regression con California Housing](../../0-datasets/ejemplo_regresion_lineal.ipynb)

üëâ **Dataset utilizado:**

Se utiliza el dataset **California Housing**, incluido directamente en la librer√≠a **scikit-learn**.  
Este dataset est√° basado en datos del **censo de California de 1990**.

El objetivo es predecir el **valor medio de la vivienda** (`MedHouseVal`) a partir de variables socioecon√≥micas y geogr√°ficas de cada bloque censal, como:

- ingresos medios (`MedInc`)
- antig√ºedad media de las viviendas (`HouseAge`)
- n√∫mero medio de habitaciones (`AveRooms`) y dormitorios (`AveBedrms`)
- poblaci√≥n y ocupaci√≥n media (`Population`, `AveOccup`)
- localizaci√≥n geogr√°fica (`Latitude`, `Longitude`)

Todas las variables son **num√©ricas**, lo que lo convierte en un dataset adecuado para introducir modelos de regresi√≥n lineales y comparar posteriormente con modelos m√°s complejos.

---

## Actividad de seguimiento: Bike Sharing Dataset

Realiza un peque√±o proyecto completo con el dataset [**Bike Sharing**](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset) siguiendo los pasos vistos en clase. El dataset es un conjunto de datos realista que recoge informaci√≥n sobre el n√∫mero de bicicletas alquiladas en un sistema de bike sharing.

El objetivo es predecir el **n√∫mero de alquileres** (atributo `cnt`) a partir de variables meteorol√≥gicas (temperatura, humedad, viento), variables temporales (estaci√≥n, mes, d√≠a de la semana, hora) y variables contextuales (si es d√≠a laborable, condiciones clim√°ticas).

:::info Features de tipo fecha
El dataset incluye una columna de fecha (`dteday`), que se proporciona con fines informativos y de an√°lisis exploratorio, pero **no se utiliza directamente como variable de entrada**.  
En su lugar, se emplean variables temporales derivadas ya incluidas en el dataset, que son m√°s adecuadas para los modelos de Machine Learning.
:::

Pasos a seguir:

1. **EDA b√°sico**
2. **Preprocesamiento**
3. **Modelado:** entrena **Linear Regression**
4. **Evaluaci√≥n**
5. **An√°lisis de atributos relevantes**

**Entrega:** Notebook (Colab) con todos los pasos y conclusiones.