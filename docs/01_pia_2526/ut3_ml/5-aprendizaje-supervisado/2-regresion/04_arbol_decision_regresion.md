---
title: "Decision Trees Regresi√≥n"
sidebar_position: 3
toc_max_heading_level: 5
description: "Introducci√≥n a Decision Tree Regression en Machine Learning. Funcionamiento del algoritmo, diferencias con √°rboles de clasificaci√≥n, hiperpar√°metros principales, visualizaci√≥n del √°rbol, importancia de variables y m√©tricas de evaluaci√≥n."
keywords: [Decision Tree, √Årboles de decisi√≥n, Regresi√≥n, Decision Tree Regression, Machine Learning, scikit-learn]
---

Los **√Årboles de Decisi√≥n para Regresi√≥n (Decision Tree Regression)** son algoritmos de Machine Learning utilizados para **predecir valores num√©ricos** mediante una serie de **reglas if‚Äìelse** aprendidas a partir de los datos.

A diferencia de la Regresi√≥n Lineal, los √°rboles:

* No asumen una relaci√≥n lineal
* Son **modelos no lineales**
* Permiten una **interpretaci√≥n visual clara**

Adem√°s, a diferencia de KNN, **s√≠ construyen un modelo expl√≠cito**, que puede analizarse y visualizarse.

---

## Idea principal del algoritmo

La idea de un √°rbol de decisi√≥n para regresi√≥n es sencilla:

> ‚ÄúDividir el espacio de datos en regiones donde los valores del target sean lo m√°s parecidos posible.‚Äù

El modelo aprende una serie de preguntas del tipo:

```text
¬øfeature ‚â§ valor?
```

Cada decisi√≥n divide los datos en dos grupos cada vez m√°s homog√©neos hasta llegar a una **hoja**, que devuelve un valor num√©rico.

:::info Los √°rboles de decisi√≥n son modelos no param√©tricos
Los √°rboles de decisi√≥n para regresi√≥n son **modelos no param√©tricos**:

* No tienen una forma fija predefinida
* La estructura del modelo depende directamente de los datos
* Si cambian los datos, cambia el √°rbol

Esto los hace muy flexibles, pero tambi√©n propensos al sobreajuste si no se controlan.
:::

---

## Funcionamiento del modelo

El √°rbol se construye de forma **recursiva**, siguiendo estos pasos:

1. Seleccionar la mejor variable y el mejor punto de corte
2. Dividir los datos en dos subconjuntos
3. Repetir el proceso en cada rama
4. Detener el crecimiento seg√∫n ciertos criterios
5. Asignar un valor num√©rico a cada hoja

En **regresi√≥n**, el valor que devuelve cada hoja suele ser la **media** de los valores del target que contiene.

![Gr√°fico EDA](../../0-img/dt_regressor.png)

:::info ¬øC√≥mo se elige la‚Äúmejor divisi√≥n‚Äù en regresi√≥n?

Aqu√≠ est√° la diferencia clave con la clasificaci√≥n.

* En **clasificaci√≥n**, el √°rbol minimiza la **impureza** (Gini, Entrop√≠a)
* En **regresi√≥n**, el √°rbol minimiza el **error** (normalmente **MSE (Mean Squared Error)**)

üëâ En los problemas de **regresi√≥n**, el √°rbol de decisi√≥n construye sus divisiones buscando **minimizar el error de predicci√≥n**. Para ello, en cada posible split eval√∫a cu√°nto se reduce el error si los datos se separan en dos grupos. Habitualmente se utiliza el **MSE (Mean Squared Error)** como medida de ese error, ya que penaliza m√°s los valores que se alejan mucho del valor medio. El √°rbol elige siempre la divisi√≥n que consigue que, dentro de cada grupo, los valores num√©ricos del target sean lo m√°s parecidos posible, logrando as√≠ predicciones m√°s precisas en las hojas.
:::

---

## Entrenamiento vs predicci√≥n

### Entrenamiento

Durante el entrenamiento, el √°rbol:

* Prueba diferentes divisiones
* Eval√∫a cu√°nto reduce el error cada split
* Construye la estructura completa del √°rbol

Este proceso puede ser **costoso computacionalmente** si el √°rbol crece mucho.

---

### Predicci√≥n

Para predecir un nuevo dato:

1. Se empieza en la ra√≠z del √°rbol
2. Se siguen las reglas if‚Äìelse
3. Se llega a una hoja
4. Se devuelve el valor num√©rico asociado a esa hoja

üëâ La predicci√≥n es **muy r√°pida**, ya que solo implica recorrer el √°rbol.

---

## Uso de √Årboles de Decisi√≥n en Regresi√≥n

### Cu√°ndo S√ç usarlos

Funcionan bien cuando:

* La relaci√≥n entre variables es no lineal
* Se busca **interpretabilidad**
* Hay variables categ√≥ricas
* El preprocesamiento debe ser simple

---

### Cu√°ndo NO funcionan bien

Suelen rendir peor cuando:

* Hay mucho ruido
* El dataset es peque√±o
* El √°rbol crece sin restricciones (overfitting)

En la pr√°ctica:

> Un √°rbol de decisi√≥n suele ser un buen modelo base, pero rara vez el modelo final m√°s robusto.

---

## Importancia del preprocesamiento

| Aspecto               | ¬øEs necesario? | Explicaci√≥n                    |
| --------------------- | -------------- | ------------------------------ |
| Tratamiento de nulos  | ‚úî S√≠           | No admite valores nulos        |
| Escalado              | ‚ùå No           | No usa distancias              |
| Outliers              | ‚ö†Ô∏è Importante  | Pueden generar splits extremos |

---

## Principales hiperpar√°metros

El rendimiento de un √°rbol de decisi√≥n para regresi√≥n depende en gran medida de **c√≥mo se controla su complejidad**.
Si el √°rbol crece sin restricciones, puede aprender patrones muy espec√≠ficos del conjunto de entrenamiento, perdiendo capacidad de generalizaci√≥n.

Por este motivo, los hiperpar√°metros del √°rbol se centran principalmente en **limitar su crecimiento**, buscando un equilibrio entre:

* **Modelo demasiado simple** ‚Üí no captura bien la relaci√≥n entre variables (underfitting)
* **Modelo demasiado complejo** ‚Üí se ajusta en exceso a los datos (overfitting)

---

### Profundidad m√°xima (`max_depth`)

El hiperpar√°metro `max_depth` controla **cu√°ntos niveles puede tener el √°rbol**, es decir, cu√°ntas decisiones consecutivas puede tomar antes de llegar a una hoja.

* Un **√°rbol muy profundo**:

  * Aprende reglas muy espec√≠ficas
  * Puede ajustarse casi perfectamente a los datos de entrenamiento
  * Tiene alto riesgo de **overfitting**

* Un **√°rbol poco profundo**:

  * Aprende reglas muy generales
  * Puede no capturar relaciones importantes entre variables
  * Produce **underfitting**

Controlar la profundidad es la forma **m√°s directa y efectiva** de regular un √°rbol de decisi√≥n.
En la pr√°ctica, limitar `max_depth` ayuda a crear modelos m√°s estables y con mejor capacidad de generalizaci√≥n.

---

### Muestras m√≠nimas (`min_samples_split`, `min_samples_leaf`)

Estos hiperpar√°metros controlan **cu√°ntos datos m√≠nimos son necesarios para crear nuevas divisiones** en el √°rbol.

* `min_samples_split`:

  * N√∫mero m√≠nimo de muestras que debe tener un nodo para poder dividirse
  * Evita que el √°rbol siga creciendo cuando quedan muy pocos datos

* `min_samples_leaf`:

  * N√∫mero m√≠nimo de muestras que debe contener una hoja
  * Garantiza que cada predicci√≥n est√© basada en suficientes datos

Establecer valores adecuados para estos par√°metros:

* Reduce la creaci√≥n de hojas con muy pocos datos
* Hace que las predicciones sean m√°s robustas
* Mejora la **generalizaci√≥n** del modelo en datos no vistos

En muchos casos, aumentar ligeramente estos valores reduce el overfitting sin perder demasiado rendimiento.

---

### Ajuste de hiperpar√°metros

Al igual que en otros algoritmos:

* Se pueden ajustar con **validaci√≥n cruzada**
* Es habitual usar **GridSearchCV**

Esto permite encontrar un equilibrio entre sesgo y varianza.

A cotinuaci√≥n se muestra una **tabla orientativa** de valores a probar. No son valores √≥ptimos, sino **rangos razonables** para empezar el GridSearch seg√∫n el tama√±o del dataset.

| Tama√±o del dataset | N¬∫ de registros | `max_depth` | `min_samples_split` | `min_samples_leaf` | Comentario                                                  |
| ------------------ | --------------- | ----------- | ------------------- | ------------------ | ----------------------------------------------------------- |
| Peque√±o            | < 1.000         | 2 ‚Äì 5       | 10 ‚Äì 50             | 5 ‚Äì 20             | Alto riesgo de overfitting, conviene limitar mucho el √°rbol |
| Mediano            | 1.000 ‚Äì 10.000  | 3 ‚Äì 10      | 5 ‚Äì 20              | 2 ‚Äì 10             | Buen equilibrio entre complejidad y generalizaci√≥n          |
| Grande             | > 10.000        | 5 ‚Äì 20      | 2 ‚Äì 10              | 1 ‚Äì 5              | M√°s datos permiten √°rboles m√°s profundos                    |


> Estos rangos sirven como punto de partida. El mejor conjunto de hiperpar√°metros siempre debe seleccionarse mediante validaci√≥n cruzada, ya que depende de las caracter√≠sticas concretas del dataset y no solo de su tama√±o.

---

## Visualizaci√≥n del √°rbol

Una de las grandes ventajas de los √°rboles de decisi√≥n es que **pueden visualizarse**.

Al representar el √°rbol, se puede observar:

* Las variables utilizadas en cada split
* Los valores de corte
* La profundidad del √°rbol
* El valor predicho en cada hoja

---

## Importancia de variables

Los √°rboles permiten calcular la **importancia de cada variable**.

Una variable ser√° m√°s importante si:

* Aparece frecuentemente en los splits
* Reduce mucho el error del modelo

Esto permite:

* Interpretar el modelo
* Identificar variables relevantes
* Apoyar procesos de selecci√≥n de variables

A diferencia de KNN, aqu√≠ **s√≠ es posible analizar qu√© variables influyen m√°s** en la predicci√≥n.

---

## M√©tricas de evaluaci√≥n

En problemas de regresi√≥n con √°rboles de decisi√≥n se utilizan las mismas m√©tricas que en otros modelos:

* **MAE** (Mean Absolute Error)
* **MSE** (Mean Squared Error)
* **R¬≤** (Coeficiente de determinaci√≥n)

---

## Flujo recomendado en un problema de √Årbol de Decisi√≥n (Regresi√≥n)

| Paso                | Qu√© se hace                        | Por qu√© es importante     |
| ------------------- | ---------------------------------- | ------------------------- |
| 1. EDA              | Analizar distribuciones y outliers | Evita splits extremos     |
| 2. Preprocesamiento | Limpieza de datos                  | El modelo no admite nulos |
| 3. Entrenamiento    | Ajustar hiperpar√°metros            | Controla overfitting      |
| 4. Evaluaci√≥n       | MAE, MSE, R¬≤ + gr√°ficos            | Medir rendimiento         |
| 5. Interpretaci√≥n   | √Årbol + importancia variables      | Entender el modelo        |
| 6. Comparaci√≥n      | Comparar con otros modelos  | Determina qu√© modelo se adapta mejor a nuestro dataset |

---

## Ejemplo: √Årbol de Decisi√≥n para Regresi√≥n

Para ver c√≥mo funciona un **Decision Tree Regressor** en la pr√°ctica, puedes ejecutar este ejemplo utilizando el dataset **California Housing**.

üëâ **Puedes abrir el cuaderno aqu√≠:**
[Colab: Decision Tree Regression](../../0-datasets/ejemplo_decision_tree_regresion.ipynb)

---

## Actividad de seguimiento: Bike Sharing Dataset

Utiliza el **Bike Sharing Dataset** y compara:

* Regresi√≥n Lineal
* KNN Regresi√≥n
* √Årbol de Decisi√≥n (Regresi√≥n)

Incluye:

* Ajuste de hiperpar√°metros
* M√©tricas de evaluaci√≥n
* Visualizaci√≥n del √°rbol
* An√°lisis de importancia de variables
* Conclusiones razonadas

**Entrega:** Notebook (Colab) con conclusiones claras y justificadas.
