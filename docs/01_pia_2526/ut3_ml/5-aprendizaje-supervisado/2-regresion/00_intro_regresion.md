---
title: "Ideas generales"
sidebar_position: 0
toc_max_heading_level: 5
description: "Introducci√≥n a los problemas de regresi√≥n en Machine Learning. Qu√© es la regresi√≥n, en qu√© se diferencia de la clasificaci√≥n, principales m√©tricas de evaluaci√≥n y visi√≥n general de los modelos de regresi√≥n."
keywords: [Regresi√≥n, Machine Learning, modelos de regresi√≥n, m√©tricas, MSE, MAE, R2, supervisado]
---

Un **problema de regresi√≥n** es un tipo de problema de **aprendizaje supervisado** en el que el objetivo es **predecir un valor num√©rico continuo**.

En estos problemas:
- El dataset contiene **features** (variables de entrada).
- Existe una variable objetivo (**target**) **num√©rica**.
- El modelo aprende una relaci√≥n entre las features y el target.

## Ejemplos de problemas de regresi√≥n
- Predecir el **precio de una vivienda**
- Estimar la **temperatura** de ma√±ana
- Calcular el **consumo el√©ctrico**
- Predecir la **nota final** de un estudiante

---

## Diferencias entre regresi√≥n y clasificaci√≥n

Aunque ambos son problemas de aprendizaje supervisado, existen diferencias clave:

| Caracter√≠stica | Regresi√≥n | Clasificaci√≥n |
|----------------|----------|---------------|
| Tipo de salida | Num√©rica continua | Categor√≠a o clase |
| Ejemplo de target | 1250.75 | "spam", "no spam" |
| M√©tricas comunes | MSE, MAE, R¬≤ | Accuracy, Precision, Recall |
| Visualizaci√≥n | Rectas, curvas | Regiones de decisi√≥n |

üëâ La **principal diferencia** est√° en el **tipo de variable objetivo**.

---

## Modelos de regresi√≥n

En regresi√≥n, el objetivo es predecir un **valor num√©rico continuo**.
Algunos modelos lo hacen ajustando una **funci√≥n global** (como una recta o un plano),
mientras que otros realizan predicciones locales basadas en los datos m√°s cercanos,
como ocurre en k-NN.

![Simple vs Multiple](../../0-img/simple_vs_multiple.png)

| Modelo | Tipo | Lineal | Param√©trico | Interpretabilidad |
|------|------|--------|-------------|------------------|
| Regresi√≥n Lineal | Base | S√≠ | S√≠ | Alta |
| Regresi√≥n Polin√≥mica | Extensi√≥n | No | S√≠ | Media |
| k-NN Regresi√≥n | Basado en instancias | No | No | Baja |
| √Årbol de Decisi√≥n | Basado en reglas | No | No | Media |
| Random Forest | Ensemble | No | No | Baja |
| Ridge | Lineal regularizado | S√≠ | S√≠ | Media |
| Lasso | Lineal regularizado | S√≠ | S√≠ | Media |

:::info ¬øQu√© significan estas caracter√≠sticas?
En la tabla se utilizan los siguientes conceptos:

- **Lineal**: indica si el modelo asume una relaci√≥n lineal entre las variables de entrada y la variable objetivo.  
  Un modelo lineal puede representarse mediante una combinaci√≥n lineal de las features.

- **Param√©trico**: indica si el modelo aprende un n√∫mero fijo de par√°metros (como coeficientes).  
  Los modelos param√©tricos asumen una forma concreta del modelo y no dependen directamente del n√∫mero de datos.

- **Interpretabilidad**: hace referencia a lo f√°cil que es entender c√≥mo el modelo realiza sus predicciones.  
  Modelos simples y lineales suelen ser m√°s interpretables, mientras que modelos m√°s complejos suelen ofrecer mayor rendimiento pero menor explicaci√≥n.
:::


Una vez ajustado el modelo, necesitamos medir qu√© tan bien se adapta a los datos.
Para ello se utilizan distintas m√©tricas de evaluaci√≥n.


---

## M√©tricas de evaluaci√≥n en regresi√≥n

Para evaluar modelos de regresi√≥n se utilizan m√©tricas que miden **el error entre el valor real y el valor predicho**.

### Mean Squared Error (MSE)

El **Mean Squared Error (MSE)** es una de las m√©tricas m√°s utilizadas en problemas de regresi√≥n. Mide el **promedio del cuadrado de la diferencia entre los valores reales y los valores predichos por el modelo**.

En otras palabras, cuantifica **cu√°nto se equivoca el modelo al predecir**, dando **m√°s peso a los errores grandes**.

Si tenemos un conjunto de $ n $ observaciones:

* $ y_i $: valor real
* $ \hat{y}_i $: valor predicho por el modelo

El MSE se define como:

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

:::tip
Cuanto **menor** sea el MSE, **mejor** es el modelo,  
porque indica que, en promedio, las predicciones del modelo est√°n m√°s
cerca de los valores reales y que los errores grandes son menos frecuentes.
:::

Ejemplo de c√°lculo del MSE con un dataset de prueba:

![C√°lculo MSE](../../0-img/calculo-mse.png)


Si el modelo comete errores grandes en pocas observaciones, el MSE **aumenta mucho**.

---

#### Relaci√≥n con el entrenamiento del modelo

En muchos modelos de regresi√≥n, el entrenamiento del modelo consiste en
encontrar los **par√°metros internos** (por ejemplo, los **coeficientes**) que
minimizan una **funci√≥n de coste**, siendo el MSE una de las m√°s utilizadas.

En este contexto, el MSE:
- Se utiliza como funci√≥n de coste durante el entrenamiento
- Gu√≠a el ajuste de los par√°metros del modelo

Adem√°s, el MSE tambi√©n puede utilizarse como m√©trica de evaluaci√≥n en
procesos de selecci√≥n de hiperpar√°metros, como GridSearch, donde se emplea
para comparar distintos modelos entrenados.

:::info ¬øQu√© es la funci√≥n de coste?
La **funci√≥n de coste** es una funci√≥n matem√°tica que mide el **error del modelo**, comparando los valores reales con los valores predichos.

Durante el entrenamiento, el objetivo del algoritmo es **minimizar la funci√≥n de coste**, ajustando los par√°metros del modelo.  
En problemas de regresi√≥n, una de las funciones de coste m√°s utilizadas es el **Mean Squared Error (MSE)**.
:::

---

### Mean Absolute Error (MAE)

El **Mean Absolute Error (MAE)** mide el **promedio del valor absoluto de la diferencia entre los valores reales y los valores predichos por el modelo**.

A diferencia del MSE, el MAE **no eleva el error al cuadrado**, por lo que **todos los errores contribuyen de forma proporcional** al resultado final.

Si tenemos un conjunto de $ n $ observaciones:

* $ y_i $: valor real
* $ \hat{y}_i $: valor predicho por el modelo

El MAE se define como:

$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} \lvert y_i - \hat{y}_i \rvert
$$

:::tip
Cuanto **menor** sea el MAE, **mejor** es el modelo, porque indica que, de media, el error de las predicciones es menor y el modelo se equivoca menos en las mismas unidades que la variable objetivo.
:::

El MAE indica, de media, **cu√°ntas unidades se equivoca el modelo en sus predicciones**.

Por ejemplo:

* Un MAE de **5** en un problema de predicci√≥n de precios significa que,
  de media, el modelo se equivoca **5 unidades monetarias**.

Esto hace que el MAE sea una m√©trica **muy f√°cil de interpretar**, ya que se
expresa en las **mismas unidades que la variable objetivo**.

---

### Coeficiente de determinaci√≥n (R¬≤)

El **coeficiente de determinaci√≥n (R¬≤)** es una m√©trica utilizada en regresi√≥n que indica **qu√© proporci√≥n de la variabilidad de la variable objetivo es explicada por el modelo**.

A diferencia del MSE o el MAE, el R¬≤ **no mide el error directamente**, sino **la calidad del ajuste global del modelo**.

El R¬≤ compara el modelo entrenado con un modelo muy simple que **siempre predice la media del target**.

* Si el modelo explica bien los datos, el R¬≤ ser√° alto.
* Si el modelo no mejora esa predicci√≥n b√°sica, el R¬≤ ser√° bajo o incluso negativo.

Los valores posibles del R¬≤ son:

* **R¬≤ = 1**
  El modelo explica el 100 % de la variabilidad del target (ajuste perfecto).

* **R¬≤ = 0**
  El modelo no mejora respecto a predecir siempre la media del target.

* **R¬≤ < 0**
  El modelo es peor que predecir la media, lo que indica un ajuste muy deficiente.

Por ejemplo:

* Un **R¬≤ = 0.80** significa que el modelo explica el **80 % de la variabilidad**
  de la variable objetivo.
* El **20 % restante** se debe a factores no capturados por el modelo.


:::info ¬øQu√© es el concepto de variabilidad?

La **variabilidad** de una variable es **cu√°nto cambian sus valores** dentro del dataset.


Imagina dos datasets con la misma media:

* Dataset A

    ```text
    10, 10, 10, 10, 10
    ```

    * Media = 10
    * Variabilidad = **muy baja** (todos los valores son iguales)

* Dataset B

    ```text
    2, 8, 10, 15, 25
    ```

    * Media ‚âà 12
    * Variabilidad = **alta** (los valores est√°n muy dispersos)

üëâ Ambos tienen media similar, pero **no se comportan igual**.

En regresi√≥n, la variabilidad del target indica:

* Cu√°nto **var√≠an los valores reales**
* Qu√© tan dif√≠cil es el problema
* Cu√°nto margen tiene el modelo para ‚Äúexplicar‚Äù los datos

Si el target casi no cambia, **no hay mucho que explicar**.
Si cambia mucho, el modelo debe capturar **patrones m√°s complejos**.

El R¬≤ responde a esta pregunta:

> ¬øCu√°nta de esa variabilidad total consigue explicar el modelo?

* **R¬≤ = 0**
  El modelo no explica nada m√°s que la media.

* **R¬≤ = 0.7**
  El modelo explica el **70 % de la variabilidad** del target.

* **R¬≤ = 1**
  El modelo explica toda la variabilidad.


Piensa en las notas de una clase:

* Si todos sacan un 7 ‚Üí poca variabilidad
* Si hay notas entre 2 y 10 ‚Üí mucha variabilidad

Un buen modelo ser√≠a aquel que **explica por qu√© unos sacan m√°s y otros menos**.
:::

---

### Comparaci√≥n entre MSE, MAE y R¬≤

| M√©trica | Qu√© mide | Penalizaci√≥n de errores grandes | Sensible a outliers | Interpretabilidad | Uso habitual |
|--------|---------|----------------------------------|---------------------|------------------|--------------|
| **MSE** | Error medio al cuadrado | Alta | Alta | Media | Funci√≥n de coste y entrenamiento |
| **MAE** | Error medio absoluto | Baja | Media | Alta | Evaluaci√≥n e interpretaci√≥n |
| **R¬≤** | Variabilidad explicada | No aplica | Baja | Alta | Comparaci√≥n de modelos |


---

### Interpretaci√≥n pr√°ctica de MAE y MSE

A diferencia de m√©tricas como la accuracy en clasificaci√≥n, en regresi√≥n **no existen valores absolutos de MAE o MSE que se consideren ‚Äúbuenos‚Äù o ‚Äúmalos‚Äù de forma universal**.  
Su interpretaci√≥n depende de la **escala y el rango de la variable objetivo**.

---

#### MAE en relaci√≥n con el rango del target

Una forma habitual y pr√°ctica de interpretar el **MAE** es compararlo con el **rango de valores de la variable objetivo**.

| MAE respecto al rango del target | Interpretaci√≥n |
|--------------------------------|----------------|
| < 5 % | Muy bueno |
| 5‚Äì10 % | Bueno |
| 10‚Äì20 % | Aceptable |
| > 20 % | Malo |

Por ejemplo, en el dataset **California Housing**:

- Rango aproximado del target: **0 ‚Äì 5**
- MAE ‚âà **0.53**

Esto supone un error medio de alrededor del **10 % del rango**, lo que se considera un resultado **aceptable o razonable** para un modelo baseline como la Regresi√≥n Lineal.

Este tipo de comparaci√≥n permite contextualizar el error y evitar interpretaciones incorrectas basadas √∫nicamente en el valor num√©rico del MAE.

---

#### C√≥mo interpretar el MSE correctamente

El **Mean Squared Error (MSE)** penaliza de forma m√°s severa los errores grandes, ya que eleva al cuadrado las diferencias entre valores reales y predichos.  
Por este motivo, **no es una m√©trica intuitiva en t√©rminos de unidades**, pero resulta muy √∫til para analizar el comportamiento del modelo.

En la pr√°ctica, el MSE se utiliza sobre todo para:

- comparar distintos modelos entre s√≠
- detectar la presencia de errores grandes u outliers

Una interpretaci√≥n pr√°ctica consiste en compararlo con el MAE:

- **MSE ‚â´ MAE** ‚Üí existen errores grandes frecuentes
- **MSE ‚âà MAE** ‚Üí los errores son moderados y no hay muchos outliers extremos

En el caso de California Housing:

- MAE ‚âà **0.53**
- MSE ‚âà **0.56**

Esto indica que no hay una explosi√≥n de errores muy grandes, aunque s√≠ existen algunos casos problem√°ticos que afectan al rendimiento global del modelo.

---

<!--

## Detecci√≥n de overfitting y underfitting

Una forma pr√°ctica y habitual de detectar **overfitting** o **underfitting** en modelos de regresi√≥n consiste en **comparar las m√©tricas de evaluaci√≥n en el conjunto de entrenamiento y en el conjunto de test**.

Para ello, se calculan las predicciones y las m√©tricas en ambos conjuntos.

---

### C√°lculo de m√©tricas en train y test

```python
from sklearn.metrics import mean_absolute_error, r2_score

# Predicciones
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# M√©tricas
mae_train = mean_absolute_error(y_train, y_train_pred)
mae_test = mean_absolute_error(y_test, y_test_pred)

r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

print(f"MAE TRAIN: {mae_train:.3f}")
print(f"MAE TEST : {mae_test:.3f}")
print(f"R¬≤ TRAIN : {r2_train:.3f}")
print(f"R¬≤ TEST  : {r2_test:.3f}")
```

Este c√≥digo permite evaluar **c√≥mo cambia el rendimiento del modelo** al pasar de datos vistos (train) a datos no vistos (test).

---

### Interpretaci√≥n de m√©tricas (train vs test)

La comparaci√≥n de las m√©tricas obtenidas en **entrenamiento** y **test** permite diagnosticar si un modelo presenta **overfitting**, **underfitting** o un **buen ajuste**.

| Situaci√≥n        | Qu√© se observa en las m√©tricas                                                               | Ejemplo t√≠pico                                                           | Interpretaci√≥n                                                                        |
| ---------------- | -------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ | ------------------------------------------------------------------------------------- |
| **Overfitting**  | MAE muy bajo en train y claramente mayor en test. R¬≤ alto en train y mucho m√°s bajo en test. | MAE TRAIN = 0.20 - MAE TEST = 0.55 - R¬≤ TRAIN = 0.92 - R¬≤ TEST = 0.58 | El modelo se ajusta demasiado a los datos de entrenamiento y no generaliza bien.      |
| **Underfitting** | MAE similar en train y test, pero ambos altos. R¬≤ bajo en ambos conjuntos.                   | MAE TRAIN = 0.60 - MAE TEST = 0.62 - R¬≤ TRAIN = 0.45 - R¬≤ TEST = 0.43 | El modelo es demasiado simple y no captura la relaci√≥n real entre variables y target. |
| **Buen ajuste**  | MAE similar y bajo en train y test. R¬≤ similar y razonablemente alto en ambos.               | MAE TRAIN = 0.48 - MAE TEST = 0.50 - R¬≤ TRAIN = 0.62 - R¬≤ TEST = 0.60 | El modelo aprende patrones √∫tiles y generaliza correctamente.                         |

---

### Conclusi√≥n pr√°ctica

Comparar m√©tricas entre entrenamiento y test es una forma sencilla y efectiva de diagnosticar el comportamiento de un modelo de regresi√≥n:

* **Gran diferencia train‚Äìtest** ‚Üí overfitting
* **Mal rendimiento en ambos conjuntos** ‚Üí underfitting
* **Rendimiento similar y razonable** ‚Üí buen ajuste

Este an√°lisis es fundamental antes de probar modelos m√°s complejos o introducir regularizaci√≥n.

---

-->

## An√°lisis gr√°fico del rendimiento en regresi√≥n

Adem√°s de las m√©tricas num√©ricas (MAE, MSE y R¬≤), es habitual analizar el comportamiento de un modelo de regresi√≥n mediante **gr√°ficos**, que permiten entender **d√≥nde y c√≥mo se equivoca el modelo**.

Estas visualizaciones no sustituyen a las m√©tricas, sino que las **complementan**, ofreciendo una interpretaci√≥n m√°s intuitiva del rendimiento del modelo.

---

### Valores reales vs valores predichos

En este gr√°fico, cada punto representa una observaci√≥n del conjunto de datos:

- El eje horizontal muestra el **valor real** de la variable objetivo.
- El eje vertical muestra el **valor predicho** por el modelo.

La l√≠nea diagonal representa la **predicci√≥n perfecta**, es decir, los casos en los que el valor predicho coincide exactamente con el valor real.

![Gr√°fico EDA](../../0-img/reales_vs_predichos.png)

**Interpretaci√≥n:**

- Cuanto m√°s cerca est√©n los puntos de la l√≠nea diagonal, **menor es el error de predicci√≥n**.
- Si los puntos siguen una tendencia ascendente, el modelo captura la relaci√≥n general entre las variables y la target.
- Una gran dispersi√≥n alrededor de la l√≠nea indica que el modelo **no explica completamente la variabilidad** de los datos.

Este gr√°fico est√° directamente relacionado con el **coeficiente de determinaci√≥n (R¬≤)**:
- Un R¬≤ alto implica puntos m√°s pr√≥ximos a la diagonal.
- Un R¬≤ bajo implica mayor dispersi√≥n y un ajuste m√°s pobre.

El objetivo de un buen modelo de regresi√≥n es conseguir que los puntos est√©n **lo m√°s pr√≥ximos posible a la l√≠nea diagonal**.

---

### Gr√°fico de residuos vs predicci√≥n

El residuo se define como la diferencia entre el valor real y el valor predicho:

$$
\text{residuo} = y - \hat{y}
$$

En este gr√°fico:

- El eje horizontal representa el **valor predicho**.
- El eje vertical representa el **residuo**.
- La l√≠nea horizontal en cero indica **error nulo**.

![Gr√°fico EDA](../../0-img/residuos.png)

**Interpretaci√≥n:**

- En un modelo adecuado, los residuos deber√≠an distribuirse de forma **aleatoria** alrededor de cero, sin patrones claros.
- La presencia de patrones (pendientes, formas de abanico, agrupaciones) indica que el modelo **no captura correctamente la relaci√≥n real** entre las variables.
- Residuos grandes implican errores elevados, que influyen especialmente en m√©tricas como el **MSE**.

Este gr√°fico permite detectar de forma visual:
- relaciones no lineales
- errores sistem√°ticos
- limitaciones del modelo utilizado

---

### Relaci√≥n entre m√©tricas y gr√°ficos

Las m√©tricas num√©ricas resumen el rendimiento global del modelo, mientras que los gr√°ficos permiten analizar **la distribuci√≥n y el comportamiento de los errores**.

- **MAE y MSE** cuantifican cu√°nto se equivoca el modelo.
- **R¬≤** indica cu√°nta variabilidad explica el modelo.
- Los gr√°ficos muestran **d√≥nde** se producen esos errores y **c√≥mo** se distribuyen.

Por este motivo, el an√°lisis gr√°fico es una herramienta fundamental para evaluar modelos de regresi√≥n, independientemente del algoritmo utilizado.
