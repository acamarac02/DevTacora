---
title: "Preprocesamiento de Datos"
sidebar_position: 4
toc_max_heading_level: 4
description: "Introducci√≥n al Preprocesamiento de Datos en Machine Learning. Qu√© es, por qu√© se realiza despu√©s del EDA, principales pasos para limpiar, transformar y preparar los datos antes del modelado utilizando pandas y scikit-learn."
keywords: [Preprocesamiento, Machine Learning, pandas, scikit-learn, limpieza de datos, escalado, codificaci√≥n, preparaci√≥n de datos]
---

<div class="justify-text">

El **Preprocesamiento de Datos** es la **segunda etapa del proceso de Machine Learning**, y comienza justo despu√©s de haber realizado el **An√°lisis Exploratorio de Datos (EDA)**.  
Si en el EDA analizamos y comprendemos los datos, en el preprocesamiento el objetivo es **prepararlos adecuadamente** para que los algoritmos de aprendizaje autom√°tico puedan trabajar con ellos de forma correcta y eficiente.

Durante esta fase, se realizan todas las tareas necesarias para **limpiar, transformar y estandarizar** los datos.  
Esto incluye, por ejemplo, rellenar valores nulos, eliminar duplicados, codificar variables categ√≥ricas o escalar las num√©ricas.

<div class="hidden-summary">

## Introducci√≥n

</div>

### ¬øPor qu√© es esencial tras el EDA?

El EDA nos permite descubrir **problemas o irregularidades** en el dataset: valores perdidos, outliers, tipos de datos incorrectos o variables irrelevantes.  
El preprocesamiento es la fase donde **se corrigen esos problemas** y se dejan los datos listos para el modelado.


| Etapa | Objetivo principal | Tipo de tareas |
|--------|--------------------|----------------|
| **EDA** | Comprender los datos y detectar patrones o errores. | An√°lisis descriptivo, visualizaci√≥n, detecci√≥n de nulos y outliers. |
| **Preprocesamiento** | Corregir, transformar y preparar los datos para el modelo. | Limpieza, imputaci√≥n, codificaci√≥n, escalado, selecci√≥n de variables. |

üí¨ **Ejemplo:**
En el Titanic, durante el EDA descubrimos que hay valores nulos en `Age` y `Cabin`, y que `Fare` tiene outliers.  
En el preprocesamiento, decidiremos **c√≥mo rellenar esos nulos**, **si eliminar o ajustar los outliers**, y **c√≥mo convertir las variables categ√≥ricas** (como `Sex` o `Embarked`) a formato num√©rico.

---

### Objetivos del preprocesamiento

El objetivo final es dejar el dataset:

* **Limpio:** sin errores, duplicados ni valores faltantes.  
* **Coherente:** con tipos de datos correctos y valores representativos.  
* **Preparado:** con todas las variables transformadas en un formato num√©rico adecuado para los modelos de Machine Learning.

En otras palabras, buscamos que los datos sean **de calidad y comparables entre s√≠**, para que el modelo aprenda de forma fiable.

---

### Herramientas que usaremos

En Python, las principales herramientas para el preprocesamiento son:

* **`pandas`** ‚Üí para limpiar, transformar y manipular los datos.  
* **`scikit-learn (sklearn.preprocessing)`** ‚Üí para aplicar transformaciones autom√°ticas como imputaci√≥n, codificaci√≥n y escalado.  

Estas librer√≠as se complementan y permiten llevar a cabo todo el flujo de preparaci√≥n de datos antes de entrenar el modelo.

---

## Paso 1. Operaciones antes de la divisi√≥n

Antes de separar el dataset en conjuntos de entrenamiento (*train*) y prueba (*test*), debemos realizar una **limpieza estructural b√°sica**.  
Este paso no modifica el contenido estad√≠stico de los datos, sino que se asegura de que el dataset sea **coherente, entendible y correctamente tipado**.

Estas operaciones se hacen **antes de dividir** porque afectan por igual a todos los registros y no implican ning√∫n tipo de aprendizaje sobre los valores de los datos (no calculan medias, ni modas, ni escalados).

---

### Paso 1.1 Limpieza estructural

El objetivo de la limpieza estructural es **asegurar que el dataset est√© bien formado**:
* Quitar columnas que no aportan informaci√≥n al modelo.
* Comprobar que los tipos de datos son correctos.
* Corregir formatos o errores de escritura en las variables.

A continuaci√≥n veremos cada una de estas operaciones aplicadas al dataset **Titanic**.

#### Eliminaci√≥n de columnas irrelevantes

Algunos campos no aportan informaci√≥n √∫til para el modelo o pueden incluso introducir ruido.  
Ejemplos t√≠picos son identificadores √∫nicos (`PassengerId`) o campos con texto libre (`Name`, `Ticket`, `Cabin`).

Estas columnas **no ayudan a predecir la supervivencia**, por lo que es recomendable eliminarlas antes del modelado.

```python
# Seleccionamos las columnas relevantes (features) y la variable objetivo (target)
features = ['Pclass', 'Sex', 'Age', 'Fare', 'SibSp', 'Parch', 'Embarked']
target = 'Survived'

X = df[features].copy()  # Hacemos una copia para modificar m√°s adelante (rellenar nulos, codificar, etc.)
y = df[target]
```

üí° **Explicaci√≥n te√≥rica:**

* Los **identificadores** (como `PassengerId`) son √∫nicos por fila, por lo que no aportan patrones comunes al modelo.
* Las columnas **de texto libre** (como `Name` o `Ticket`) contienen informaci√≥n no estructurada dif√≠cil de procesar sin t√©cnicas de *Feature Engineering* avanzadas (como procesamiento de texto), que no vamos a abordar.
* **Cabin** tiene la mayor√≠a de sus valores nulos, por lo que se elimina tambi√©n.

üí¨ **Conclusi√≥n:**

> Al eliminar variables irrelevantes, reducimos el ruido y simplificamos el modelo sin perder informaci√≥n √∫til.

---

#### Correcci√≥n de tipos de datos

Los tipos de datos son fundamentales.
Pandas puede leer columnas con el tipo incorrecto (por ejemplo, n√∫meros como texto o fechas como `object`), lo que puede generar errores en el an√°lisis y el modelado.

Podemos revisar los tipos con:

```python
df.info()
```

Si detectamos alguna columna con tipo err√≥neo, podemos convertirla usando `astype()`.

Ejemplo: supongamos que una columna num√©rica ha sido le√≠da como texto (`object`):

```python
# Convertir una columna a tipo num√©rico
df["Fare"] = df["Fare"].astype(float)
```

üí° **Explicaci√≥n te√≥rica:**

* Los modelos de Machine Learning requieren que **las variables num√©ricas sean realmente num√©ricas**, no cadenas de texto.
* Asegurar los tipos correctos evita errores posteriores en imputaci√≥n, escalado o codificaci√≥n.

üí¨ **Conclusi√≥n:**

> Revisar y corregir los tipos de datos garantiza que todas las columnas se comporten como se espera en las transformaciones posteriores.

---

#### Normalizaci√≥n de formatos y categor√≠as

En las variables categ√≥ricas o de texto, a veces encontramos **errores de formato**:

* Espacios en blanco (`" S "` en lugar de `"S"`).
* May√∫sculas/min√∫sculas mezcladas (`"Male"` vs `"male"`).
* Valores mal escritos (`"Southampton"` vs `"southampton"`).

Estos errores de formato los veriamos al ejecutar `unique()` sobre una variable categ√≥rica o al generar gr√°ficos sobre ellas. A continuaci√≥n se muestra un ejemplo donde metemos una dato err√≥neo en Titanic para estudiar c√≥mo solucionarlo:

```python
# Crear una nueva fila con "Male" como sexo para que podamos ver el error
nueva_fila = {
    "PassengerId": 9999,
    "Survived": 0,
    "Pclass": 3,
    "Name": "Smith, Mr. John",
    "Sex": "Male",
    "Age": 32,
    "SibSp": 0,
    "Parch": 0,
    "Ticket": "A/5 9999",
    "Fare": 7.25,
    "Cabin": "",
    "Embarked": "S"
}

# A√±adir la fila al DataFrame
df = pd.concat([df, pd.DataFrame([nueva_fila])], ignore_index=True)

# Mostrar los valores √∫nicos de la columna 'Sex'
print(df["Sex"].unique())
```

La salida obtenida en este caso ser√≠a:

```python
array(['male', 'female', 'Male'], dtype=object)
```

Estos detalles pueden hacer que el modelo interprete **dos valores iguales como distintos**, lo que distorsiona la codificaci√≥n posterior.

Ese problema anterior se solucionar√≠a pasando a min√∫sculas todos los valores:

```python
# Normalizar los valores de la columna 'Sex'
# Pasamos a min√∫sculas y, de paso, eliminamos posibles espacios
df["Sex"] = df["Sex"].str.lower().str.strip()

# Verificar que se ha corregido
print(df["Sex"].unique())
```

Salida esperada:

```python
array(['male', 'female'], dtype=object)
```

Si aparecieran errores tipogr√°ficos, se pueden reemplazar manualmente:

```python
# Ejemplo: sustituir valores incorrectos (si los hubiera)
df["Embarked"].replace({"SOUTHAMPTON": "S", "CHERBOURG": "C", "QUEENSTOWN": "Q"}, inplace=True)
```

üí° **Explicaci√≥n te√≥rica:**

* La **homogeneizaci√≥n de categor√≠as** es clave para que el modelo reconozca correctamente las clases.
* Si dos valores equivalentes se escriben distinto, el modelo los tratar√° como categor√≠as diferentes.

üí¨ **Conclusi√≥n:**

> Antes de codificar las variables categ√≥ricas, debemos garantizar que todas las categor√≠as est√©n escritas de forma uniforme.

---

### Paso 1.2 Duplicados

En muchos datasets reales, especialmente cuando los datos provienen de distintas fuentes o se han unido varios ficheros, es frecuente encontrar **filas duplicadas**.  
Estas duplicidades pueden provocar que el modelo **aprenda varias veces la misma informaci√≥n**, generando sesgos o influyendo en las estad√≠sticas de forma incorrecta.

Por eso, un paso b√°sico de la limpieza estructural consiste en **detectar y eliminar los registros duplicados** antes de seguir con el preprocesamiento.

En el An√°lisis Exploratorio de Datos ya estudiamos c√≥mo consultar los duplicados:

```python
# Comprobar si hay filas duplicadas en el dataset
df.duplicated().sum()
````

Salida esperada (en el Titanic original):

```python
0
```

üí° **Explicaci√≥n te√≥rica:**

* Si el resultado es `0`, significa que **no hay filas duplicadas exactas**.
* Si devuelve un n√∫mero mayor que `0`, indica cu√°ntos registros est√°n repetidos completamente.

Podemos ver cu√°les son esas filas duplicadas con:

```python
# Mostrar las filas duplicadas (si las hubiera)
df[df.duplicated()]
```

Si detectamos registros repetidos, se eliminan f√°cilmente con `drop_duplicates()`:

```python
# Eliminar filas duplicadas
df = df.drop_duplicates()
```

üí° **Explicaci√≥n te√≥rica:**

* Por defecto, `drop_duplicates()` elimina las filas repetidas **manteniendo la primera aparici√≥n**.
* Este m√©todo elimina duplicados considerando **todas las columnas**.


### Paso 1.3 Outliers y errores evidentes

Los **outliers** (valores at√≠picos) son datos que se alejan mucho del resto de observaciones. A veces son simplemente **casos reales extremos** (por ejemplo, una tarifa muy alta en primera clase), pero otras veces se deben a **errores de registro o introducci√≥n de datos** (por ejemplo, una edad de 250 a√±os).

En esta fase inicial del preprocesamiento, **no buscamos eliminar todos los valores extremos**, sino **detectar y corregir solo los claramente imposibles o err√≥neos**.

Una forma sencilla de detectar outliers es mediante un **boxplot (diagrama de caja)**. Por ejemplo, si introducimos un registro nuevo con una edad de 250 a√±os, podr√≠amos ver lo siguiente: 

![Gr√°fico EDA](./0-img/boxplot-valor-erroneo.png)

No todos los outliers deben eliminarse. Algunos son **casos reales v√°lidos**, y eliminarlos podr√≠a distorsionar el modelo.
Por eso, antes de borrar nada, conviene preguntarse:

> ¬øEste valor es posible en el contexto del dataset?

Veamos ejemplos:

```python
# Revisar valores m√°ximos en columnas num√©ricas
df[["Age", "Fare"]].max()
```

Posible salida:

```
Age     80.0
Fare    512.3292
dtype: float64
```

üí° **Interpretaci√≥n:**

* Una edad de 80 a√±os es realista (no hay problema).
* Una tarifa de m√°s de 500 tambi√©n puede ser v√°lida para pasajeros de primera clase.
* Pero si encontr√°ramos una edad de **250 a√±os** o un valor negativo en la tarifa (`Fare = -10`), ser√≠an claramente errores de registro.

Podemos usar condiciones simples para **detectar valores imposibles** y corregirlos o eliminarlos.

```python
# Ejemplo: eliminar edades imposibles
df = df[df["Age"] <= 100]

# Ejemplo: corregir tarifas negativas (si existieran)
# Esto aprende de datos, se deber√≠a hacer despu√©s de dividir en train y test
df.loc[df["Fare"] < 0, "Fare"] = df["Fare"].median()
```

üí° **Explicaci√≥n te√≥rica:**

* Usamos **filtros l√≥gicos** (`df["Age"] <= 100`) para quedarnos solo con valores v√°lidos.
* Si un valor err√≥neo es aislado y no queremos eliminar la fila completa, podemos **reemplazarlo** por un valor representativo (por ejemplo, la mediana).

:::warning CUIDADO CON EL TRATAMIENTO DE LOS OUTLIERS
Si lo que queremos es **corregir datos**, como el caso de la tarifa que se explicaba antes, habr√≠a que hacerlo **despu√©s de dividir en train y test**.
:::

---

## Paso 2. Divisi√≥n en *train/test*

Una vez que el dataset est√° limpio a nivel **estructural** (sin duplicados, sin columnas irrelevantes, sin tipos incorrectos y sin errores evidentes), estamos listos para el paso m√°s importante del flujo de Machine Learning: la **divisi√≥n en conjuntos de entrenamiento y prueba**.

Este paso es fundamental para construir modelos fiables y evaluar su rendimiento de forma justa.

:::info ¬øPor qu√© dividir antes de ‚Äúaprender‚Äù par√°metros?

Cada vez que aplicamos una t√©cnica de preprocesamiento que **aprende algo de los datos**, como calcular la **mediana** para imputar nulos, determinar las **categor√≠as** para One-Hot Encoding, calcular la **media y desviaci√≥n** para escalar, estamos extrayendo **informaci√≥n estad√≠stica** del dataset.

Si hacemos esta extracci√≥n **antes** de dividir, estar√≠amos utilizando **informaci√≥n del futuro** (del conjunto de test) para preparar nuestros datos. Esto se llama **data leakage** (*filtraci√≥n de datos*), y provoca:

* modelos que parecen m√°s precisos de lo que realmente son,  
* una evaluaci√≥n injusta,  
* generalizaci√≥n mucho peor en datos nuevos.

üí° **Regla de oro del preprocesamiento:**

> Todo lo que **aprende par√°metros** debe ajustarse **solo con los datos de entrenamiento**, y luego aplicarse a los datos de prueba sin volver a aprender nada.
:::

---

### Paso 2.1. Separar las features y target

Antes de dividir, debemos indicar:
* qu√© columnas vamos a usar para predecir ‚Üí **X** (features) 
* qu√© columna queremos predecir ‚Üí **y** (target)

Ejemplo con Titanic:

```python
# Seleccionamos las columnas relevantes (features) y la variable objetivo (target)
# En EDA ya dejamos claro que Name, Cabin, etc. no eran √∫tiles
features = ['Pclass', 'Sex', 'Age', 'Fare', 'SibSp', 'Parch', 'Embarked']
target = 'Survived'

X = df[features].copy()
y = df[target]
```

üí° **Explicaci√≥n te√≥rica:**

* `X` contiene todas las variables que el modelo utilizar√° como entrada.
* `y` contiene √∫nicamente la variable que queremos predecir.
* Usamos `.copy()` para evitar modificar el DataFrame original cuando realicemos transformaciones posteriores.

Supongamos que nuestro Dataframe (variable `df`) contiene:

```
   Survived  Pclass     Sex   Age   Fare  SibSp  Parch Embarked
0         0       3    male  22.0   7.25      1      0        S
1         1       1  female  38.0  71.28      1      0        C
2         1       3  female  26.0   7.92      0      0        S
```

Despu√©s de aplicar el c√≥digo anterior, la variable `X` ser√≠a un Dataframe con el contenido:

``` 
   Pclass     Sex   Age   Fare  SibSp  Parch Embarked
0       3    male  22.0   7.25      1      0        S
1       1  female  38.0  71.28      1      0        C
2       3  female  26.0   7.92      0      0        S
```

Y la variable `y` ser√≠a una Serie con la variable objetivo correspondiente a cada fila de X.

```
0    0
1    1
2    1
Name: Survived, dtype: int64
```

---

### Paso 2.2. Divisi√≥n del dataset

La funci√≥n `train_test_split` de scikit-learn permite dividir el dataset de forma aleatoria en:

* **train** ‚Üí usado para entrenar (ajustar) el modelo.
* **test** ‚Üí usado para evaluar el rendimiento final del modelo.

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,       # el 20% para test, 80% para train
    random_state=42,     # fija la aleatoriedad para reproducir resultados
    stratify=y           # mantiene la proporci√≥n de clases en train y test
)
```

üí° **Explicaci√≥n de los par√°metros:**

* **`test_size=0.2`**
  Usamos un 20% para test. Es est√°ndar para muchos problemas y datasets medianos.

* **`random_state=42`**
  Asegura que todos obtengamos exactamente la misma divisi√≥n.
  Si no lo pones, cada ejecuci√≥n divide el dataset de forma distinta.

* **`stratify=y`**
  Muy importante cuando el target tiene clases desbalanceadas (por ejemplo, supervivencia en Titanic).
  Genera train y test con la **misma proporci√≥n de clases** que el dataset original.
  Ejemplo: si el Titanic tiene un 38% de supervivientes y un 62% de fallecidos, ambos conjuntos mantendr√°n esa proporci√≥n.


## Paso 3. Operaciones despu√©s de la divisi√≥n

Una vez que ya tenemos nuestros conjuntos **X_train, X_test, y_train y y_test**, comienza la fase m√°s importante del preprocesamiento:   todas las transformaciones que **aprenden par√°metros** deben realizarse **ajust√°ndose √∫nicamente al train**, y luego aplicarse al test.

El objetivo principal en esta fase es:

> **Aprender par√°metros con train ‚Üí aplicar esos mismos par√°metros a test.**

Esto evita el **data leakage** y garantiza que el modelo se eval√∫a de forma justa.


:::tip Nota clave: lo que ocurre a partir de aqu√≠

Una vez dividido el dataset, **todas las transformaciones que aprendan informaci√≥n del conjunto** (imputaci√≥n, codificaci√≥n, escalado‚Ä¶) deben seguir esta estructura:

```python
# EJEMPLO DE FLUJO CORRECTO (imputaci√≥n como ejemplo)
imputer = SimpleImputer(strategy="median")

X_train["Age"] = imputer.fit_transform(X_train[["Age"]])  # aprende la mediana solo con train
X_test["Age"]  = imputer.transform(X_test[["Age"]])        # aplica lo aprendido al test
```

üí° **Por qu√© es tan importante:**

* `fit` (entrenar) ‚Üí solo en **train**
* `transform` (aplicar) ‚Üí en **train** y **test**

‚ö†Ô∏è Nunca debe llamarse `fit` con los datos de **test**.

**Ejemplo visual para entenderlo**

Imagina que tenemos esta columna de edades antes de dividir:

```
[20, 30, 40, 80]
```

Despu√©s del *train/test split*, queda as√≠:

Train (lo que el modelo puede ver al aprender):

```
[20, 30, 40]
```

Test (datos nuevos que simulan el futuro):

```
[80]
```

Si hacemos:

```python
imputer.fit(X_train[["Age"]])
```

El imputador **aprende la mediana de train**, que es:

```
mediana(train) = 30
```

Y despu√©s aplicamos:

```python
imputer.transform(X_test[["Age"]])
```

‚Üí El imputador usa **solo la mediana aprendida (30)**, sin mirar el valor 80 del futuro.


**¬øQu√© pasar√≠a si hici√©ramos `fit` tambi√©n con test?**

Estar√≠amos calculando la mediana as√≠:

```
mediana([20, 30, 40, 80]) = 35
```

Es decir, el imputador est√° usando informaci√≥n del test para aprender. Esto es **data leakage** (filtraci√≥n de datos). El modelo estar√≠a ‚Äúviendo el futuro‚Äù y la evaluaci√≥n ya no ser√≠a realista.

:::

---

### Paso 3.1 Tratamiento de valores nulos (post-split)

Ya vimos en EDA que pod√≠amos ver los nulos con la instrucci√≥n:

```python
X_train.isnull().sum()
```

Las estrategias de tratamiento m√°s comunes son:

* **Eliminar filas o columnas** con demasiados nulos (poco habitual aqu√≠, porque ya hicimos la limpieza estructural antes del split).
* **Imputar valores faltantes** usando:
  * Media
  * Mediana (m√°s robusta)
  * Moda (para categ√≥ricas)

---

#### Imputaci√≥n para variables num√©ricas

En Titanic, la columna `Age` tiene valores nulos, as√≠ que usaremos `SimpleImputer` con la **mediana**, que suele funcionar mejor que la media en datos sesgados.

```python
from sklearn.impute import SimpleImputer

# Imputador para columnas num√©ricas con estrategia de mediana
imputer_num = SimpleImputer(strategy="median")

# Ajustamos (fit) SOLO con train
X_train["Age"] = imputer_num.fit_transform(X_train[["Age"]])

# Transformamos test con lo aprendido
X_test["Age"] = imputer_num.transform(X_test[["Age"]])
```

üí° **Explicaci√≥n:**

* `fit_transform()` aprende la mediana de Age en train ‚Üí la aplica al propio train.
* `transform()` aplica esa misma mediana al test, sin recalcular nada.

:::info ¬øMedia o mediana? Decisi√≥n seg√∫n la distribuci√≥n

Para decidir si debemos imputar con **media** o **mediana**, es importante observar **la forma de la distribuci√≥n**. En el caso de la edad en el Titanic, vemos que **no tiene forma de campana sim√©trica** (no es una distribuci√≥n normal).  
Hay muchos valores concentrados entre 15 y 35 a√±os, y luego la cola se alarga hacia la derecha.

Esto significa que la variable est√° **sesgada** (asim√©trica), y por tanto **la media estar√≠a demasiado influida por los valores altos**. En cambio, la **mediana** es m√°s robusta y representa mejor el valor central real.

Por eso, en este caso, **la estrategia recomendada es usar la mediana** para imputar los valores nulos de `Age`.

![Gr√°fico EDA](./0-img/histograma-edad.png)

:::

---

#### Imputaci√≥n para variables categ√≥ricas

Ejemplo con `Embarked`:

```python
# Imputador para columnas categ√≥ricas, qued√°ndose con la categor√≠a m√°s frecuente
imputer_cat = SimpleImputer(strategy="most_frequent")

X_train["Embarked"] = imputer_cat.fit_transform(X_train[["Embarked"]])
X_test["Embarked"]  = imputer_cat.transform(X_test[["Embarked"]])
```

Esto rellena los valores nulos con la **categor√≠a m√°s frecuente** de los pasajeros en *train*.

---

### Paso 3.2 Codificaci√≥n de variables categ√≥ricas

Los modelos de Machine Learning **no pueden trabajar directamente con texto**, por lo que todas las variables categ√≥ricas deben convertirse a valores num√©ricos. 

Para ello existen dos t√©cnicas principales: **Label Encoding** y **One-Hot Encoding**. Cada una se usa en situaciones distintas y genera tipos de columnas diferentes.

---

#### üî¢ Label Encoding (para variables *ordinales*)

El **Label Encoding** convierte cada categor√≠a en un n√∫mero entero. Es √∫til **solo cuando las categor√≠as tienen un orden l√≥gico**, es decir, una jerarqu√≠a natural. Por ejemplo:

* Talla de ropa: `S < M < L < XL`
* Nivel de educaci√≥n: `Primaria < Secundaria < Universidad`
* Clase en Titanic: `1st < 2nd < 3rd`


**No se debe usar cuando las categor√≠as no tienen orden**, porque el modelo entender√≠a err√≥neamente que un valor ‚Äúvale m√°s‚Äù que otro.

**Ejemplo peque√±o**

```python
from sklearn.preprocessing import LabelEncoder

enc = LabelEncoder()

sizes = ["S", "M", "L", "S", "XL", "M"]
encoded = enc.fit_transform(sizes)

print(encoded)
```

Posible salida:

```
[0 1 2 0 3 1]
```

### üìå ¬øQu√© columnas genera?

Genera **una sola columna num√©rica**, en la que cada categor√≠a es reemplazada por un n√∫mero entero.

Por ejemplo:

| Talla | Encoded |
| ----- | ------- |
| S     | 0       |
| M     | 1       |
| L     | 2       |
| XL    | 3       |

---

## üéõÔ∏è One-Hot Encoding (para variables *nominales*)

El **One-Hot Encoding** transforma cada categor√≠a en una columna nueva, con valores 0/1 (presencia o ausencia).
Es la opci√≥n correcta cuando **las categor√≠as NO tienen orden**.

### ‚úîÔ∏è Cu√°ndo usarlo

Cuando las categor√≠as son simplemente nombres y **no existe jerarqu√≠a**:

* Sexo: `male`, `female`
* Puerto de embarque: `S`, `C`, `Q`
* Tipo de producto: `A`, `B`, `C`

### ‚ùå Cu√°ndo NO usarlo

Cuando la variable tiene demasiadas categor√≠as (cientos o miles), porque genera muchas columnas.

### üîç Ejemplo peque√±o

```python
import pandas as pd

df = pd.DataFrame({"Embarked": ["S", "C", "Q", "S"]})
encoded = pd.get_dummies(df, columns=["Embarked"])

encoded
```

Salida:

```
   Embarked_C  Embarked_Q  Embarked_S
0           0           0           1
1           1           0           0
2           0           1           0
3           0           0           1
```

### üìå ¬øQu√© columnas genera?

El One-Hot crea **una columna por categor√≠a**, con 1 si la fila pertenece a esa categor√≠a y 0 si no.

Por ejemplo, a partir de:

```
Embarked
S
C
Q
S
```

Se obtiene:

| Embarked_C | Embarked_Q | Embarked_S |
| ---------- | ---------- | ---------- |
| 0          | 0          | 1          |
| 1          | 0          | 0          |
| 0          | 1          | 0          |
| 0          | 0          | 1          |

---

## üß† ¬øC√≥mo elegir entre Label y One-Hot?

| Tipo de variable          | Ejemplo             | Codificaci√≥n recomendada           |
| ------------------------- | ------------------- | ---------------------------------- |
| **Ordinal** (tiene orden) | Clase del Titanic   | Label Encoding                     |
| **Nominal** (sin orden)   | Sexo, puerto        | One-Hot Encoding                   |
| **Much√≠simas categor√≠as** | C√≥digos √∫nicos, IDs | Ninguna (mejor eliminar o agrupar) |

---

## üß© Ejemplo real con Titanic

**Sex** ‚Üí no tiene orden ‚Üí `One-Hot Encoding`
**Embarked** ‚Üí tampoco ‚Üí `One-Hot Encoding`
**Pclass** ‚Üí s√≠ tiene orden (1 < 2 < 3) ‚Üí `Label Encoding` opcional, aunque muchos modelos la aceptan directamente como n√∫mero

---

üí¨ **Conclusi√≥n:**

> Usa **Label Encoding** solo cuando las categor√≠as tienen un orden real.
> Usa **One-Hot Encoding** cuando las categor√≠as no tienen jerarqu√≠a, como `Sex` o `Embarked` en Titanic.
> Ambos m√©todos permiten que los modelos trabajen correctamente con variables categ√≥ricas.



</div>







# üß© √çNDICE REVISADO ‚Äî *Preprocesamiento de Datos en Machine Learning (Nivel Intermedio)*

## Introducci√≥n

* Qu√© es el preprocesamiento y por qu√© es esencial tras el EDA.
* Diferencia entre EDA y preprocesamiento.
* Objetivo: dejar los datos limpios, coherentes y listos para entrenar el modelo.
* Herramientas principales: `pandas` y `sklearn.preprocessing`.

---

## √çndice del tema ‚Äî Preprocesamiento (reestructurado por fases)

### Paso 1. Operaciones **antes** de la divisi√≥n (*pre-split*)
* Objetivo: dejar el dataset **coherente y limpio** a nivel estructural.
* 1.1 Limpieza estructural
  - Eliminar columnas irrelevantes (IDs, texto libre sin uso).
  - Correcci√≥n de **tipos de datos** (`object` ‚Üí `int`, `float`, `datetime`).
  - Normalizaci√≥n b√°sica de formatos (espacios, may√∫sculas/min√∫sculas, categor√≠as mal escritas).
* 1.2 Duplicados
  - Detecci√≥n: `df.duplicated()`
  - Eliminaci√≥n: `drop_duplicates()`
* 1.3 Outliers y errores **evidentes**
  - Detecci√≥n visual (boxplot) y sentido com√∫n.
  - Acci√≥n: corregir o eliminar solo los **imposibles** (no tratamiento estad√≠stico a√∫n).

---

### Paso 2. **Divisi√≥n** en *train/test*
* Por qu√© dividir antes de ‚Äúaprender‚Äù par√°metros (evitar **data leakage**).
* Separar `X` (features) y `y` (target).
* `train_test_split` (proporci√≥n, `random_state`, `stratify` cuando aplique).
* Nota: A partir de aqu√≠, todo lo que **aprende** algo de los datos se **ajusta en train** y se **aplica a test**.

---

### Paso 3. Operaciones **despu√©s** de la divisi√≥n (*post-split*)
* Objetivo: **aprender** par√°metros con *train* y **transformar** *test* con los mismos.

* 3.1 Valores nulos
  - Detecci√≥n: `isnull().sum()`, `info()`.
  - Estrategias:
    - Eliminaci√≥n de filas/columnas con demasiados nulos (si procede).
    - **Imputaci√≥n** (media, mediana, moda) con `SimpleImputer`.
    - Imputaci√≥n condicional (por grupos) cuando tenga sentido.
  - Ejemplo con Titanic y `SimpleImputer`.

* 3.2 Codificaci√≥n de variables categ√≥ricas
  - Cu√°ndo usar **Label Encoding** (ordinales).
  - Cu√°ndo usar **One-Hot Encoding** (nominales).
  - Implementaci√≥n con `OneHotEncoder` o `pandas.get_dummies()` (manteniendo la l√≥gica *fit* en train y *transform* en test).

* 3.3 Escalado y normalizaci√≥n de variables num√©ricas
  - **StandardScaler** (media 0, desviaci√≥n 1).
  - **MinMaxScaler** (0 a 1).
  - Aplicaci√≥n correcta: `.fit()` en *train*, `.transform()` en *test*.

* 3.4 (Opcional y simple) Feature Engineering b√°sico
  - Creaci√≥n de variables intuitivas (e.g., `FamilySize`).
  - Eliminaci√≥n de variables redundantes tras crear nuevas.

---

### Paso 4. Preparaci√≥n final del dataset
* Revisi√≥n r√°pida de coherencia tras transformaciones.
* Conjunto listo para el modelado.
* Exportaci√≥n: `to_csv` (dataset limpio y, si aplica, versiones transformadas).

---

### Ejemplo completo: *Preprocesamiento Titanic*
* Aplicaci√≥n paso a paso con c√≥digo y breves conclusiones por bloque.

---

### Actividad de seguimiento: *Preprocesamiento Employee Attrition*
* Dataset: `employee.csv`
* Tareas:
  - **Antes del split:** limpieza estructural, duplicados, outliers imposibles.
  - **Split train/test.**
  - **Despu√©s del split:** imputaci√≥n con `SimpleImputer`, codificaci√≥n, escalado, FE b√°sico.
  - Exportaci√≥n del dataset limpio.
* Entregable: cuaderno de Google Colab.

---

> üí° **Regla de oro:** Todo lo que **aprende par√°metros** de los datos (medias, modas, categor√≠as, escalas‚Ä¶) **se ajusta con *train*** y **se aplica a *test*** sin volver a aprender.
