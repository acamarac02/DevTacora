---
title: "Preprocesamiento de Datos"
sidebar_position: 4
toc_max_heading_level: 4
description: "Introducci√≥n al Preprocesamiento de Datos en Machine Learning. Qu√© es, por qu√© se realiza despu√©s del EDA, principales pasos para limpiar, transformar y preparar los datos antes del modelado utilizando pandas y scikit-learn."
keywords: [Preprocesamiento, Machine Learning, pandas, scikit-learn, limpieza de datos, escalado, codificaci√≥n, preparaci√≥n de datos]
---

<div class="justify-text">

El **Preprocesamiento de Datos** es la **segunda etapa del proceso de Machine Learning**, y comienza justo despu√©s de haber realizado el **An√°lisis Exploratorio de Datos (EDA)**.  
Si en el EDA analizamos y comprendemos los datos, en el preprocesamiento el objetivo es **prepararlos adecuadamente** para que los algoritmos de aprendizaje autom√°tico puedan trabajar con ellos de forma correcta y eficiente.

Durante esta fase, se realizan todas las tareas necesarias para **limpiar, transformar y estandarizar** los datos.  
Esto incluye, por ejemplo, rellenar valores nulos, eliminar duplicados, codificar variables categ√≥ricas o escalar las num√©ricas.

<div class="hidden-summary">

## Introducci√≥n

</div>

### ¬øPor qu√© es esencial tras el EDA?

El EDA nos permite descubrir **problemas o irregularidades** en el dataset: valores perdidos, outliers, tipos de datos incorrectos o variables irrelevantes.  
El preprocesamiento es la fase donde **se corrigen esos problemas** y se dejan los datos listos para el modelado.


| Etapa | Objetivo principal | Tipo de tareas |
|--------|--------------------|----------------|
| **EDA** | Comprender los datos y detectar patrones o errores. | An√°lisis descriptivo, visualizaci√≥n, detecci√≥n de nulos y outliers. |
| **Preprocesamiento** | Corregir, transformar y preparar los datos para el modelo. | Limpieza, imputaci√≥n, codificaci√≥n, escalado, selecci√≥n de variables. |

üí¨ **Ejemplo:**
En el Titanic, durante el EDA descubrimos que hay valores nulos en `Age` y `Cabin`, y que `Fare` tiene outliers.  
En el preprocesamiento, decidiremos **c√≥mo rellenar esos nulos**, **si eliminar o ajustar los outliers**, y **c√≥mo convertir las variables categ√≥ricas** (como `Sex` o `Embarked`) a formato num√©rico.

---

### Objetivos del preprocesamiento

El objetivo final es dejar el dataset:

* **Limpio:** sin errores, duplicados ni valores faltantes.  
* **Coherente:** con tipos de datos correctos y valores representativos.  
* **Preparado:** con todas las variables transformadas en un formato num√©rico adecuado para los modelos de Machine Learning.

En otras palabras, buscamos que los datos sean **de calidad y comparables entre s√≠**, para que el modelo aprenda de forma fiable.

---

### Herramientas que usaremos

En Python, las principales herramientas para el preprocesamiento son:

* **`pandas`** ‚Üí para limpiar, transformar y manipular los datos.  
* **`scikit-learn (sklearn.preprocessing)`** ‚Üí para aplicar transformaciones autom√°ticas como imputaci√≥n, codificaci√≥n y escalado.  

Estas librer√≠as se complementan y permiten llevar a cabo todo el flujo de preparaci√≥n de datos antes de entrenar el modelo.

---

## Paso 1. Operaciones antes de la divisi√≥n

Antes de separar el dataset en conjuntos de entrenamiento (*train*) y prueba (*test*), debemos realizar una **limpieza estructural b√°sica**.  
Este paso no modifica el contenido estad√≠stico de los datos, sino que se asegura de que el dataset sea **coherente, entendible y correctamente tipado**.

Estas operaciones se hacen **antes de dividir** porque afectan por igual a todos los registros y no implican ning√∫n tipo de aprendizaje sobre los valores de los datos (no calculan medias, ni modas, ni escalados).

---

### Paso 1.1 Limpieza estructural

El objetivo de la limpieza estructural es **asegurar que el dataset est√© bien formado**:
* Comprobar que los tipos de datos son correctos.
* Corregir formatos o errores de escritura en las variables.

A continuaci√≥n veremos cada una de estas operaciones aplicadas al dataset **Titanic**.

---

#### Correcci√≥n de tipos de datos

Los tipos de datos son fundamentales.
Pandas puede leer columnas con el tipo incorrecto (por ejemplo, n√∫meros como texto o fechas como `object`), lo que puede generar errores en el an√°lisis y el modelado.

Podemos revisar los tipos con:

```python
df.info()
```

Si detectamos alguna columna con tipo err√≥neo, podemos convertirla usando `astype()`.

Ejemplo: supongamos que una columna num√©rica ha sido le√≠da como texto (`object`):

```python
# Convertir una columna a tipo num√©rico
df["Fare"] = df["Fare"].astype(float)
```

üí° **Explicaci√≥n te√≥rica:**

* Los modelos de Machine Learning requieren que **las variables num√©ricas sean realmente num√©ricas**, no cadenas de texto.
* Asegurar los tipos correctos evita errores posteriores en imputaci√≥n, escalado o codificaci√≥n.

üí¨ **Conclusi√≥n:**

> Revisar y corregir los tipos de datos garantiza que todas las columnas se comporten como se espera en las transformaciones posteriores.

---

#### Normalizaci√≥n de formatos y categor√≠as

En las variables categ√≥ricas o de texto, a veces encontramos **errores de formato**:

* Espacios en blanco (`" S "` en lugar de `"S"`).
* May√∫sculas/min√∫sculas mezcladas (`"Male"` vs `"male"`).
* Valores mal escritos (`"Southampton"` vs `"southampton"`).

Estos errores de formato los veriamos al ejecutar `unique()` sobre una variable categ√≥rica o al generar gr√°ficos sobre ellas. A continuaci√≥n se muestra un ejemplo donde metemos una dato err√≥neo en Titanic para estudiar c√≥mo solucionarlo:

```python
# Crear una nueva fila con "Male" como sexo para que podamos ver el error
nueva_fila = {
    "PassengerId": 9999,
    "Survived": 0,
    "Pclass": 3,
    "Name": "Smith, Mr. John",
    "Sex": "Male",
    "Age": 32,
    "SibSp": 0,
    "Parch": 0,
    "Ticket": "A/5 9999",
    "Fare": 7.25,
    "Cabin": "",
    "Embarked": "S"
}

# A√±adir la fila al DataFrame
df = pd.concat([df, pd.DataFrame([nueva_fila])], ignore_index=True)

# Mostrar los valores √∫nicos de la columna 'Sex'
print(df["Sex"].unique())
```

La salida obtenida en este caso ser√≠a:

```python
array(['male', 'female', 'Male'], dtype=object)
```

Estos detalles pueden hacer que el modelo interprete **dos valores iguales como distintos**, lo que distorsiona la codificaci√≥n posterior.

Ese problema anterior se solucionar√≠a pasando a min√∫sculas todos los valores:

```python
# Normalizar los valores de la columna 'Sex'
# Pasamos a min√∫sculas y, de paso, eliminamos posibles espacios
df["Sex"] = df["Sex"].str.lower().str.strip()

# Verificar que se ha corregido
print(df["Sex"].unique())
```

Salida esperada:

```python
array(['male', 'female'], dtype=object)
```

Si aparecieran errores tipogr√°ficos, se pueden reemplazar manualmente:

```python
# Ejemplo: sustituir valores incorrectos (si los hubiera)
df["Embarked"].replace({"SOUTHAMPTON": "S", "CHERBOURG": "C", "QUEENSTOWN": "Q"}, inplace=True)
```

üí° **Explicaci√≥n te√≥rica:**

* La **homogeneizaci√≥n de categor√≠as** es clave para que el modelo reconozca correctamente las clases.
* Si dos valores equivalentes se escriben distinto, el modelo los tratar√° como categor√≠as diferentes.

üí¨ **Conclusi√≥n:**

> Antes de codificar las variables categ√≥ricas, debemos garantizar que todas las categor√≠as est√©n escritas de forma uniforme.

---

### Paso 1.2 Duplicados

En muchos datasets reales, especialmente cuando los datos provienen de distintas fuentes o se han unido varios ficheros, es frecuente encontrar **filas duplicadas**.  
Estas duplicidades pueden provocar que el modelo **aprenda varias veces la misma informaci√≥n**, generando sesgos o influyendo en las estad√≠sticas de forma incorrecta.

Por eso, un paso b√°sico de la limpieza estructural consiste en **detectar y eliminar los registros duplicados** antes de seguir con el preprocesamiento.

En el An√°lisis Exploratorio de Datos ya estudiamos c√≥mo consultar los duplicados:

```python
# Comprobar si hay filas duplicadas en el dataset
df.duplicated().sum()
````

Salida esperada (en el Titanic original):

```python
0
```

üí° **Explicaci√≥n te√≥rica:**

* Si el resultado es `0`, significa que **no hay filas duplicadas exactas**.
* Si devuelve un n√∫mero mayor que `0`, indica cu√°ntos registros est√°n repetidos completamente.

Podemos ver cu√°les son esas filas duplicadas con:

```python
# Mostrar las filas duplicadas (si las hubiera)
df[df.duplicated()]
```

Si detectamos registros repetidos, se eliminan f√°cilmente con `drop_duplicates()`:

```python
# Eliminar filas duplicadas
df = df.drop_duplicates()
```

üí° **Explicaci√≥n te√≥rica:**

* Por defecto, `drop_duplicates()` elimina las filas repetidas **manteniendo la primera aparici√≥n**.
* Este m√©todo elimina duplicados considerando **todas las columnas**.

:::danger Nota importante sobre el orden EDA ‚Üí Preprocesamiento

Los pasos **1.1 (Correcci√≥n de tipos y normalizaci√≥n de categor√≠as)** y **1.2 (Eliminaci√≥n de columnas irrelevantes o duplicadas)** pueden realizarse **durante el EDA**, antes de generar gr√°ficos univariantes y bivariantes.
Esto permite obtener **gr√°ficos limpios y coherentes**, evitando categor√≠as duplicadas (‚Äúmale‚Äù, ‚ÄúMale‚Äù, ‚Äú male ‚Äù), tipos incorrectos o columnas que no aportan valor visual.

Sin embargo, **todos los pasos restantes del preprocesamiento** (tratamiento de valores imposibles, imputaci√≥n, codificaci√≥n, escalado, feature engineering‚Ä¶) **deben realizarse √∫nicamente despu√©s de completar el EDA**, cuando ya se ha comprendido la estructura y los problemas del dataset.

:::

---

### Paso 1.3 Outliers y errores evidentes

Los **outliers** (valores at√≠picos) son datos que se alejan mucho del resto de observaciones. A veces son simplemente **casos reales extremos** (por ejemplo, una tarifa muy alta en primera clase), pero otras veces se deben a **errores de registro o introducci√≥n de datos** (por ejemplo, una edad de 250 a√±os).

En esta fase inicial del preprocesamiento, **no buscamos eliminar todos los valores extremos**, sino **detectar y corregir solo los claramente imposibles o err√≥neos**.

Una forma sencilla de detectar outliers es mediante un **boxplot (diagrama de caja)**. Por ejemplo, si introducimos un registro nuevo con una edad de 250 a√±os, podr√≠amos ver lo siguiente: 

![Gr√°fico EDA](./0-img/boxplot-valor-erroneo.png)

No todos los outliers deben eliminarse. Algunos son **casos reales v√°lidos**, y eliminarlos podr√≠a distorsionar el modelo.
Por eso, antes de borrar nada, conviene preguntarse:

> ¬øEste valor es posible en el contexto del dataset?

Veamos ejemplos:

```python
# Revisar valores m√°ximos en columnas num√©ricas
df[["Age", "Fare"]].max()
```

Posible salida:

```
Age     80.0
Fare    512.3292
dtype: float64
```

üí° **Interpretaci√≥n:**

* Una edad de 80 a√±os es realista (no hay problema).
* Una tarifa de m√°s de 500 tambi√©n puede ser v√°lida para pasajeros de primera clase.
* Pero si encontr√°ramos una edad de **250 a√±os** o un valor negativo en la tarifa (`Fare = -10`), ser√≠an claramente errores de registro.

Podemos usar condiciones simples para **detectar valores imposibles** y corregirlos o eliminarlos. En nuestro caso, vamos a sustituirlos por valores vac√≠os, que m√°s tarde se imputar√°n.

```python
# Ejemplo: eliminar edades imposibles
df.loc[(df["Age"] < 0) | (df["Age"] > 120), "Age"] = np.nan

# Ejemplo: corregir tarifas negativas (si existieran)
# Esto aprende de datos, se deber√≠a hacer despu√©s de dividir en train y test
df.loc[df["Fare"] < 0, "Fare"] = df["Fare"].median()
```

üí° **Explicaci√≥n te√≥rica:**

* Usamos **filtros l√≥gicos** (`df["Age"] <= 100`) para quedarnos solo con valores v√°lidos.
* Si un valor err√≥neo es aislado y no queremos eliminar la fila completa, podemos **reemplazarlo** por un valor representativo (por ejemplo, la mediana).

:::warning CUIDADO CON EL TRATAMIENTO DE LOS OUTLIERS
Si lo que queremos es **corregir datos**, como el caso de la tarifa que se explicaba antes, habr√≠a que hacerlo **despu√©s de dividir en train y test**.
:::

---

## Paso 2. Divisi√≥n en *train/test*

Una vez que el dataset est√° limpio a nivel **estructural** (sin duplicados, sin columnas irrelevantes, sin tipos incorrectos y sin errores evidentes), estamos listos para el paso m√°s importante del flujo de Machine Learning: la **divisi√≥n en conjuntos de entrenamiento y prueba**.

Este paso es fundamental para construir modelos fiables y evaluar su rendimiento de forma justa.

:::info ¬øPor qu√© dividir antes de ‚Äúaprender‚Äù par√°metros?

Cada vez que aplicamos una t√©cnica de preprocesamiento que **aprende algo de los datos**, como calcular la **mediana** para imputar nulos, determinar las **categor√≠as** para One-Hot Encoding, calcular la **media y desviaci√≥n** para escalar, estamos extrayendo **informaci√≥n estad√≠stica** del dataset.

Si hacemos esta extracci√≥n **antes** de dividir, estar√≠amos utilizando **informaci√≥n del futuro** (del conjunto de test) para preparar nuestros datos. Esto se llama **data leakage** (*filtraci√≥n de datos*), y provoca:

* modelos que parecen m√°s precisos de lo que realmente son,  
* una evaluaci√≥n injusta,  
* generalizaci√≥n mucho peor en datos nuevos.

üí° **Regla de oro del preprocesamiento:**

> Todo lo que **aprende par√°metros** debe ajustarse **solo con los datos de entrenamiento**, y luego aplicarse a los datos de prueba sin volver a aprender nada.
:::

---

### Paso 2.1. Separar las features y target

Antes de dividir, debemos indicar:
* qu√© columnas vamos a usar para predecir ‚Üí **X** (features) 
* qu√© columna queremos predecir ‚Üí **y** (target)

Ejemplo con Titanic:

```python
# Seleccionamos las columnas relevantes (features) y la variable objetivo (target)
# En EDA ya dejamos claro que Name, Cabin, etc. no eran √∫tiles
features = ['Pclass', 'Sex', 'Age', 'Fare', 'SibSp', 'Parch', 'Embarked']
target = 'Survived'

X = df[features].copy()
y = df[target]
```

üí° **Explicaci√≥n te√≥rica:**

* `X` contiene todas las variables que el modelo utilizar√° como entrada.
* `y` contiene √∫nicamente la variable que queremos predecir.
* Usamos `.copy()` para evitar modificar el DataFrame original cuando realicemos transformaciones posteriores.

Supongamos que nuestro Dataframe (variable `df`) contiene:

```
   Survived  Pclass     Sex   Age   Fare  SibSp  Parch Embarked
0         0       3    male  22.0   7.25      1      0        S
1         1       1  female  38.0  71.28      1      0        C
2         1       3  female  26.0   7.92      0      0        S
```

Despu√©s de aplicar el c√≥digo anterior, la variable `X` ser√≠a un Dataframe con el contenido:

``` 
   Pclass     Sex   Age   Fare  SibSp  Parch Embarked
0       3    male  22.0   7.25      1      0        S
1       1  female  38.0  71.28      1      0        C
2       3  female  26.0   7.92      0      0        S
```

Y la variable `y` ser√≠a una Serie con la variable objetivo correspondiente a cada fila de X.

```
0    0
1    1
2    1
Name: Survived, dtype: int64
```

---

### Paso 2.2. Divisi√≥n del dataset

La funci√≥n `train_test_split` de scikit-learn permite dividir el dataset de forma aleatoria en:

* **train** ‚Üí usado para entrenar (ajustar) el modelo.
* **test** ‚Üí usado para evaluar el rendimiento final del modelo.

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,       # el 20% para test, 80% para train
    random_state=42,     # fija la aleatoriedad para reproducir resultados
    stratify=y           # mantiene la proporci√≥n de clases en train y test
)
```

üí° **Explicaci√≥n de los par√°metros:**

* **`test_size=0.2`**
  Usamos un 20% para test. Es est√°ndar para muchos problemas y datasets medianos.

* **`random_state=42`**
  Asegura que todos obtengamos exactamente la misma divisi√≥n.
  Si no lo pones, cada ejecuci√≥n divide el dataset de forma distinta.

* **`stratify=y`**
  Muy importante cuando el target tiene clases desbalanceadas (por ejemplo, supervivencia en Titanic).
  Genera train y test con la **misma proporci√≥n de clases** que el dataset original.
  Ejemplo: si el Titanic tiene un 38% de supervivientes y un 62% de fallecidos, ambos conjuntos mantendr√°n esa proporci√≥n.


## Paso 3. Operaciones despu√©s de la divisi√≥n

Una vez que ya tenemos nuestros conjuntos **X_train, X_test, y_train y y_test**, comienza la fase m√°s importante del preprocesamiento:   todas las transformaciones que **aprenden par√°metros** deben realizarse **ajust√°ndose √∫nicamente al train**, y luego aplicarse al test.

El objetivo principal en esta fase es:

> **Aprender par√°metros con train ‚Üí aplicar esos mismos par√°metros a test.**

Esto evita el **data leakage** y garantiza que el modelo se eval√∫a de forma justa.


:::tip Nota clave: lo que ocurre a partir de aqu√≠

Una vez dividido el dataset, **todas las transformaciones que aprendan informaci√≥n del conjunto** (imputaci√≥n, codificaci√≥n, escalado‚Ä¶) deben seguir esta estructura:

```python
# EJEMPLO DE FLUJO CORRECTO (imputaci√≥n como ejemplo)
imputer = SimpleImputer(strategy="median")

X_train["Age"] = imputer.fit_transform(X_train[["Age"]])  # aprende la mediana solo con train
X_test["Age"]  = imputer.transform(X_test[["Age"]])        # aplica lo aprendido al test
```

üí° **Por qu√© es tan importante:**

* `fit` (entrenar) ‚Üí solo en **train**
* `transform` (aplicar) ‚Üí en **train** y **test**

‚ö†Ô∏è Nunca debe llamarse `fit` con los datos de **test**.

**Ejemplo visual para entenderlo**

Imagina que tenemos esta columna de edades antes de dividir:

```
[20, 30, 40, 80]
```

Despu√©s del *train/test split*, queda as√≠:

Train (lo que el modelo puede ver al aprender):

```
[20, 30, 40]
```

Test (datos nuevos que simulan el futuro):

```
[80]
```

Si hacemos:

```python
imputer.fit(X_train[["Age"]])
```

El imputador **aprende la mediana de train**, que es:

```
mediana(train) = 30
```

Y despu√©s aplicamos:

```python
imputer.transform(X_test[["Age"]])
```

‚Üí El imputador usa **solo la mediana aprendida (30)**, sin mirar el valor 80 del futuro.


**¬øQu√© pasar√≠a si hici√©ramos `fit` tambi√©n con test?**

Estar√≠amos calculando la mediana as√≠:

```
mediana([20, 30, 40, 80]) = 35
```

Es decir, el imputador est√° usando informaci√≥n del test para aprender. Esto es **data leakage** (filtraci√≥n de datos). El modelo estar√≠a ‚Äúviendo el futuro‚Äù y la evaluaci√≥n ya no ser√≠a realista.

:::

---

### Paso 3.1 Tratamiento de valores nulos (post-split)

Ya vimos en EDA que pod√≠amos ver los nulos con la instrucci√≥n:

```python
X_train.isnull().sum()
```

Las estrategias de tratamiento m√°s comunes son:

* **Eliminar filas o columnas** con demasiados nulos (poco habitual aqu√≠, porque ya hicimos la limpieza estructural antes del split).
* **Imputar valores faltantes** usando:
  * Media
  * Mediana (m√°s robusta)
  * Moda (para categ√≥ricas)

---

#### Imputaci√≥n para variables num√©ricas

En Titanic, la columna `Age` tiene valores nulos, as√≠ que usaremos `SimpleImputer` con la **mediana**, que suele funcionar mejor que la media en datos sesgados.

```python
from sklearn.impute import SimpleImputer

# Imputador para columnas num√©ricas con estrategia de mediana
imputer_num = SimpleImputer(strategy="median")

# Ajustamos (fit) SOLO con train
X_train["Age"] = imputer_num.fit_transform(X_train[["Age"]])

# Transformamos test con lo aprendido
X_test["Age"] = imputer_num.transform(X_test[["Age"]])
```

üí° **Explicaci√≥n:**

* `fit_transform()` aprende la mediana de Age en train ‚Üí la aplica al propio train.
* `transform()` aplica esa misma mediana al test, sin recalcular nada.

:::info ¬øMedia o mediana? Decisi√≥n seg√∫n la distribuci√≥n

Para decidir si debemos imputar con **media** o **mediana**, es importante observar **la forma de la distribuci√≥n**. En el caso de la edad en el Titanic, vemos que **no tiene forma de campana sim√©trica** (no es una distribuci√≥n normal).  
Hay muchos valores concentrados entre 15 y 35 a√±os, y luego la cola se alarga hacia la derecha.

Esto significa que la variable est√° **sesgada** (asim√©trica), y por tanto **la media estar√≠a demasiado influida por los valores altos**. En cambio, la **mediana** es m√°s robusta y representa mejor el valor central real.

Por eso, en este caso, **la estrategia recomendada es usar la mediana** para imputar los valores nulos de `Age`.

![Gr√°fico EDA](./0-img/histograma-edad.png)

:::

---

#### Imputaci√≥n para variables categ√≥ricas

Ejemplo con `Embarked`:

```python
# Imputador para columnas categ√≥ricas, qued√°ndose con la categor√≠a m√°s frecuente
imputer_cat = SimpleImputer(strategy="most_frequent")

X_train["Embarked"] = imputer_cat.fit_transform(X_train[["Embarked"]]).ravel()
X_test["Embarked"]  = imputer_cat.transform(X_test[["Embarked"]]).ravel()
```

Esto rellena los valores nulos con la **categor√≠a m√°s frecuente** de los pasajeros en *train*.

---

### Paso 3.2 Codificaci√≥n de variables categ√≥ricas

Los modelos de Machine Learning **no pueden trabajar directamente con texto**, por lo que todas las variables categ√≥ricas deben convertirse a valores num√©ricos. 

Para ello existen dos t√©cnicas principales: **Label Encoding** y **One-Hot Encoding**. Cada una se usa en situaciones distintas y genera tipos de columnas diferentes.

---

#### ¬øC√≥mo elegir entre Label y One-Hot?

| Tipo de variable          | Ejemplo             | Codificaci√≥n recomendada           |
| ------------------------- | ------------------- | ---------------------------------- |
| **Ordinal** (tiene orden) | Clase del Titanic   | Label Encoding                     |
| **Nominal** (sin orden)   | Sexo, puerto        | One-Hot Encoding                   |
| **Much√≠simas categor√≠as** | C√≥digos √∫nicos, IDs | Ninguna (mejor eliminar o agrupar) |

---

#### Label Encoding (para variables *ordinales*)

Vamos a suponer que en el Titanic la columna **Pclass** no viniera como n√∫meros `1, 2, 3`, sino como texto:

```text
"1st", "2nd", "3rd"
```

Es una variable **categ√≥rica ordinal** (hay un orden claro: 1st < 2nd < 3rd), as√≠ que aqu√≠ **s√≠ tiene sentido** usar **Label Encoding**.

La idea es la misma que con los nulos:

üëâ **Ajustamos (fit) solo con `X_train`** y despu√©s **aplicamos (transform) a `X_train` y `X_test`**.

```python
from sklearn.preprocessing import LabelEncoder

encoder_pclass = LabelEncoder()

# Ajustamos el encoder SOLO con los datos de train
encoder_pclass.fit(X_train["Pclass"])

# Transformamos train y test con el mismo mapeo aprendido
X_train["Pclass_encoded"] = encoder_pclass.transform(X_train["Pclass"])
X_test["Pclass_encoded"]  = encoder_pclass.transform(X_test["Pclass"])
```

Si las clases fueran, por ejemplo:

```text
["1st", "2nd", "3rd"]
```

el encoder podr√≠a aprender algo como:

```text
"1st" -> 0
"2nd" -> 1
"3rd" -> 2
```

Y las nuevas columnas quedar√≠an as√≠:

| Pclass | Pclass_encoded |
| ------ | -------------- |
| 1st    | 0              |
| 3rd    | 2              |
| 2nd    | 1              |

üí° **Importante:**

* `fit` aprende qu√© categor√≠as existen y en qu√© orden las codifica ‚Üí **solo en X_train**.
* `transform` aplica ese mismo mapeo a **X_train y X_test**.
* Genera **una √∫nica columna num√©rica**, que conserva el orden natural de la variable (`1st < 2nd < 3rd`).

---

#### One-Hot Encoding (para variables *nominales*)

En el Titanic, columnas como **Sex** o **Embarked** contienen categor√≠as que **no tienen un orden**:

```text
Sex:       "male", "female"
Embarked:  "S", "C", "Q"
```

Cuando las categor√≠as **no tienen jerarqu√≠a**, no podemos asignarles n√∫meros como 0, 1, 2 porque el modelo podr√≠a interpretar err√≥neamente que uno ‚Äúvale m√°s‚Äù que otro.

üëâ En estos casos, la t√©cnica correcta es **One-Hot Encoding**.

Esta t√©cnica crea **una columna por cada categor√≠a**, con valores 0/1 indicando si esa fila pertenece a esa categor√≠a.

La idea es la misma que antes:

* Ajustamos (fit) el codificador solo con `X_train`
* Transformamos tanto `X_train` como `X_test` con lo aprendido

**Ejemplo con la columna `Embarked` del Titanic**

Vamos a transformar esta variable nominal en columnas num√©ricas:

```python
from sklearn.preprocessing import OneHotEncoder

# Empezamos por Embarked
encoder_embarked = OneHotEncoder(handle_unknown="ignore", sparse_output=False)

# Ajustamos SOLO con los datos de train
encoder_embarked.fit(X_train[["Embarked"]])

# Transformamos train y test usando lo aprendido
# embarked_train y embarked_test son arrays numpy, hay que procesarlos m√°s tarde para obtener un DataFrame
embarked_train = encoder_embarked.transform(X_train[["Embarked"]])
embarked_test  = encoder_embarked.transform(X_test[["Embarked"]])

# Convertimos las matrices a DataFrames para verlas mejor y a√±adirlas posteriormente a nuestro DataFrame completo
embarked_train = pd.DataFrame(embarked_train, 
                              columns=encoder_embarked.get_feature_names_out(["Embarked"]),
                              index=X_train.index)
embarked_test  = pd.DataFrame(embarked_test,
                              columns=encoder_embarked.get_feature_names_out(["Embarked"]),
                              index=X_test.index)
```

Vamos a entender c√≥mo funciona el c√≥digo anterior. Supongamos que tenemos este DataFrame:


| Embarked |
| -------- |
| S        |
| C        |
| Q        |
| S        |
| C        |


Si aplicamos **One-Hot Encoding**, obtendremos tres nuevas columnas (una por cada categor√≠a):

```
Embarked_C   Embarked_Q   Embarked_S
```

La transformaci√≥n completa quedar√≠a as√≠:

| Embarked | Embarked_C | Embarked_Q | Embarked_S |
| -------- | ---------- | ---------- | ---------- |
| S        | 0          | 0          | 1          |
| C        | 1          | 0          | 0          |
| Q        | 0          | 1          | 0          |
| S        | 0          | 0          | 1          |
| C        | 1          | 0          | 0          |


üí° **Interpretaci√≥n r√°pida:**

* Cada categor√≠a se convierte en una columna.
* El valor 1 indica la categor√≠a correspondiente de esa fila.
* Solo una columna vale 1 porque cada pasajero solo puede embarcar por un puerto.

---

Tras aplicar One-Hot Encoding, en el √∫ltimo paso:

1. Se **eliminan las columnas originales** (`Embarked`, `Sex`, etc.)
2. Se **a√±aden las columnas generadas** al DataFrame

Ejemplo:

```python
# Eliminamos columnas categ√≥ricas originales, sustituy√©ndolas por las nuevas generadas
X_train = pd.concat([X_train.drop(columns=["Embarked"]), embarked_train], axis=1)
X_test  = pd.concat([X_test.drop(columns=["Embarked"]), embarked_test], axis=1)
```

---

### Paso 3.3. Feature Engineering b√°sico

El **Feature Engineering** consiste en crear nuevas variables (features) que puedan aportar informaci√≥n adicional al modelo.  
En esta fase del curso solo veremos **transformaciones sencillas y muy intuitivas**, sin t√©cnicas avanzadas.

El objetivo es mejorar la capacidad predictiva del modelo utilizando informaci√≥n que ya existe en el dataset, pero combinada de forma m√°s √∫til.

#### Creaci√≥n de variables intuitivas

A veces, combinar varias columnas puede generar una nueva variable con m√°s significado que las originales por separado.

En el Titanic, las columnas:

* `SibSp` ‚Üí n√∫mero de hermanos/esposos a bordo  
* `Parch` ‚Üí n√∫mero de padres/hijos a bordo  

por separado aportan informaci√≥n, pero **juntas pueden representar mejor el tama√±o del grupo familiar**.

Creamos una nueva columna:

```python
# Crear tama√±o familiar
X_train["FamilySize"] = X_train["SibSp"] + X_train["Parch"] + 1
X_test["FamilySize"]  = X_test["SibSp"] + X_test["Parch"] + 1
```

¬øPor qu√© sumamos 1?

üëâ Para incluir al propio pasajero en el tama√±o total de la familia.

Ejemplo:

| SibSp | Parch | FamilySize |
| ----- | ----- | ---------- |
| 1     | 0     | 2          |
| 0     | 0     | 1          |
| 3     | 1     | 5          |

üí° **Interpretaci√≥n:**
Los grupos m√°s grandes ten√≠an, en general, menor probabilidad de sobrevivir, por lo que esta variable puede ayudar al modelo.

---

#### Eliminaci√≥n de variables redundantes

Una vez que hemos creado una nueva variable derivada de otras dos, es posible que las variables originales **ya no sean necesarias** o aporten informaci√≥n duplicada.

En este nivel b√°sico, la regla que seguiremos ser√°:

üëâ **Si la nueva variable resume bien la informaci√≥n, podemos eliminar las columnas que la generaron.**

Por ejemplo, tras crear `FamilySize`, podr√≠amos eliminar `SibSp` y `Parch` para evitar redundancia:

```python
X_train = X_train.drop(columns=["SibSp", "Parch"])
X_test  = X_test.drop(columns=["SibSp", "Parch"])
```

Esto hace el dataset m√°s compacto y claro para el modelo.

---

### Paso 3.4. Escalado y normalizaci√≥n de variables num√©ricas

Tras imputar valores nulos y codificar las variables categ√≥ricas, el siguiente paso es **escalar o normalizar las variables num√©ricas**.  
Este proceso es fundamental en muchos modelos de Machine Learning, especialmente aquellos que son sensibles a la magnitud de los valores (por ejemplo, KNN, regresi√≥n log√≠stica, redes neuronales, SVM‚Ä¶).

En un dataset como el Titanic, algunas columnas num√©ricas tienen escalas muy distintas:

| Variable | Rango aproximado |
|----------|------------------|
| `Age`    | 0 ‚Äì 80           |
| `Fare`   | 0 ‚Äì 512          |
| `SibSp`  | 0 ‚Äì 8            |

Si no escalamos estas variables:

* Los modelos podr√≠an **dar m√°s importancia** a las columnas con valores m√°s grandes (`Fare`).
* La distancia entre puntos en modelos basados en distancia (como KNN o clustering) estar√≠a **sesgada**.
* El entrenamiento podr√≠a tardar m√°s y converger peor.

‚û°Ô∏è **Escalar no cambia la forma de los datos**, pero s√≠ su rango, para que todas las variables ‚Äújueguen en igualdad de condiciones‚Äù.

:::info Nota avanzada (Paso previo sobre los outliers)

En algunas variables **muy sesgadas**, como `Fare` en el Titanic, los valores altos son mucho mayores que los valores t√≠picos.  
Esto produce una distribuci√≥n con **cola larga**, que puede afectar a ciertos modelos o a algunos m√©todos de escalado (por ejemplo, MinMaxScaler).

![Gr√°fico EDA](./0-img/boxplot-fare.png)

En an√°lisis m√°s avanzados existe la posibilidad de aplicar **transformaciones matem√°ticas** como:

* `log()` ‚Üí reduce el impacto de los valores muy grandes  
* `sqrt()` ‚Üí suaviza moderadamente la distribuci√≥n  
* Transformaciones m√°s complejas como **Box-Cox** o **Yeo-Johnson**

Estas transformaciones no eliminan outliers reales, sino que simplemente **reducen su influencia** para modelos muy sensibles a distribuciones sesgadas.

Sin embargo, estas t√©cnicas pertenecen a un nivel m√°s avanzado de *Feature Engineering*.  
En este curso inicial **no son necesarias** y no las aplicaremos, ya que los modelos que veremos funcionan correctamente sin esta complejidad adicional.
:::

---

#### M√©todos m√°s utilizados

Aqu√≠ veremos los dos escaladores que se usan en la mayor√≠a de proyectos:

- **StandardScaler** ‚Üí distribuye con media = 0 y desviaci√≥n est√°ndar = 1  
- **MinMaxScaler** ‚Üí lleva todos los valores al rango [0, 1]

Ambos se utilizan **despu√©s** del *train/test split*, tras las imputaciones necesarias y antes de entrenar el modelo.

---

#### StandardScaler (escalado est√°ndar)

El `StandardScaler` transforma cada variable num√©rica para que tenga:

* **Media = 0**
* **Desviaci√≥n est√°ndar = 1**

Matem√°ticamente:

```
valor_escalado = (valor - media) / desviaci√≥n_est√°ndar
```

**¬øCu√°ndo usar StandardScaler?**

‚úî **Para casi todos los modelos cl√°sicos de Machine Learning.**

Porque:
* Centra los datos (media=0), lo cual ayuda al entrenamiento.
* No obliga a tener datos en un rango fijo.
* Funciona bien incluso si las variables no est√°n ‚Äúperfectamente distribuidas‚Äù.

**Ejemplo con Titanic**

```python
from sklearn.preprocessing import StandardScaler

# ¬°IMPORTANTE! Seleccionamos solo las columnas num√©ricas que queremos escalar (por ahora seleccionaremos todas)
num_cols = ["Pclass", "Age", "Fare", "SibSp", "Parch"]

# Creamos el escalador
scaler = StandardScaler()

# Ajustamos el escalador SOLO con los datos de train (fit)
X_train[num_cols] = scaler.fit_transform(X_train[num_cols])

# Aplicamos la transformaci√≥n a test (transform)
X_test[num_cols] = scaler.transform(X_test[num_cols])
```

Tras esto, cada columna quedar√° escalada, por ejemplo:


| Variable | Antes | Despu√©s |
| -------- | ----- | ------- |
| Age      | 22    | -0.73   |
| Age      | 38    | 1.22    |
| Fare     | 512   | 4.11    |
| Parch    | 0     | -0.45   |


üí° **Interpretaci√≥n:**

* Valores negativos ‚Üí menores que la media
* Valores positivos ‚Üí mayores que la media

---

#### MinMaxScaler (normalizaci√≥n 0‚Äì1)

El `MinMaxScaler` transforma cada variable num√©rica para que todos sus valores queden dentro del rango:

* **M√≠nimo = 0**
* **M√°ximo = 1**

Matem√°ticamente:

```
valor_escalado = (valor - min) / (max - min)
```

Es decir, cada valor se reescala proporcionalmente seg√∫n el valor m√≠nimo y m√°ximo de la columna.

---

**¬øCu√°ndo usar MinMaxScaler?**

‚úî **Cuando queremos que todos los valores queden entre 0 y 1.**  
‚úî √ötil en modelos que funcionan mejor con entradas normalizadas en un rango fijo, como:
* Redes neuronales
* Modelos que trabajan con activaciones entre 0 y 1   

‚ùå **No es ideal si hay valores extremos muy altos (outliers reales).** En esos casos, un √∫nico valor muy grande puede hacer que casi todos los dem√°s queden muy cerca de 0 tras escalar.

**Ejemplo con Titanic**

Vamos a escalar las mismas columnas num√©ricas que antes:

```python
from sklearn.preprocessing import MinMaxScaler

# Seleccionamos las columnas num√©ricas que queremos escalar
num_cols = ["Pclass", "Age", "Fare", "SibSp", "Parch"]

# Creamos el escalador
scaler = MinMaxScaler()

# Ajustamos el escalador SOLO con los datos de train (fit)
X_train[num_cols] = scaler.fit_transform(X_train[num_cols])

# Aplicamos la transformaci√≥n a test (transform)
X_test[num_cols] = scaler.transform(X_test[num_cols])
```

Tras esto, cada columna quedar√° normalizada al rango 0‚Äì1. Por ejemplo:

| Variable | Antes | Despu√©s |
| -------- | ----- | ------- |
| Age      | 22    | 0.28    |
| Age      | 38    | 0.54    |
| Fare     | 512   | 1.00    |
| Parch    | 0     | 0.00    |


üí° **Interpretaci√≥n:**

* **0** representa el valor m√≠nimo visto en *train*.
* **1** representa el valor m√°ximo visto en *train*.
* El resto de valores quedan en posiciones proporcionales dentro del intervalo 0‚Äì1.

:::info Nota sobre MinMaxScaler
Aunque MinMaxScaler funciona bien en muchos casos, recuerda que si existe un valor extremadamente alto (como una tarifa de 500), el resto de valores quedar√°n muy cerca de 0.
Por eso, aunque es √∫til, suele utilizarse menos que StandardScaler en problemas cl√°sicos.
:::

---

#### Resumen StandardScaler vs MinMaxScaler

| Caracter√≠stica          | StandardScaler               | MinMaxScaler                      |
| ----------------------- | ---------------------------- | --------------------------------- |
| Rango                   | No fijo (puede ser negativo) | Entre 0 y 1                       |
| Usa                     | Media y desviaci√≥n           | M√≠nimo y m√°ximo                   |
| Afectado por outliers   | S√≠ (mucho)                   | S√≠ (much√≠simo)                    |
| Mejor para              | Modelos lineales, KNN, SVM   | Redes neuronales, datos entre 0-1 |

---

#### ¬øDebo escalar todas las columnas num√©ricas?

‚úî **S√≠**, si usas modelos basados en distancias (KNN, SVM).  
‚úî **S√≠**, si usas regresi√≥n log√≠stica o redes neuronales.  
‚ùå **No es necesario** para √°rboles de decisi√≥n o Random Forest (no les afecta).

---

## Paso 4. Preparaci√≥n final del dataset

Despu√©s de haber realizado todas las tareas de preprocesamiento ‚Äîlimpieza estructural, divisi√≥n en *train/test*, imputaci√≥n, codificaci√≥n, escalado y feature engineering‚Äî ya tenemos nuestros datos pr√°cticamente listos para entrenar modelos de Machine Learning.

Antes de continuar, es recomendable hacer una **√∫ltima revisi√≥n r√°pida** para comprobar que todo ha quedado correctamente transformado.

---

### Paso 4.1 Revisi√≥n r√°pida de coherencia

En este punto debemos asegurarnos de que:

‚úî **No quedan valores nulos**

```python
X_train.isnull().sum()
X_test.isnull().sum()
```

Si alguna columna sigue teniendo nulos, puede deberse a:

* una codificaci√≥n incompleta,
* un problema en la imputaci√≥n,
* o columnas que no se incluyeron en el proceso.

‚úî **Todas las columnas son num√©ricas**

Los modelos cl√°sicos de Machine Learning **solo aceptan variables num√©ricas**. Debemos asegurarnos de que ya no quedan columnas categ√≥ricas sin transformar:

```python
X_train.dtypes
```

En este punto todo deber√≠a ser `int`, `float` o `uint8` (en caso de One-Hot Encoding).


‚úî **Las columnas de train y test coinciden**

Esto es MUY importante. Si el n√∫mero o nombre de columnas no coincide entre `X_train` y `X_test`, el modelo no podr√° predecir correctamente.

```python
print(X_train.shape)
print(X_test.shape)

print(X_train.columns)
print(X_test.columns)
```

Si no coinciden, normalmente significa que:

* falt√≥ eliminar alguna columna original antes de concatenar,
* hubo categor√≠as presentes en train que no aparecieron en test,
* o se mezclaron escaladores o imputadores incorrectamente.

---

### Paso 4.2 Dataset final listo para el modelado

Cuando se cumple todo los descrito en el paso anterior, **nuestro dataset est√° preparado para entrenar un modelo.**

A partir de aqu√≠, podemos comenzar con:

* Regresi√≥n log√≠stica
* √Årboles
* Random Forest
* KNN
* SVM
* etc.

---

### Paso 4.3 Guardar los datasets transformados (opcional, pero recomendable)

Es muy habitual guardar las versiones preprocesadas de los datos, especialmente si queremos:

* reutilizarlos,
* compartirlos,
* hacer pruebas con diferentes modelos,
* o evitar repetir todo el proceso de preprocesamiento.

```python
X_train.to_csv("titanic_X_train_preprocessed.csv", index=False)
X_test.to_csv("titanic_X_test_preprocessed.csv", index=False)
y_train.to_csv("titanic_y_train.csv", index=False)
y_test.to_csv("titanic_y_test.csv", index=False)
```

---

## Ejercicio de Titanic

Realiza el preprocesamiento del dataset **Titanic**, pero esta vez con un fichero ‚Äúensuciado‚Äù a prop√≥sito para poder aplicar la mayor√≠a de las t√©cnicas vistas.

Puedes partir del cuaderno que ya tienes de EDA (cuidado, tendr√°s que volver a ejecutarlo con el nuevo dataset):

üëâ [Cuaderno de EDA ‚Äî Titanic](./0-datasets/EDA_Titanic.ipynb)

üìÇ Dataset para este ejercicio: [`titanic_sucio.csv`](./0-datasets/titanic_sucio.csv)

Como opci√≥n adicional, puedes entrenar un modelo sencillo (por ejemplo, **KNN**) con el dataset ya preprocesado para comprobar que todo el flujo funciona correctamente y que los resultados mejoran con respecto al preprocesamiento sencillo que hac√≠amos al inicio del tema.

</div>