---
title: "Funciones de activaci√≥n y Backpropagation"
sidebar_position: 2
description: "Entendiendo el motor de las redes neuronales: c√≥mo la no linealidad y el ajuste de errores permiten que la IA aprenda patrones complejos."
keywords: [funciones de activaci√≥n, relu, sigmoid, softmax, backpropagation, descenso por gradiente, optimizadores, redes neuronales]
---

En el apartado anterior vimos c√≥mo una neurona simple puede aprender una relaci√≥n lineal (como Celsius a Fahrenheit). Pero, ¬øqu√© ocurre cuando el problema no es una l√≠nea recta? ¬øC√≥mo hace una red para aprender a reconocer un gato o predecir el precio de una vivienda con cientos de variables?

En esta secci√≥n vamos a descubrir:
* **El "pegamento" de las redes**: Las funciones de activaci√≥n.
* **El cerebro del entrenamiento**: El algoritmo de Backpropagation.
* **La br√∫jula**: El descenso por gradiente y los optimizadores.

:::info V√≠deo recomendado

Para reforzar visualmente la explicaci√≥n de backpropagation y funciones de activaci√≥n, te recomiendo ver el siguiente v√≠deo:

https://www.youtube.com/watch?v=_0wdproot34&t=2s

Explica de forma clara c√≥mo se propaga el error en una red neuronal y c√≥mo se aplican las derivadas mediante la regla de la cadena.
:::


---

## ¬øPor qu√© necesitamos funciones de activaci√≥n?

Si recordamos el perceptr√≥n, su c√°lculo era una suma ponderada: $salida = \sum(x_i w_i) + b$. Esto es, esencialmente, una operaci√≥n lineal.

### El problema de la linealidad

Imagina que construyes una red neuronal con 100 capas de profundidad, pero **no utilizas** funciones de activaci√≥n. Matem√°ticamente, la combinaci√≥n de muchas funciones lineales sigue siendo una funci√≥n lineal. 

> **Dicho de otro modo:** Sin funciones de activaci√≥n, una red neuronal ultra-compleja de mil capas tiene la misma capacidad de aprendizaje que una simple Regresi√≥n Lineal. No importa cu√°ntas neuronas a√±adas; solo podr√≠as dibujar l√≠neas rectas.

### Introduciendo la "no linealidad"

El mundo real no es lineal. Para que una red neuronal pueda aprender patrones complejos (como curvas, bordes en una imagen o estructuras de lenguaje), necesitamos algo que "rompa" esa linealidad.

Las **funciones de activaci√≥n** se encargan de esto:
* Deciden si una neurona debe "dispararse" (activarse) o no.
* Transforman la suma ponderada en algo m√°s complejo.
* Permiten que la red aprenda **formas curvas y patrones intrincados** en los datos.


Al aplicar una funci√≥n de activaci√≥n a la salida de cada neurona, permitimos que la red se convierta en un **aproximador universal**, capaz de representar pr√°cticamente cualquier relaci√≥n matem√°tica entre la entrada y la salida.

## Cat√°logo de funciones de activaci√≥n m√°s comunes

Para que la red neuronal pueda aprender relaciones complejas, necesitamos aplicar diferentes "filtros" matem√°ticos a la salida de las neuronas. Estas son las funciones de activaci√≥n m√°s utilizadas en la actualidad:

### ReLU (Rectified Linear Unit)

Es el est√°ndar de la industria para las **capas ocultas**. Su f√≥rmula es $f(x) = \max(0, x)$.

![Gr√°fica](../0-img/fa-relu.png)

* **Funcionamiento**: Si la entrada es positiva, la deja pasar exactamente como est√°; si es negativa, la convierte en cero.
* **Por qu√© se usa**: Es extremadamente r√°pida de calcular. Adem√°s, ayuda a que los modelos grandes aprendan m√°s r√°pido porque no "aplana" los valores positivos (evita la saturaci√≥n).
* **Riesgo**: Existe el problema de la "muerte de neuronas" si muchas entradas se vuelven negativas y se quedan bloqueadas en cero.

### Sigmoide (Sigmoid) üîó

Esta funci√≥n toma cualquier n√∫mero y lo "aplasta" para que quepa en el rango entre **0 y 1**.

![Gr√°fica](../0-img/fa-sigmoid.png)

* **Interpretaci√≥n**: Ese valor entre 0 y 1 se puede leer directamente como una **probabilidad** (por ejemplo, 0.85 = 85% de probabilidad).
* **Uso ideal**: Casi exclusivamente en la **capa de salida** de problemas donde la respuesta es "S√≠ o No" (clasificaci√≥n binaria).
* **Limitaci√≥n**: En capas profundas es problem√°tica porque sus extremos son muy planos, lo que hace que el aprendizaje sea lent√≠simo (problema del gradiente desvaneciente).

### Tanh (Tangente hiperb√≥lica)

Es una versi√≥n "estirada" de la sigmoide que va de **-1 a 1**.

![Gr√°fica](../0-img/fa-tanh.png)

* **Ventaja sobre la sigmoide**: Al estar centrada en el **cero**, los datos que salen de ella tienen una media cercana a cero, lo que suele facilitar que la siguiente capa aprenda mejor.
* **Uso**: Se usa en capas ocultas cuando queremos que el modelo sea capaz de manejar valores negativos con facilidad.

### Softmax

A diferencia de las anteriores, Softmax no mira a una sola neurona, sino a **todo el grupo** de neuronas de la capa de salida.

![Gr√°fico EDA](../0-img/fa-softmax.png)

* **L√≥gica**: Toma todas las puntuaciones de salida y las convierte en una **distribuci√≥n de probabilidad** que suma 100% (o 1.0).
* **Uso**: Es obligatoria en la capa de salida para problemas de **clasificaci√≥n multiclase** (por ejemplo, decidir si una imagen es un gato, perro o p√°jaro).


### Comparativa

| Funci√≥n | F√≥rmula simplificada | Rango | Aplicaci√≥n T√≠pica |
| --- | --- | --- | --- |
| **ReLU** |  $$f(x) = \max(0, x)$$  | $[0, \infty)$ | Capas ocultas (General) |
| **Sigmoide** | $$f(x) = \frac{1}{1 + e^{-x}}$$ | $(0, 1)$ | Salida binaria (Probabilidad) |
| **Tanh** | $$f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$ | $(-1, 1)$ | Capas ocultas espec√≠ficas |
| **Softmax** | $$f(x_i) = \frac{e^{x_i}}{\sum e^{x_j}}$$ | $(0, 1)$ | Salida multiclase (Excluyente) |

---

## ¬øC√≥mo aprende la red? El descenso por gradiente

Una vez que la red ha dado una respuesta (gracias a sus pesos, sesgos y funciones de activaci√≥n), comparamos ese resultado con el valor real usando la **funci√≥n de p√©rdida (Loss Function)**. Si el error es alto, hay que ajustar los pesos. Pero, ¬øhacia d√≥nde los movemos?

### Analog√≠a del arquero: ¬øQu√© estamos corrigiendo?

Imagina que est√°s aprendiendo a disparar con un arco. No eres un experto, as√≠ que la primera vez que disparas, el resultado es un poco ca√≥tico.

El proceso del arquero ser√≠a algo similar a:

1. **El Disparo (Forward Propagation):** Te preparas, ajustas la tensi√≥n de la cuerda, inclinas el brazo y sueltas la flecha. La flecha vuela y aterriza en alg√∫n lugar.
2. **La Medida del Error (Loss Function):** Miras d√≥nde cay√≥ la flecha en relaci√≥n con el centro de la diana. Si cay√≥ 20 cm a la derecha y 10 cm arriba, esa distancia es tu "error".
3. **La Reflexi√≥n (Backpropagation):** Aqu√≠ es donde ocurre la magia. No solo dices "fall√©", sino que piensas: *"Si la flecha se fue a la derecha, ¬øqu√© caus√≥ eso?"*. Tal vez fue la presi√≥n de tus dedos o la posici√≥n de tu hombro. Vas "hacia atr√°s", desde el error hasta tus movimientos iniciales, para entender qu√© corregir.
4. **El Ajuste (Optimizer):** La pr√≥xima vez, decides tensar un poco menos o mover el brazo un mil√≠metro a la izquierda. No cambias todo dr√°sticamente, haces un ajuste peque√±o.

El error de nuestra red (la distancia al centro de la diana) se debe a dos factores:

* **El Sesgo (Bias) üéØ**: Es la distancia entre el centro de la diana y el lugar donde suelen caer las flechas. Si el arquero siempre dispara demasiado a la izquierda, tiene un **sesgo alto**. En nuestra red, esto significa que el modelo es demasiado simple (underfitting) y no alcanza a entender la complejidad de los datos.
* **La Varianza (Variance) üìà**: Es la dispersi√≥n de las flechas. Si el arquero dispara y las flechas quedan muy lejos unas de otras, tiene una **varianza alta**. En la red, esto ocurre cuando el modelo es demasiado sensible a peque√±as variaciones (ruido) del entrenamiento (overfitting).

> **El objetivo del entrenamiento**: El Descenso por Gradiente es el proceso de ajustar la t√©cnica del arquero tras cada disparo para que sea preciso (bajo sesgo) y constante (baja varianza).

![Gr√°fica](../0-img/arquero.png)

### La analog√≠a de la monta√±a con niebla

¬øC√≥mo sabe el arquero qu√© corregir? Imagina que est√°s en la cima de una monta√±a y hay una niebla tan espesa que no ves nada. Tu objetivo es llegar al **valle** (el punto donde el error es m√≠nimo).

1. Como no ves el camino, tanteas con el pie el terreno a tu alrededor.
2. Buscas la direcci√≥n en la que el suelo **baja m√°s r√°pido**.
3. Das un paso peque√±o en esa direcci√≥n (el tama√±o del paso es el **Learning Rate**).
4. Repites el proceso hasta que sientes que el suelo ya no baja m√°s.

![Gr√°fico EDA](../0-img/descenso-gradiente.png)



### El Gradiente como br√∫jula

En matem√°ticas, el **gradiente** es un vector que nos indica la direcci√≥n de m√°xima subida. Como nosotros queremos **bajar** el error, seguimos la direcci√≥n opuesta al gradiente.

* Si el gradiente es positivo: el peso debe disminuir.
* Si el gradiente es negativo: el peso debe aumentar.

![Gr√°fica](../0-img/descenso-gradiente-2.png)

:::warning ACLARACI√ìN DESCENSO DE GRADIENTE
 El descenso por gradiente no nos dice cu√°nto error hay (eso lo dice la funci√≥n de p√©rdida), nos dice **hacia d√≥nde caminar** para que el error baje en la siguiente iteraci√≥n.

 Para m√°s informaci√≥n puedes leer este art√≠culo de [codificandobits](https://codificandobits.com/blog/el-gradiente-descendente/)
:::

---

## Backpropagation: El arte de repartir culpas

El **Backpropagation** (o propagaci√≥n hacia atr√°s) es el coraz√≥n del aprendizaje en las redes neuronales. Es el proceso matem√°tico que permite "repartir las culpas" del error entre todos los pesos y sesgos de la red para que el **descenso por gradiente** sepa exactamente qu√© ajustar.

Si el descenso por gradiente es nuestra br√∫jula para bajar la monta√±a, el **Backpropagation** es el mecanismo que nos dice cu√°nto contribuy√≥ cada parte de nuestra red a que termin√°ramos en el lugar equivocado.

### El flujo de la informaci√≥n: Forward vs. Backward

Para entender el Backpropagation, hay que visualizar el viaje de los datos en dos sentidos:

1. **Forward Pass (Hacia adelante)**: Los datos entran, se multiplican por los pesos, pasan por las funciones de activaci√≥n y generan una predicci√≥n.
2. **C√°lculo del Error**: Comparamos la predicci√≥n con el valor real usando la funci√≥n de p√©rdida.
3. **Backward Pass (Hacia atr√°s)**: El error viaja en sentido contrario, desde la salida hasta la entrada, calculando la responsabilidad de cada neurona en ese fallo.

### ¬øQui√©n tiene m√°s culpa?

Imagina que el arquero falla el tiro por 10 cm. El Backpropagation analiza la "cadena de mando" hacia atr√°s:

* ¬øFue culpa de la tensi√≥n del arco (capa de salida)?
* ¬øFue culpa de la postura del brazo (capa oculta)?
* ¬øO fue culpa de c√≥mo coloc√≥ los pies al principio (capa de entrada)?

Matem√°ticamente, esto se hace mediante la **Regla de la Cadena**. Esta regla permite calcular c√≥mo cambia el error total cuando movemos un peso espec√≠fico en el medio de la red.

> **En resumen**: El Backpropagation calcula el **gradiente** (la direcci√≥n del ajuste) para cada peso individual de la red, permitiendo que el optimizador sepa si debe subirlo o bajarlo ligeramente.

---

### Experimentaci√≥n Pr√°ctica: Google Colab

Para asentar estos conceptos, hemos preparado un cuaderno interactivo donde puedes ver al "arquero" en acci√≥n.

üëâ **Puedes abrir el cuaderno aqu√≠:**
[Demo descenso gradiente](../0-colab/demo_descenso_gradiente.ipynb)

**¬øPara qu√© sirve este laboratorio?**

* **Visualizar la "Monta√±a del Error" üèîÔ∏è**: Ver√°s c√≥mo la funci√≥n de p√©rdida crea una pendiente que la red debe bajar.
* **Ajustar el paso (Learning Rate) üë£**: Puedes cambiar el `lr` para ver qu√© pasa si el arquero da pasos muy cortos o demasiado largos.
* **Entender la ReLU üõ§Ô∏è**: Observar√°s por qu√© la gr√°fica se vuelve plana a la izquierda (el error es constante) y c√≥mo eso afecta al aprendizaje.

**Anatom√≠a de nuestra mini-red**

* **1 Neurona de Entrada ():** Es el punto donde la red recibe el dato inicial (en nuestro caso, el n√∫mero ).
* **1 Neurona Oculta:** Aqu√≠ es donde ocurre la primera transformaci√≥n. La entrada se multiplica por el **Peso 1 ()** y luego pasa por la funci√≥n de activaci√≥n **ReLU**.
* **1 Neurona de Salida:** El resultado de la neurona oculta se multiplica por el **Peso 2 ()** para darnos la **Predicci√≥n** final.

Trabajar con una sola neurona nos permite ver con total claridad la relaci√≥n matem√°tica entre el **peso** y el **error**. En una red real con millones de neuronas, esta "monta√±a de error" tendr√≠a millones de dimensiones, pero la l√≥gica de bajar hacia el valle sigue siendo la misma.

---

## Optimizadores: El ritmo del aprendizaje

Si el **Backpropagation** nos da la direcci√≥n (el gradiente) y el **Learning Rate** define el tama√±o del paso, los **Optimizadores** son algoritmos que deciden c√≥mo aplicar esos ajustes de forma inteligente para llegar al valle del error lo m√°s r√°pido posible sin "tropezar".

### ¬øPor qu√© necesitamos un optimizador?

En el ejemplo del Colab, vimos que si el paso es muy grande, el peso puede rebotar de un lado a otro. Un optimizador ayuda a suavizar ese movimiento. Su funci√≥n principal es gestionar la **velocidad** y la **inercia** del descenso.

* **Inercia (Momentum) ‚öΩ**: Imagina una bola bajando la monta√±a. Si la pendiente es muy pronunciada, la bola gana velocidad. El optimizador usa esto para atravesar zonas planas o peque√±os baches que podr√≠an detener a un arquero que va paso a paso.
* **Adaptabilidad üß†**: No todos los pesos necesitan cambiar a la misma velocidad. Algunos necesitan pasos grandes al principio y otros pasos min√∫sculos para ajustar la precisi√≥n final.

### Tipos comunes de Optimizadores

Existen varios "estilos" de descenso, pero estos son los m√°s utilizados:

| Optimizador | Caracter√≠sticas | Analog√≠a |
| --- | --- | --- |
| **SGD** (Stochastic Gradient Descent) | El m√°s b√°sico. Actualiza los pesos con cada grupo de datos. | Un caminante con br√∫jula que da pasos constantes. |
| **Adam** (Adaptive Moment Estimation) | El est√°ndar actual. Combina inercia y pasos adaptables para cada peso. | Un explorador con GPS que corre en las bajadas y camina con cuidado al llegar al destino. |
| **RMSprop** | Muy bueno para redes que trabajan con secuencias (como texto). | Un corredor que ajusta su velocidad seg√∫n lo accidentado que sea el terreno. |

> **Nota t√©cnica:** En la mayor√≠a de los proyectos modernos, se empieza usando **Adam** por defecto, ya que suele encontrar el valle del error de forma m√°s eficiente que el resto.

---

### Laboratorio Comparativo: SGD vs. Adam

En este experimento, enfrentamos al optimizador m√°s b√°sico contra el est√°ndar de la industria. El objetivo es observar c√≥mo la **inercia** y la **adaptabilidad** cambian la forma en que una red encuentra la soluci√≥n.

**¬øQu√© observar en este Colab?**

* **La "Memoria" de Adam üß†**: Nota c√≥mo Adam acumula velocidad. Si el error es grande durante varios pasos, Adam "se conf√≠a" y acelera, lo que puede provocar que se pase de largo (overshoot) antes de frenar.
* **La Constancia de SGD üê¢**: Observa que SGD es mucho m√°s previsible. Sus pasos dependen solo de la pendiente actual, lo que lo hace m√°s lento pero a veces m√°s estable en problemas simples.
* **El rebasamiento (Overshoot) üé¢**: F√≠jate en los valores negativos de Adam. ¬øPor qu√© sigue avanzando hacia la izquierda si el error ya era cero? Es la inercia de sus pasos anteriores intentando estabilizarse.

---

## Problemas Comunes en el Entrenamiento

Incluso con el mejor optimizador, el aprendizaje puede fallar por factores matem√°ticos o de configuraci√≥n. Aqu√≠ los tres m√°s importantes:

### Gradientes Explosivos y Desvanecientes

Ocurre cuando el gradiente, al viajar hacia atr√°s por las capas, se vuelve extremadamente grande o casi cero.

* **Explosivos**: El ajuste es tan violento que los pesos "explotan" (se vuelven `NaN` o infinitos). La red se vuelve inestable.
* **Desvanecientes (Vanishing)**: El ajuste es tan min√∫sculo que las capas profundas dejan de aprender. Es como intentar mover una monta√±a con un soplido.

### Sobreajuste (Overfitting)

La red se vuelve tan buena memorizando los datos de entrenamiento que pierde la capacidad de generalizar. Es como un alumno que memoriza las respuestas del examen pero no entiende la materia: si le cambias una coma, suspende.

### M√≠nimos Locales y Mesetas (Plateaus)

A veces el "arquero" se queda atrapado en un peque√±o valle que no es el punto m√°s bajo de la monta√±a (m√≠nimo local) o en una zona tan plana que no sabe hacia d√≥nde seguir bajando. Aqu√≠ es donde **Adam** brilla gracias a su "inercia" para seguir avanzando.

![Gr√°fica](../0-img/mesetas.png)

---

## Actividad de Seguimiento: Jugando en el Laboratorio de Neuronas

Para entender c√≥mo interact√∫an todas las piezas que hemos estudiado, entra en [TensorFlow Playground](https://playground.tensorflow.org/) y realiza el siguiente experimento guiado.

### El Objetivo

Configurar una red neuronal capaz de clasificar el conjunto de datos **"Circular"** (el que tiene un c√≠rculo azul dentro de un anillo naranja) con el menor error posible.

### Instrucciones de Configuraci√≥n

1. **Datos**: Selecciona el dataset circular (arriba a la izquierda).
2. **Arquitectura**: Empieza con una red muy simple: **1 capa oculta con 2 neuronas**. Si no es suficiente, prueba con problemas m√°s complejos.
3. **El "Pegamento" (Activaci√≥n)**: Cambia entre las diferentes.
4. **La Br√∫jula (Learning Rate)**: Config√∫ralo en **0.03**. Ve cambiando el valor hasta conseguir que se separen correctamente los datos.

